{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŒ± CloudGarden - Smart Plant Monitoring System",
        "## HW3 - Refactored Version",
        "",
        "**Features:**",
        "- Real-time IoT Dashboard",
        "- Plant Disease Detection",
        "- RAG-powered Q&A (with improved search)",
        "- AI Chat with Database Persistence",
        "- Gamification System",
        "- Report Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ 1. Dependencies Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install all required packages\n",
        "!pip install -q --upgrade gradio pandas matplotlib python-docx\n",
        "!pip install -q --upgrade firebase-admin plotly gdown\n",
        "!pip install -q cerebras-cloud-sdk\n",
        "!pip install -q google-genai\n",
        "!pip install -q scikit-learn  # For TF-IDF vectorization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# ALL IMPORTS - Organized by category\n",
        "# =============================================================================\n",
        "\n",
        "# --- Standard Library ---\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import tempfile\n",
        "import warnings\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from zoneinfo import ZoneInfo\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any, Optional\n",
        "from urllib.parse import quote\n",
        "from dataclasses import dataclass, field\n",
        "from abc import ABC, abstractmethod\n",
        "import hashlib\n",
        "\n",
        "# --- Data Science ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Plotly ---\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# --- ML/NLP ---\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# --- Web ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Gradio ---\n",
        "import gradio as gr\n",
        "\n",
        "# --- ML Model ---\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- Firebase ---\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, db\n",
        "import gdown\n",
        "\n",
        "# --- Document Processing ---\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.shared import Inches, Pt, RGBColor\n",
        "\n",
        "# --- AI APIs ---\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- Setup ---\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "print(\"âœ… All imports loaded successfully\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - All settings in one place\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Central configuration for the entire application.\"\"\"\n",
        "    \n",
        "    # --- Server Settings ---\n",
        "    SERVER_URL: str = \"https://server-cloud-v645.onrender.com/\"\n",
        "    FEED_NAME: str = \"json\"\n",
        "    BATCH_LIMIT: int = 200\n",
        "    \n",
        "    # --- Firebase Settings ---\n",
        "    FIREBASE_URL: str = \"https://cloud-81451-default-rtdb.europe-west1.firebasedatabase.app/\"\n",
        "    FIREBASE_KEY_ID: str = \"1ESnh8BIbGKrVEijA9nKNgNJNdD5kAaYC\"\n",
        "    FIREBASE_KEY_FILE: str = \"firebase_key.json\"\n",
        "    \n",
        "    # --- AI Models ---\n",
        "    CEREBRAS_MODEL: str = \"llama3.1-8b\"\n",
        "    GEMINI_MODEL: str = \"gemini-2.5-flash\"\n",
        "    PLANT_MODEL: str = \"linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification\"\n",
        "    \n",
        "    # --- Sensor Thresholds ---\n",
        "    TEMP_MIN: float = 18.0\n",
        "    TEMP_MAX: float = 32.0\n",
        "    HUMIDITY_MIN: float = 35.0\n",
        "    HUMIDITY_MAX: float = 75.0\n",
        "    SOIL_MIN: float = 20.0\n",
        "    SOIL_MAX: float = 60.0\n",
        "    \n",
        "    # --- Colors ---\n",
        "    COLOR_TEMP: str = \"#1f77b4\"\n",
        "    COLOR_HUMIDITY: str = \"#ff7f0e\"\n",
        "    COLOR_SOIL: str = \"#2ca02c\"\n",
        "    COLOR_OK: str = \"#2ca02c\"\n",
        "    COLOR_WARN: str = \"#ffbf00\"\n",
        "    COLOR_BAD: str = \"#d62728\"\n",
        "    \n",
        "    # --- Timezone ---\n",
        "    TIMEZONE: str = \"Asia/Jerusalem\"\n",
        "    \n",
        "    # --- RAG Settings ---\n",
        "    RAG_CHUNK_SIZE: int = 1000\n",
        "    RAG_CHUNK_OVERLAP: int = 200\n",
        "    RAG_TOP_K: int = 3\n",
        "    BM25_K1: float = 1.5\n",
        "    BM25_B: float = 0.75\n",
        "    \n",
        "    # --- Document URLs for RAG ---\n",
        "    DOC_URLS: List[str] = field(default_factory=lambda: [\n",
        "        \"https://doi.org/10.1038/s41598-025-20629-y\",\n",
        "        \"https://doi.org/10.3389/fpls.2016.01419\",\n",
        "        \"https://doi.org/10.1038/s41598-025-05102-0\",\n",
        "        \"https://doi.org/10.1038/s41598-025-04758-y\",\n",
        "        \"https://doi.org/10.2174/0118743315321139240627092707\",\n",
        "    ])\n",
        "\n",
        "\n",
        "# Create global config instance\n",
        "CONFIG = Config()\n",
        "\n",
        "print(\"âœ… Configuration loaded\")\n",
        "print(f\"   Server: {CONFIG.SERVER_URL}\")\n",
        "print(f\"   Firebase: {CONFIG.FIREBASE_URL}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¥ 4. Firebase Service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# FIREBASE SERVICE - All Firebase operations\n",
        "# =============================================================================\n",
        "\n",
        "class FirebaseService:\n",
        "    \"\"\"Centralized Firebase operations.\"\"\"\n",
        "    \n",
        "    _instance = None\n",
        "    _initialized = False\n",
        "    \n",
        "    def __new__(cls):\n",
        "        if cls._instance is None:\n",
        "            cls._instance = super().__new__(cls)\n",
        "        return cls._instance\n",
        "    \n",
        "    def initialize(self) -> bool:\n",
        "        \"\"\"Initialize Firebase connection.\"\"\"\n",
        "        if self._initialized:\n",
        "            return True\n",
        "            \n",
        "        try:\n",
        "            # Download credentials\n",
        "            if os.path.exists(CONFIG.FIREBASE_KEY_FILE):\n",
        "                os.remove(CONFIG.FIREBASE_KEY_FILE)\n",
        "            \n",
        "            url = f'https://drive.google.com/uc?id={CONFIG.FIREBASE_KEY_ID}'\n",
        "            gdown.download(url, CONFIG.FIREBASE_KEY_FILE, quiet=True, fuzzy=True)\n",
        "            \n",
        "            # Initialize Firebase Admin\n",
        "            if not firebase_admin._apps:\n",
        "                firebase_admin.initialize_app(\n",
        "                    credentials.Certificate(CONFIG.FIREBASE_KEY_FILE),\n",
        "                    {'databaseURL': CONFIG.FIREBASE_URL}\n",
        "                )\n",
        "            \n",
        "            self._initialized = True\n",
        "            print(\"âœ… Firebase initialized\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Firebase initialization failed: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def get(self, path: str) -> Any:\n",
        "        \"\"\"Get data from Firebase path.\"\"\"\n",
        "        try:\n",
        "            return db.reference(path).get()\n",
        "        except Exception as e:\n",
        "            print(f\"Firebase GET error ({path}): {e}\")\n",
        "            return None\n",
        "    \n",
        "    def set(self, path: str, data: Any) -> bool:\n",
        "        \"\"\"Set data at Firebase path.\"\"\"\n",
        "        try:\n",
        "            db.reference(path).set(data)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Firebase SET error ({path}): {e}\")\n",
        "            return False\n",
        "    \n",
        "    def push(self, path: str, data: Any) -> Optional[str]:\n",
        "        \"\"\"Push data to Firebase path, returns key.\"\"\"\n",
        "        try:\n",
        "            ref = db.reference(path).push(data)\n",
        "            return ref.key\n",
        "        except Exception as e:\n",
        "            print(f\"Firebase PUSH error ({path}): {e}\")\n",
        "            return None\n",
        "    \n",
        "    def update(self, path: str, data: Dict) -> bool:\n",
        "        \"\"\"Update data at Firebase path.\"\"\"\n",
        "        try:\n",
        "            db.reference(path).update(data)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Firebase UPDATE error ({path}): {e}\")\n",
        "            return False\n",
        "    \n",
        "    def delete(self, path: str) -> bool:\n",
        "        \"\"\"Delete data at Firebase path.\"\"\"\n",
        "        try:\n",
        "            db.reference(path).delete()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Firebase DELETE error ({path}): {e}\")\n",
        "            return False\n",
        "    \n",
        "    def http_get(self, path: str) -> Any:\n",
        "        \"\"\"Direct HTTP GET (for RAG index operations).\"\"\"\n",
        "        url = f\"{CONFIG.FIREBASE_URL.rstrip('/')}/{path}.json\"\n",
        "        try:\n",
        "            r = requests.get(url, timeout=30)\n",
        "            return r.json() if r.status_code == 200 else None\n",
        "        except:\n",
        "            return None\n",
        "    \n",
        "    def http_put(self, path: str, data: Any) -> bool:\n",
        "        \"\"\"Direct HTTP PUT (for RAG index operations).\"\"\"\n",
        "        url = f\"{CONFIG.FIREBASE_URL.rstrip('/')}/{path}.json\"\n",
        "        try:\n",
        "            r = requests.put(url, json=data, timeout=30)\n",
        "            return r.status_code == 200\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "\n",
        "# Create global instance\n",
        "firebase = FirebaseService()\n",
        "firebase.initialize()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤– 5. AI API Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# AI API CLIENTS - Cerebras and Gemini\n",
        "# =============================================================================\n",
        "\n",
        "class AIClients:\n",
        "    \"\"\"Manage AI API clients.\"\"\"\n",
        "    \n",
        "    _cerebras_client = None\n",
        "    _gemini_client = None\n",
        "    \n",
        "    @classmethod\n",
        "    def get_cerebras(cls):\n",
        "        \"\"\"Get Cerebras client.\"\"\"\n",
        "        if cls._cerebras_client is None:\n",
        "            api_key = os.environ.get(\"CEREBRAS_API_KEY\", \"csk-r8npfcy9jckcxcd98t4422mw99wx3ew89k4h3rrhdvy5ekde\")\n",
        "            os.environ[\"CEREBRAS_API_KEY\"] = api_key\n",
        "            cls._cerebras_client = Cerebras(api_key=api_key)\n",
        "        return cls._cerebras_client\n",
        "    \n",
        "    @classmethod\n",
        "    def get_gemini(cls):\n",
        "        \"\"\"Get Gemini client.\"\"\"\n",
        "        if cls._gemini_client is None:\n",
        "            # Try multiple sources for API key\n",
        "            api_key = None\n",
        "            try:\n",
        "                from google.colab import userdata\n",
        "                api_key = userdata.get(\"GEMINI_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            if not api_key:\n",
        "                api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "            \n",
        "            if not api_key:\n",
        "                raise RuntimeError(\"Missing GEMINI_API_KEY\")\n",
        "            \n",
        "            cls._gemini_client = genai.Client(api_key=api_key)\n",
        "        return cls._gemini_client\n",
        "    \n",
        "    @classmethod\n",
        "    def generate_gemini(cls, prompt: str, temperature: float = 0.7, max_tokens: int = 1024) -> str:\n",
        "        \"\"\"Generate text using Gemini.\"\"\"\n",
        "        try:\n",
        "            client = cls.get_gemini()\n",
        "            response = client.models.generate_content(\n",
        "                model=CONFIG.GEMINI_MODEL,\n",
        "                contents=[types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt)])],\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=temperature,\n",
        "                    max_output_tokens=max_tokens,\n",
        "                ),\n",
        "            )\n",
        "            return (response.text or \"\").strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini error: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    @classmethod\n",
        "    def generate_cerebras(cls, prompt: str, system: str = \"\") -> str:\n",
        "        \"\"\"Generate text using Cerebras.\"\"\"\n",
        "        try:\n",
        "            client = cls.get_cerebras()\n",
        "            messages = []\n",
        "            if system:\n",
        "                messages.append({\"role\": \"system\", \"content\": system})\n",
        "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "            \n",
        "            response = client.chat.completions.create(\n",
        "                model=CONFIG.CEREBRAS_MODEL,\n",
        "                messages=messages,\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Cerebras error: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "# Load plant disease classifier\n",
        "plant_classifier = pipeline(\"image-classification\", model=CONFIG.PLANT_MODEL)\n",
        "print(\"âœ… AI clients ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ 6. NLP Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# NLP UTILITIES - Text processing for RAG\n",
        "# =============================================================================\n",
        "\n",
        "class NLPProcessor:\n",
        "    \"\"\"NLP processing utilities for RAG.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self._tfidf_vectorizer = None\n",
        "        self._doc_vectors = None\n",
        "    \n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text into words.\"\"\"\n",
        "        return re.findall(r\"\\w+\", (text or \"\").lower())\n",
        "    \n",
        "    def preprocess(self, text: str) -> List[str]:\n",
        "        \"\"\"Full preprocessing: tokenize, remove stopwords, stem.\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        tokens = [t for t in tokens if t not in self.stop_words and len(t) > 2]\n",
        "        tokens = [self.stemmer.stem(t) for t in tokens]\n",
        "        return tokens\n",
        "    \n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Preprocess and join back to string.\"\"\"\n",
        "        return \" \".join(self.preprocess(text))\n",
        "    \n",
        "    def chunk_text(self, text: str, chunk_size: int = None, overlap: int = None) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "        chunk_size = chunk_size or CONFIG.RAG_CHUNK_SIZE\n",
        "        overlap = overlap or CONFIG.RAG_CHUNK_OVERLAP\n",
        "        \n",
        "        if not text or len(text) < chunk_size:\n",
        "            return [text] if text else []\n",
        "        \n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end]\n",
        "            \n",
        "            # Try to break at sentence boundary\n",
        "            if end < len(text):\n",
        "                last_period = chunk.rfind('.')\n",
        "                if last_period > chunk_size // 2:\n",
        "                    chunk = chunk[:last_period + 1]\n",
        "                    end = start + last_period + 1\n",
        "            \n",
        "            chunks.append(chunk.strip())\n",
        "            start = end - overlap\n",
        "        \n",
        "        return [c for c in chunks if c]\n",
        "    \n",
        "    def build_tfidf(self, documents: List[str]) -> None:\n",
        "        \"\"\"Build TF-IDF vectors for documents.\"\"\"\n",
        "        if not documents:\n",
        "            return\n",
        "        \n",
        "        self._tfidf_vectorizer = TfidfVectorizer(\n",
        "            preprocessor=self.preprocess_text,\n",
        "            max_features=5000,\n",
        "            ngram_range=(1, 2)\n",
        "        )\n",
        "        self._doc_vectors = self._tfidf_vectorizer.fit_transform(documents)\n",
        "    \n",
        "    def tfidf_search(self, query: str, top_k: int = 3) -> List[Tuple[int, float]]:\n",
        "        \"\"\"Search using TF-IDF similarity.\"\"\"\n",
        "        if self._tfidf_vectorizer is None or self._doc_vectors is None:\n",
        "            return []\n",
        "        \n",
        "        query_vec = self._tfidf_vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vec, self._doc_vectors).flatten()\n",
        "        \n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "        return [(int(idx), float(similarities[idx])) for idx in top_indices if similarities[idx] > 0]\n",
        "\n",
        "\n",
        "# Create global NLP processor\n",
        "nlp = NLPProcessor()\n",
        "print(\"âœ… NLP processor ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š 7. Sensor Data Service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SENSOR DATA SERVICE - IoT data operations\n",
        "# =============================================================================\n",
        "\n",
        "class SensorDataService:\n",
        "    \"\"\"Handle all sensor data operations.\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def fetch_from_server(feed: str, limit: int = 200) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Fetch sensor data from IoT server.\"\"\"\n",
        "        try:\n",
        "            resp = requests.get(\n",
        "                f\"{CONFIG.SERVER_URL}/history\",\n",
        "                params={\"feed\": feed, \"limit\": limit},\n",
        "                timeout=30\n",
        "            )\n",
        "            data = resp.json()\n",
        "            \n",
        "            if \"data\" not in data or not data[\"data\"]:\n",
        "                return None\n",
        "            \n",
        "            df = pd.DataFrame(data[\"data\"])\n",
        "            if \"created_at\" not in df.columns or \"value\" not in df.columns:\n",
        "                return None\n",
        "            \n",
        "            df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n",
        "            df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "            df = df.dropna(subset=[\"created_at\", \"value\"]).sort_values(\"created_at\")\n",
        "            \n",
        "            return df if not df.empty else None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Server fetch error: {e}\")\n",
        "            return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def load_from_firebase() -> pd.DataFrame:\n",
        "        \"\"\"Load all sensor data from Firebase.\"\"\"\n",
        "        data = firebase.get('/sensor_data')\n",
        "        if not data:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        records = []\n",
        "        for v in data.values():\n",
        "            try:\n",
        "                records.append({\n",
        "                    'timestamp': pd.to_datetime(v['created_at']),\n",
        "                    'temperature': float(v['temperature']),\n",
        "                    'humidity': float(v['humidity']),\n",
        "                    'soil': float(v['soil'])\n",
        "                })\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        if not records:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        df = pd.DataFrame(records)\n",
        "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "        \n",
        "        # Clip to valid ranges\n",
        "        df['temperature'] = df['temperature'].clip(-50, 100)\n",
        "        df['humidity'] = df['humidity'].clip(0, 100)\n",
        "        df['soil'] = df['soil'].clip(0, 100)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    @staticmethod\n",
        "    def sync_to_firebase() -> Tuple[str, int]:\n",
        "        \"\"\"Sync new data from server to Firebase.\"\"\"\n",
        "        msgs = [\"Starting sync...\"]\n",
        "        \n",
        "        # Get latest timestamp from Firebase\n",
        "        try:\n",
        "            latest_data = db.reference('/sensor_data').order_by_child('created_at').limit_to_last(1).get()\n",
        "            latest_ts = list(latest_data.values())[0]['created_at'] if latest_data else None\n",
        "        except:\n",
        "            latest_ts = None\n",
        "        \n",
        "        msgs.append(f\"Latest in DB: {latest_ts or 'None'}\")\n",
        "        \n",
        "        # Fetch from server\n",
        "        try:\n",
        "            resp = requests.get(\n",
        "                f\"{CONFIG.SERVER_URL}/history\",\n",
        "                params={\"feed\": CONFIG.FEED_NAME, \"limit\": CONFIG.BATCH_LIMIT},\n",
        "                timeout=180\n",
        "            ).json()\n",
        "        except Exception as e:\n",
        "            return \"\\n\".join(msgs + [f\"Server error: {e}\"]), 0\n",
        "        \n",
        "        if \"data\" not in resp:\n",
        "            return \"\\n\".join(msgs + [\"No data from server\"]), 0\n",
        "        \n",
        "        # Filter new records\n",
        "        new_records = [s for s in resp[\"data\"] if not latest_ts or s[\"created_at\"] > latest_ts]\n",
        "        \n",
        "        if not new_records:\n",
        "            return \"\\n\".join(msgs + [\"No new data\"]), 0\n",
        "        \n",
        "        # Save to Firebase\n",
        "        ref = db.reference('/sensor_data')\n",
        "        saved = 0\n",
        "        \n",
        "        for sample in new_records:\n",
        "            try:\n",
        "                vals = json.loads(sample['value'])\n",
        "                timestamp_key = sample['created_at'].replace(':', '-').replace('.', '-')\n",
        "                \n",
        "                ref.child(timestamp_key).set({\n",
        "                    'created_at': sample['created_at'],\n",
        "                    'temperature': max(-50, min(100, float(vals['temperature']))),\n",
        "                    'humidity': max(0, min(100, float(vals['humidity']))),\n",
        "                    'soil': max(0, min(100, float(vals['soil'])))\n",
        "                })\n",
        "                saved += 1\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        return \"\\n\".join(msgs + [f\"Found {len(new_records)} new\", f\"Saved {saved}!\"]), saved\n",
        "    \n",
        "    @staticmethod\n",
        "    def check_plant_status(temp: float, humidity: float, soil: float) -> Tuple[str, str, List[str]]:\n",
        "        \"\"\"Check plant health based on sensor values.\"\"\"\n",
        "        issues = []\n",
        "        warnings = []\n",
        "        \n",
        "        checks = [\n",
        "            (\"Temperature\", temp, CONFIG.TEMP_MIN, CONFIG.TEMP_MAX, 1),\n",
        "            (\"Air humidity\", humidity, CONFIG.HUMIDITY_MIN, CONFIG.HUMIDITY_MAX, 3),\n",
        "            (\"Soil moisture\", soil, CONFIG.SOIL_MIN, CONFIG.SOIL_MAX, 3),\n",
        "        ]\n",
        "        \n",
        "        for name, value, low, high, margin in checks:\n",
        "            if not (low <= value <= high):\n",
        "                issues.append(f\"{name} out of range ({value:.1f})\")\n",
        "            elif value <= low + margin or value >= high - margin:\n",
        "                warnings.append(f\"{name} near limit ({value:.1f})\")\n",
        "        \n",
        "        if issues:\n",
        "            status = \"ðŸ”´ Plant Status: Not OK\"\n",
        "            color = \"bad\"\n",
        "        elif warnings:\n",
        "            status = \"ðŸŸ¡ Plant Status: Warning\"\n",
        "            color = \"warn\"\n",
        "        else:\n",
        "            status = \"ðŸŸ¢ Plant Status: OK\"\n",
        "            color = \"ok\"\n",
        "        \n",
        "        details = issues + warnings if (issues or warnings) else [\"All sensors within valid ranges\"]\n",
        "        \n",
        "        return status, color, details\n",
        "\n",
        "\n",
        "# Create global instance\n",
        "sensor_service = SensorDataService()\n",
        "print(\"âœ… Sensor data service ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¬ 8. Chat Service (with Database Persistence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CHAT SERVICE - Conversations stored in Firebase\n",
        "# =============================================================================\n",
        "\n",
        "class ChatService:\n",
        "    \"\"\"Chat service with Firebase persistence for conversation history.\"\"\"\n",
        "    \n",
        "    FIREBASE_PATH = \"chat_conversations\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.current_session_id: Optional[str] = None\n",
        "        self.session_history: List[Tuple[str, str]] = []\n",
        "    \n",
        "    def _generate_session_id(self) -> str:\n",
        "        \"\"\"Generate unique session ID.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        random_suffix = hashlib.md5(str(random.random()).encode()).hexdigest()[:6]\n",
        "        return f\"session_{timestamp}_{random_suffix}\"\n",
        "    \n",
        "    def start_new_session(self) -> str:\n",
        "        \"\"\"Start a new chat session.\"\"\"\n",
        "        self.current_session_id = self._generate_session_id()\n",
        "        self.session_history = []\n",
        "        \n",
        "        # Create session in Firebase\n",
        "        session_data = {\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"updated_at\": datetime.now().isoformat(),\n",
        "            \"messages\": []\n",
        "        }\n",
        "        firebase.set(f\"{self.FIREBASE_PATH}/{self.current_session_id}\", session_data)\n",
        "        \n",
        "        return self.current_session_id\n",
        "    \n",
        "    def load_session(self, session_id: str) -> bool:\n",
        "        \"\"\"Load existing session from Firebase.\"\"\"\n",
        "        data = firebase.get(f\"{self.FIREBASE_PATH}/{session_id}\")\n",
        "        if not data:\n",
        "            return False\n",
        "        \n",
        "        self.current_session_id = session_id\n",
        "        messages = data.get(\"messages\", [])\n",
        "        \n",
        "        # Reconstruct history as list of tuples\n",
        "        self.session_history = []\n",
        "        for msg in messages:\n",
        "            if msg.get(\"role\") == \"user\":\n",
        "                user_msg = msg.get(\"content\", \"\")\n",
        "                # Find corresponding assistant message\n",
        "                idx = messages.index(msg)\n",
        "                if idx + 1 < len(messages) and messages[idx + 1].get(\"role\") == \"assistant\":\n",
        "                    assistant_msg = messages[idx + 1].get(\"content\", \"\")\n",
        "                    self.session_history.append((user_msg, assistant_msg))\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def get_all_sessions(self) -> List[Dict]:\n",
        "        \"\"\"Get list of all chat sessions.\"\"\"\n",
        "        data = firebase.get(self.FIREBASE_PATH)\n",
        "        if not data:\n",
        "            return []\n",
        "        \n",
        "        sessions = []\n",
        "        for session_id, session_data in data.items():\n",
        "            sessions.append({\n",
        "                \"id\": session_id,\n",
        "                \"created_at\": session_data.get(\"created_at\", \"\"),\n",
        "                \"message_count\": len(session_data.get(\"messages\", [])),\n",
        "            })\n",
        "        \n",
        "        # Sort by creation time, newest first\n",
        "        sessions.sort(key=lambda x: x[\"created_at\"], reverse=True)\n",
        "        return sessions\n",
        "    \n",
        "    def _save_message(self, role: str, content: str) -> None:\n",
        "        \"\"\"Save a message to Firebase.\"\"\"\n",
        "        if not self.current_session_id:\n",
        "            self.start_new_session()\n",
        "        \n",
        "        message = {\n",
        "            \"role\": role,\n",
        "            \"content\": content,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        # Get current messages\n",
        "        path = f\"{self.FIREBASE_PATH}/{self.current_session_id}\"\n",
        "        data = firebase.get(path) or {\"messages\": []}\n",
        "        messages = data.get(\"messages\", [])\n",
        "        \n",
        "        # Handle Firebase converting empty list to None\n",
        "        if messages is None:\n",
        "            messages = []\n",
        "        \n",
        "        messages.append(message)\n",
        "        \n",
        "        # Update session\n",
        "        firebase.update(path, {\n",
        "            \"messages\": messages,\n",
        "            \"updated_at\": datetime.now().isoformat()\n",
        "        })\n",
        "    \n",
        "    def chat(self, user_message: str, temperature: float = 0.7) -> Tuple[str, List[Tuple[str, str]]]:\n",
        "        \"\"\"Send message and get response, persisted to database.\"\"\"\n",
        "        user_message = (user_message or \"\").strip()\n",
        "        if not user_message:\n",
        "            return \"\", self.session_history\n",
        "        \n",
        "        # Ensure we have a session\n",
        "        if not self.current_session_id:\n",
        "            self.start_new_session()\n",
        "        \n",
        "        # Build conversation context\n",
        "        contents = []\n",
        "        for u, a in self.session_history:\n",
        "            contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=u)]))\n",
        "            contents.append(types.Content(role=\"model\", parts=[types.Part.from_text(text=a)]))\n",
        "        contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=user_message)]))\n",
        "        \n",
        "        # Generate response\n",
        "        try:\n",
        "            client = AIClients.get_gemini()\n",
        "            response = client.models.generate_content(\n",
        "                model=CONFIG.GEMINI_MODEL,\n",
        "                contents=contents,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    system_instruction=\"You are a helpful, friendly chatbot for a smart garden application. Answer clearly and helpfully.\",\n",
        "                    temperature=temperature,\n",
        "                    max_output_tokens=512,\n",
        "                ),\n",
        "            )\n",
        "            answer = (response.text or \"\").strip()\n",
        "        except Exception as e:\n",
        "            answer = f\"Error generating response: {e}\"\n",
        "        \n",
        "        if not answer:\n",
        "            answer = \"I couldn't generate an answer. Please try again.\"\n",
        "        \n",
        "        # Save to database\n",
        "        self._save_message(\"user\", user_message)\n",
        "        self._save_message(\"assistant\", answer)\n",
        "        \n",
        "        # Update local history\n",
        "        self.session_history.append((user_message, answer))\n",
        "        \n",
        "        return answer, self.session_history\n",
        "    \n",
        "    def clear_session(self) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Clear current session and start new one.\"\"\"\n",
        "        self.start_new_session()\n",
        "        return []\n",
        "    \n",
        "    def delete_session(self, session_id: str) -> bool:\n",
        "        \"\"\"Delete a session from Firebase.\"\"\"\n",
        "        return firebase.delete(f\"{self.FIREBASE_PATH}/{session_id}\")\n",
        "    \n",
        "    def get_session_history_formatted(self) -> str:\n",
        "        \"\"\"Get formatted history for display.\"\"\"\n",
        "        if not self.session_history:\n",
        "            return \"No messages yet.\"\n",
        "        \n",
        "        lines = []\n",
        "        for i, (user, assistant) in enumerate(self.session_history, 1):\n",
        "            lines.append(f\"[{i}] User: {user}\")\n",
        "            lines.append(f\"    Assistant: {assistant}\")\n",
        "            lines.append(\"\")\n",
        "        \n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# Create global chat service\n",
        "chat_service = ChatService()\n",
        "print(\"âœ… Chat service ready (with database persistence)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“„ 9. Document Fetcher (for RAG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# DOCUMENT FETCHER - Fetch and process documents for RAG\n",
        "# =============================================================================\n",
        "\n",
        "class DocumentFetcher:\n",
        "    \"\"\"Fetch and extract text from documents (HTML, PDF).\"\"\"\n",
        "    \n",
        "    HEADERS = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    }\n",
        "    \n",
        "    # Sections to skip in academic papers\n",
        "    SKIP_SECTIONS = {\n",
        "        \"references\", \"bibliography\", \"acknowledgements\", \"acknowledgments\",\n",
        "        \"author information\", \"ethics declarations\", \"additional information\",\n",
        "        \"supplementary information\", \"rights and permissions\", \"data availability\"\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def fetch_html(cls, url: str, timeout: int = 25) -> Tuple[str, str, int]:\n",
        "        \"\"\"Fetch HTML from URL.\"\"\"\n",
        "        try:\n",
        "            r = requests.get(url, headers=cls.HEADERS, timeout=timeout, allow_redirects=True)\n",
        "            return r.text or \"\", r.url, r.status_code\n",
        "        except Exception as e:\n",
        "            return \"\", url, 0\n",
        "    \n",
        "    @classmethod\n",
        "    def extract_text_from_html(cls, html: str) -> str:\n",
        "        \"\"\"Extract main text content from HTML.\"\"\"\n",
        "        if not html:\n",
        "            return \"\"\n",
        "        \n",
        "        try:\n",
        "            soup = BeautifulSoup(html, \"lxml\")\n",
        "        except:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "        \n",
        "        # Remove unwanted elements\n",
        "        for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"iframe\", \"nav\", \"footer\"]):\n",
        "            tag.decompose()\n",
        "        \n",
        "        # Try to find main content\n",
        "        root = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", class_=\"content\") or soup\n",
        "        \n",
        "        # Extract title\n",
        "        title = \"\"\n",
        "        if soup.title and soup.title.string:\n",
        "            title = soup.title.string.strip()\n",
        "        \n",
        "        # Extract description\n",
        "        desc = \"\"\n",
        "        meta = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "        if meta and meta.get(\"content\"):\n",
        "            desc = meta[\"content\"].strip()\n",
        "        \n",
        "        # Extract body text\n",
        "        chunks = []\n",
        "        for el in root.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"]):\n",
        "            text = el.get_text(\" \", strip=True)\n",
        "            if text and len(text) >= 30:\n",
        "                # Skip reference sections\n",
        "                text_lower = text.lower()\n",
        "                if any(skip in text_lower for skip in cls.SKIP_SECTIONS):\n",
        "                    continue\n",
        "                chunks.append(text)\n",
        "        \n",
        "        # Remove duplicates while preserving order\n",
        "        chunks = list(dict.fromkeys(chunks))\n",
        "        \n",
        "        # Build result\n",
        "        parts = []\n",
        "        if title:\n",
        "            parts.append(f\"TITLE: {title}\")\n",
        "        if desc:\n",
        "            parts.append(f\"DESCRIPTION: {desc}\")\n",
        "        if chunks:\n",
        "            parts.append(\"\\n\".join(chunks))\n",
        "        \n",
        "        return \"\\n\".join(parts).strip()\n",
        "    \n",
        "    @classmethod\n",
        "    def normalize_doi(cls, url: str) -> str:\n",
        "        \"\"\"Extract DOI from URL.\"\"\"\n",
        "        s = (url or \"\").strip()\n",
        "        s = s.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\")\n",
        "        return s.strip()\n",
        "    \n",
        "    @classmethod\n",
        "    def fetch_semantic_scholar(cls, doi: str) -> Dict:\n",
        "        \"\"\"Fetch metadata from Semantic Scholar.\"\"\"\n",
        "        try:\n",
        "            url = f\"https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}?fields=title,abstract,openAccessPdf\"\n",
        "            r = requests.get(url, timeout=15)\n",
        "            return r.json() if r.status_code == 200 else {}\n",
        "        except:\n",
        "            return {}\n",
        "    \n",
        "    @classmethod\n",
        "    def fetch_openalex(cls, doi: str) -> Dict:\n",
        "        \"\"\"Fetch metadata from OpenAlex.\"\"\"\n",
        "        try:\n",
        "            url = f\"https://api.openalex.org/works/https://doi.org/{doi}\"\n",
        "            r = requests.get(url, timeout=15)\n",
        "            if r.status_code == 200:\n",
        "                data = r.json()\n",
        "                return {\n",
        "                    \"title\": data.get(\"title\", \"\"),\n",
        "                    \"abstract\": data.get(\"abstract\", \"\"),\n",
        "                    \"pdf_url\": data.get(\"open_access\", {}).get(\"oa_url\", \"\")\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "        return {}\n",
        "    \n",
        "    @classmethod\n",
        "    def get_document_text(cls, url: str, min_chars: int = 500) -> str:\n",
        "        \"\"\"Get best available text for a document URL.\"\"\"\n",
        "        # Try HTML extraction first\n",
        "        html, final_url, status = cls.fetch_html(url)\n",
        "        text = cls.extract_text_from_html(html)\n",
        "        \n",
        "        if len(text) >= min_chars:\n",
        "            return text\n",
        "        \n",
        "        # Try academic APIs for DOI URLs\n",
        "        if \"doi.org/\" in url:\n",
        "            doi = cls.normalize_doi(url)\n",
        "            \n",
        "            # Try Semantic Scholar\n",
        "            ss = cls.fetch_semantic_scholar(doi)\n",
        "            if ss:\n",
        "                parts = []\n",
        "                if ss.get(\"title\"):\n",
        "                    parts.append(f\"TITLE: {ss['title']}\")\n",
        "                if ss.get(\"abstract\"):\n",
        "                    parts.append(f\"ABSTRACT: {ss['abstract']}\")\n",
        "                \n",
        "                api_text = \"\\n\".join(parts)\n",
        "                if len(api_text) > len(text):\n",
        "                    text = api_text\n",
        "            \n",
        "            # Try OpenAlex\n",
        "            if len(text) < min_chars:\n",
        "                oa = cls.fetch_openalex(doi)\n",
        "                if oa.get(\"abstract\"):\n",
        "                    parts = []\n",
        "                    if oa.get(\"title\"):\n",
        "                        parts.append(f\"TITLE: {oa['title']}\")\n",
        "                    parts.append(f\"ABSTRACT: {oa['abstract']}\")\n",
        "                    api_text = \"\\n\".join(parts)\n",
        "                    if len(api_text) > len(text):\n",
        "                        text = api_text\n",
        "        \n",
        "        return text\n",
        "\n",
        "\n",
        "print(\"âœ… Document fetcher ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” 10. RAG Service (Improved with TF-IDF + BM25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# RAG SERVICE - Improved retrieval with hybrid TF-IDF + BM25 ranking\n",
        "# =============================================================================\n",
        "\n",
        "class RAGService:\n",
        "    \"\"\"RAG service with improved search using TF-IDF + BM25 hybrid ranking.\"\"\"\n",
        "    \n",
        "    FIREBASE_INDEX_PATH = \"indexes/public_index\"\n",
        "    FIREBASE_DOCMAP_PATH = \"indexes/doc_map\"\n",
        "    FIREBASE_DOCTEXT_PATH = \"indexes/doc_text\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.documents: Dict[int, str] = {}  # doc_id -> full text\n",
        "        self.chunks: Dict[int, List[str]] = {}  # doc_id -> list of chunks\n",
        "        self.all_chunks: List[Tuple[int, int, str]] = []  # (doc_id, chunk_id, text)\n",
        "        self.doc_urls: Dict[int, str] = {}  # doc_id -> URL\n",
        "        self.inverted_index: Dict[str, List[int]] = {}  # term -> doc_ids\n",
        "        self.doc_lengths: Dict[int, int] = {}  # doc_id -> token count\n",
        "        self.avg_doc_length: float = 0\n",
        "        self.is_loaded: bool = False\n",
        "        \n",
        "        # TF-IDF components\n",
        "        self._tfidf_vectorizer: Optional[TfidfVectorizer] = None\n",
        "        self._chunk_vectors = None\n",
        "    \n",
        "    def build_index(self, urls: List[str], force_rebuild: bool = False) -> bool:\n",
        "        \"\"\"Build or load the RAG index.\"\"\"\n",
        "        \n",
        "        # Try to load from Firebase first\n",
        "        if not force_rebuild:\n",
        "            if self._load_from_firebase():\n",
        "                self._build_search_structures()\n",
        "                print(f\"âœ… Loaded index from Firebase ({len(self.documents)} docs)\")\n",
        "                return True\n",
        "        \n",
        "        print(\"Building new index...\")\n",
        "        \n",
        "        # Fetch documents\n",
        "        for i, url in enumerate(urls):\n",
        "            print(f\"  Fetching doc {i+1}/{len(urls)}: {url[:50]}...\")\n",
        "            text = DocumentFetcher.get_document_text(url)\n",
        "            self.documents[i] = text\n",
        "            self.doc_urls[i] = url\n",
        "            \n",
        "            # Create chunks\n",
        "            self.chunks[i] = nlp.chunk_text(text)\n",
        "        \n",
        "        # Build inverted index\n",
        "        self._build_inverted_index()\n",
        "        \n",
        "        # Save to Firebase\n",
        "        self._save_to_firebase()\n",
        "        \n",
        "        # Build search structures\n",
        "        self._build_search_structures()\n",
        "        \n",
        "        self.is_loaded = True\n",
        "        print(f\"âœ… Index built ({len(self.documents)} docs, {len(self.all_chunks)} chunks)\")\n",
        "        return True\n",
        "    \n",
        "    def _build_inverted_index(self) -> None:\n",
        "        \"\"\"Build inverted index for BM25.\"\"\"\n",
        "        self.inverted_index = defaultdict(set)\n",
        "        total_length = 0\n",
        "        \n",
        "        for doc_id, text in self.documents.items():\n",
        "            tokens = nlp.preprocess(text)\n",
        "            self.doc_lengths[doc_id] = len(tokens)\n",
        "            total_length += len(tokens)\n",
        "            \n",
        "            for term in set(tokens):\n",
        "                self.inverted_index[term].add(doc_id)\n",
        "        \n",
        "        # Convert sets to lists\n",
        "        self.inverted_index = {k: list(v) for k, v in self.inverted_index.items()}\n",
        "        self.avg_doc_length = total_length / max(len(self.documents), 1)\n",
        "    \n",
        "    def _build_search_structures(self) -> None:\n",
        "        \"\"\"Build TF-IDF vectors for chunk-level search.\"\"\"\n",
        "        # Flatten chunks\n",
        "        self.all_chunks = []\n",
        "        for doc_id, chunks in self.chunks.items():\n",
        "            for chunk_id, chunk in enumerate(chunks):\n",
        "                self.all_chunks.append((doc_id, chunk_id, chunk))\n",
        "        \n",
        "        if not self.all_chunks:\n",
        "            return\n",
        "        \n",
        "        # Build TF-IDF\n",
        "        chunk_texts = [c[2] for c in self.all_chunks]\n",
        "        self._tfidf_vectorizer = TfidfVectorizer(\n",
        "            preprocessor=nlp.preprocess_text,\n",
        "            max_features=5000,\n",
        "            ngram_range=(1, 2),\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        self._chunk_vectors = self._tfidf_vectorizer.fit_transform(chunk_texts)\n",
        "        self.is_loaded = True\n",
        "    \n",
        "    def _load_from_firebase(self) -> bool:\n",
        "        \"\"\"Load index from Firebase.\"\"\"\n",
        "        try:\n",
        "            doc_text = firebase.http_get(self.FIREBASE_DOCTEXT_PATH)\n",
        "            doc_map = firebase.http_get(self.FIREBASE_DOCMAP_PATH)\n",
        "            inv_index = firebase.http_get(self.FIREBASE_INDEX_PATH)\n",
        "            \n",
        "            if not doc_text or not doc_map:\n",
        "                return False\n",
        "            \n",
        "            # Convert keys to int\n",
        "            self.documents = {int(k): v for k, v in doc_text.items() if v}\n",
        "            \n",
        "            # Handle doc_map as list or dict\n",
        "            if isinstance(doc_map, list):\n",
        "                self.doc_urls = {i: url for i, url in enumerate(doc_map) if url}\n",
        "            else:\n",
        "                self.doc_urls = {int(k): v for k, v in doc_map.items() if v}\n",
        "            \n",
        "            if inv_index:\n",
        "                self.inverted_index = inv_index\n",
        "            \n",
        "            # Recreate chunks\n",
        "            for doc_id, text in self.documents.items():\n",
        "                self.chunks[doc_id] = nlp.chunk_text(text)\n",
        "            \n",
        "            # Rebuild doc lengths\n",
        "            total_length = 0\n",
        "            for doc_id, text in self.documents.items():\n",
        "                tokens = nlp.preprocess(text)\n",
        "                self.doc_lengths[doc_id] = len(tokens)\n",
        "                total_length += len(tokens)\n",
        "            self.avg_doc_length = total_length / max(len(self.documents), 1)\n",
        "            \n",
        "            return len(self.documents) > 0\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Firebase load error: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def _save_to_firebase(self) -> None:\n",
        "        \"\"\"Save index to Firebase.\"\"\"\n",
        "        try:\n",
        "            doc_text = {str(k): v for k, v in self.documents.items()}\n",
        "            doc_map = {str(k): v for k, v in self.doc_urls.items()}\n",
        "            \n",
        "            firebase.http_put(self.FIREBASE_DOCTEXT_PATH, doc_text)\n",
        "            firebase.http_put(self.FIREBASE_DOCMAP_PATH, doc_map)\n",
        "            firebase.http_put(self.FIREBASE_INDEX_PATH, self.inverted_index)\n",
        "            \n",
        "            print(\"âœ… Index saved to Firebase\")\n",
        "        except Exception as e:\n",
        "            print(f\"Firebase save error: {e}\")\n",
        "    \n",
        "    def _bm25_score(self, query_terms: List[str], doc_id: int) -> float:\n",
        "        \"\"\"Calculate BM25 score for a document.\"\"\"\n",
        "        k1, b = CONFIG.BM25_K1, CONFIG.BM25_B\n",
        "        N = len(self.documents)\n",
        "        doc_len = self.doc_lengths.get(doc_id, 1)\n",
        "        \n",
        "        score = 0.0\n",
        "        doc_text = self.documents.get(doc_id, \"\")\n",
        "        doc_tokens = nlp.preprocess(doc_text)\n",
        "        tf_counter = Counter(doc_tokens)\n",
        "        \n",
        "        for term in query_terms:\n",
        "            if term not in self.inverted_index:\n",
        "                continue\n",
        "            \n",
        "            df = len(self.inverted_index[term])\n",
        "            idf = math.log((N - df + 0.5) / (df + 0.5) + 1)\n",
        "            tf = tf_counter.get(term, 0)\n",
        "            \n",
        "            numerator = tf * (k1 + 1)\n",
        "            denominator = tf + k1 * (1 - b + b * (doc_len / self.avg_doc_length))\n",
        "            \n",
        "            score += idf * (numerator / denominator)\n",
        "        \n",
        "        return score\n",
        "    \n",
        "    def search(self, query: str, top_k: int = None) -> List[Dict]:\n",
        "        \"\"\"Hybrid search using TF-IDF + BM25.\"\"\"\n",
        "        if not self.is_loaded:\n",
        "            return []\n",
        "        \n",
        "        top_k = top_k or CONFIG.RAG_TOP_K\n",
        "        query_terms = nlp.preprocess(query)\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        # Method 1: BM25 document-level scoring\n",
        "        bm25_scores = {}\n",
        "        for doc_id in self.documents.keys():\n",
        "            score = self._bm25_score(query_terms, doc_id)\n",
        "            if score > 0:\n",
        "                bm25_scores[doc_id] = score\n",
        "        \n",
        "        # Method 2: TF-IDF chunk-level scoring\n",
        "        tfidf_scores = {}\n",
        "        if self._tfidf_vectorizer and self._chunk_vectors is not None:\n",
        "            query_vec = self._tfidf_vectorizer.transform([query])\n",
        "            similarities = cosine_similarity(query_vec, self._chunk_vectors).flatten()\n",
        "            \n",
        "            # Aggregate by document (max chunk score per doc)\n",
        "            for idx, sim in enumerate(similarities):\n",
        "                if sim > 0:\n",
        "                    doc_id = self.all_chunks[idx][0]\n",
        "                    if doc_id not in tfidf_scores or sim > tfidf_scores[doc_id]:\n",
        "                        tfidf_scores[doc_id] = float(sim)\n",
        "        \n",
        "        # Combine scores (normalize and weight)\n",
        "        all_docs = set(bm25_scores.keys()) | set(tfidf_scores.keys())\n",
        "        \n",
        "        max_bm25 = max(bm25_scores.values()) if bm25_scores else 1\n",
        "        max_tfidf = max(tfidf_scores.values()) if tfidf_scores else 1\n",
        "        \n",
        "        combined = []\n",
        "        for doc_id in all_docs:\n",
        "            bm25_norm = bm25_scores.get(doc_id, 0) / max_bm25\n",
        "            tfidf_norm = tfidf_scores.get(doc_id, 0) / max_tfidf\n",
        "            \n",
        "            # Weighted combination (60% BM25, 40% TF-IDF)\n",
        "            final_score = 0.6 * bm25_norm + 0.4 * tfidf_norm\n",
        "            combined.append((doc_id, final_score, bm25_scores.get(doc_id, 0), tfidf_scores.get(doc_id, 0)))\n",
        "        \n",
        "        # Sort by combined score\n",
        "        combined.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # Build results\n",
        "        for doc_id, combined_score, bm25, tfidf in combined[:top_k]:\n",
        "            results.append({\n",
        "                \"doc_id\": doc_id,\n",
        "                \"score\": combined_score,\n",
        "                \"bm25_score\": bm25,\n",
        "                \"tfidf_score\": tfidf,\n",
        "                \"url\": self.doc_urls.get(doc_id, \"\"),\n",
        "                \"title\": self._get_doc_title(doc_id),\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def search_chunks(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search at chunk level for more precise retrieval.\"\"\"\n",
        "        if not self._tfidf_vectorizer or self._chunk_vectors is None:\n",
        "            return []\n",
        "        \n",
        "        query_vec = self._tfidf_vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vec, self._chunk_vectors).flatten()\n",
        "        \n",
        "        # Get top chunks\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "        \n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0:\n",
        "                doc_id, chunk_id, chunk_text = self.all_chunks[idx]\n",
        "                results.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"score\": float(similarities[idx]),\n",
        "                    \"text\": chunk_text[:500],\n",
        "                    \"url\": self.doc_urls.get(doc_id, \"\"),\n",
        "                    \"title\": self._get_doc_title(doc_id),\n",
        "                })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _get_doc_title(self, doc_id: int) -> str:\n",
        "        \"\"\"Extract title from document.\"\"\"\n",
        "        text = self.documents.get(doc_id, \"\")\n",
        "        match = re.search(r\"^TITLE:\\s*(.+)$\", text, re.MULTILINE)\n",
        "        if match:\n",
        "            return match.group(1).strip()[:100]\n",
        "        return f\"Document {doc_id}\"\n",
        "    \n",
        "    def answer_question(self, question: str, top_k: int = 3) -> Dict:\n",
        "        \"\"\"Answer question using RAG with LLM.\"\"\"\n",
        "        # Search for relevant chunks\n",
        "        chunks = self.search_chunks(question, top_k=top_k * 2)\n",
        "        docs = self.search(question, top_k=top_k)\n",
        "        \n",
        "        if not chunks and not docs:\n",
        "            return {\n",
        "                \"answer\": \"No relevant documents found for your question.\",\n",
        "                \"sources\": [],\n",
        "                \"chunks\": []\n",
        "            }\n",
        "        \n",
        "        # Build context from top chunks\n",
        "        context_parts = []\n",
        "        seen_chunks = set()\n",
        "        for chunk in chunks[:5]:\n",
        "            chunk_key = (chunk[\"doc_id\"], chunk[\"chunk_id\"])\n",
        "            if chunk_key not in seen_chunks:\n",
        "                seen_chunks.add(chunk_key)\n",
        "                context_parts.append(f\"[Source: {chunk['title']}]\\n{chunk['text']}\")\n",
        "        \n",
        "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "        \n",
        "        # Generate answer with LLM\n",
        "        prompt = f\"\"\"Based on the following context, answer the question. \n",
        "If the answer is not in the context, say \"I don't have enough information to answer this.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            answer = AIClients.generate_gemini(prompt, temperature=0.3, max_tokens=512)\n",
        "        except:\n",
        "            answer = \"Error generating answer. Please try again.\"\n",
        "        \n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [d[\"url\"] for d in docs if d[\"url\"]],\n",
        "            \"chunks\": chunks,\n",
        "            \"docs\": docs\n",
        "        }\n",
        "\n",
        "\n",
        "# Create global RAG service\n",
        "rag_service = RAGService()\n",
        "print(\"âœ… RAG service ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ® 11. Gamification Service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# GAMIFICATION SERVICE - Points, missions, rewards\n",
        "# =============================================================================\n",
        "\n",
        "class GamificationService:\n",
        "    \"\"\"Gamification system with Firebase persistence.\"\"\"\n",
        "    \n",
        "    FIREBASE_PATH = \"gamification/global\"\n",
        "    \n",
        "    WHEEL_REWARDS = [\n",
        "        (\"+5 points\", {\"points\": 5}),\n",
        "        (\"+10 points\", {\"points\": 10}),\n",
        "        (\"+20 points\", {\"points\": 20}),\n",
        "        (\"Coupon: 5% off\", {\"coupon\": {\"code\": \"CG-5OFF\", \"label\": \"5% off\"}}),\n",
        "        (\"Coupon: 10% off\", {\"coupon\": {\"code\": \"CG-10OFF\", \"label\": \"10% off\"}}),\n",
        "        (\"+50 points\", {\"points\": 50}),\n",
        "    ]\n",
        "    \n",
        "    DEFAULT_PROFILE = {\n",
        "        \"points\": 0,\n",
        "        \"spins_available\": 0,\n",
        "        \"missions\": {\n",
        "            \"sync_data\": {\"last_completed\": None, \"total_completed\": 0},\n",
        "            \"analyze_plant\": {\"last_completed\": None, \"total_completed\": 0},\n",
        "            \"generate_report\": {\"last_completed\": None, \"total_completed\": 0},\n",
        "        },\n",
        "        \"coupons\": []\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def _today_key(cls) -> str:\n",
        "        return datetime.now(ZoneInfo(CONFIG.TIMEZONE)).strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    @classmethod\n",
        "    def _now_iso(cls) -> str:\n",
        "        return datetime.now(ZoneInfo(CONFIG.TIMEZONE)).isoformat()\n",
        "    \n",
        "    @classmethod\n",
        "    def get_profile(cls) -> Dict:\n",
        "        \"\"\"Get current gamification profile.\"\"\"\n",
        "        data = firebase.get(cls.FIREBASE_PATH) or {}\n",
        "        \n",
        "        profile = {\n",
        "            \"points\": int(data.get(\"points\", 0)),\n",
        "            \"spins_available\": int(data.get(\"spins_available\", 0)),\n",
        "            \"missions\": data.get(\"missions\", {}) or {},\n",
        "            \"coupons\": data.get(\"coupons\", []) or [],\n",
        "        }\n",
        "        \n",
        "        # Merge default missions\n",
        "        for mid, base in cls.DEFAULT_PROFILE[\"missions\"].items():\n",
        "            if mid not in profile[\"missions\"]:\n",
        "                profile[\"missions\"][mid] = dict(base)\n",
        "        \n",
        "        return profile\n",
        "    \n",
        "    @classmethod\n",
        "    def save_profile(cls, profile: Dict) -> None:\n",
        "        \"\"\"Save profile to Firebase.\"\"\"\n",
        "        firebase.set(cls.FIREBASE_PATH, profile)\n",
        "    \n",
        "    @classmethod\n",
        "    def complete_mission(cls, mission_id: str, points: int) -> Tuple[Dict, bool]:\n",
        "        \"\"\"Complete a mission (once per day). Returns (profile, earned_today).\"\"\"\n",
        "        profile = cls.get_profile()\n",
        "        today = cls._today_key()\n",
        "        \n",
        "        mission = profile[\"missions\"].get(mission_id, {\"last_completed\": None, \"total_completed\": 0})\n",
        "        \n",
        "        # Check if already completed today\n",
        "        if mission.get(\"last_completed\") == today:\n",
        "            return profile, False\n",
        "        \n",
        "        # Award points and spin\n",
        "        profile[\"points\"] += points\n",
        "        profile[\"spins_available\"] += 1\n",
        "        \n",
        "        mission[\"last_completed\"] = today\n",
        "        mission[\"total_completed\"] = mission.get(\"total_completed\", 0) + 1\n",
        "        profile[\"missions\"][mission_id] = mission\n",
        "        \n",
        "        cls.save_profile(profile)\n",
        "        return profile, True\n",
        "    \n",
        "    @classmethod\n",
        "    def spin_wheel(cls) -> Tuple[Dict, str]:\n",
        "        \"\"\"Spin the wheel if spins available. Returns (profile, reward_message).\"\"\"\n",
        "        profile = cls.get_profile()\n",
        "        \n",
        "        if profile[\"spins_available\"] <= 0:\n",
        "            return profile, \"No spins available! Complete missions to earn spins.\"\n",
        "        \n",
        "        profile[\"spins_available\"] -= 1\n",
        "        \n",
        "        # Random reward\n",
        "        reward_label, reward_data = random.choice(cls.WHEEL_REWARDS)\n",
        "        \n",
        "        if \"points\" in reward_data:\n",
        "            profile[\"points\"] += reward_data[\"points\"]\n",
        "        \n",
        "        if \"coupon\" in reward_data:\n",
        "            coupon = {\n",
        "                \"code\": f\"{reward_data['coupon']['code']}-{random.randint(1000,9999)}\",\n",
        "                \"label\": reward_data['coupon']['label'],\n",
        "                \"created_at\": cls._now_iso(),\n",
        "                \"redeemed\": False\n",
        "            }\n",
        "            profile[\"coupons\"].append(coupon)\n",
        "        \n",
        "        cls.save_profile(profile)\n",
        "        return profile, f\"ðŸŽ‰ You won: {reward_label}!\"\n",
        "    \n",
        "    @classmethod\n",
        "    def get_status_html(cls) -> str:\n",
        "        \"\"\"Get HTML status display.\"\"\"\n",
        "        profile = cls.get_profile()\n",
        "        today = cls._today_key()\n",
        "        \n",
        "        missions_html = \"\"\n",
        "        mission_names = {\n",
        "            \"sync_data\": \"Sync Data\",\n",
        "            \"analyze_plant\": \"Analyze Plant\",\n",
        "            \"generate_report\": \"Generate Report\"\n",
        "        }\n",
        "        \n",
        "        for mid, mdata in profile[\"missions\"].items():\n",
        "            completed_today = mdata.get(\"last_completed\") == today\n",
        "            status = \"âœ…\" if completed_today else \"â¬œ\"\n",
        "            name = mission_names.get(mid, mid)\n",
        "            missions_html += f\"<div>{status} {name}</div>\"\n",
        "        \n",
        "        return f\"\"\"\n",
        "        <div style='padding:15px; background:#f0f8ff; border-radius:10px;'>\n",
        "            <h3>ðŸŽ® Your Stats</h3>\n",
        "            <p><b>Points:</b> {profile['points']}</p>\n",
        "            <p><b>Spins Available:</b> {profile['spins_available']}</p>\n",
        "            <h4>Daily Missions:</h4>\n",
        "            {missions_html}\n",
        "            <p><small>Complete missions to earn points & spins!</small></p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "# Create alias for backward compatibility\n",
        "gamification = GamificationService()\n",
        "print(\"âœ… Gamification service ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“„ 12. Report Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# REPORT GENERATOR - Create DOCX reports\n",
        "# =============================================================================\n",
        "\n",
        "class ReportGenerator:\n",
        "    \"\"\"Generate DOCX reports from sensor data.\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def unify_sensor_dfs(dfs: Dict) -> pd.DataFrame:\n",
        "        \"\"\"Unify multiple sensor dataframes into one.\"\"\"\n",
        "        def prep(df, col):\n",
        "            if df is None or df.empty:\n",
        "                return pd.DataFrame(columns=[\"timestamp\", col])\n",
        "            \n",
        "            out = df.copy()\n",
        "            \n",
        "            # Normalize column names\n",
        "            if \"timestamp\" not in out.columns and \"created_at\" in out.columns:\n",
        "                out = out.rename(columns={\"created_at\": \"timestamp\"})\n",
        "            \n",
        "            if \"timestamp\" not in out.columns:\n",
        "                if out.index.name:\n",
        "                    out = out.reset_index()\n",
        "                else:\n",
        "                    out = out.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
        "            \n",
        "            if \"timestamp\" not in out.columns or \"value\" not in out.columns:\n",
        "                return pd.DataFrame(columns=[\"timestamp\", col])\n",
        "            \n",
        "            out = out[[\"timestamp\", \"value\"]]\n",
        "            \n",
        "            # Convert timestamp\n",
        "            ts = out[\"timestamp\"]\n",
        "            if pd.api.types.is_numeric_dtype(ts):\n",
        "                unit = \"ms\" if ts.median() > 1e12 else \"s\"\n",
        "                out[\"timestamp\"] = pd.to_datetime(ts, unit=unit, utc=True).dt.tz_convert(CONFIG.TIMEZONE).dt.tz_localize(None)\n",
        "            else:\n",
        "                out[\"timestamp\"] = pd.to_datetime(ts, errors=\"coerce\", utc=True).dt.tz_convert(CONFIG.TIMEZONE).dt.tz_localize(None)\n",
        "            \n",
        "            out = out.dropna(subset=[\"timestamp\"])\n",
        "            out[\"value\"] = pd.to_numeric(out[\"value\"], errors=\"coerce\")\n",
        "            out = out.dropna(subset=[\"value\"])\n",
        "            out = out.rename(columns={\"value\": col})\n",
        "            \n",
        "            return out\n",
        "        \n",
        "        t = prep(dfs.get(\"temperature\"), \"temperature\")\n",
        "        h = prep(dfs.get(\"humidity\"), \"humidity\")\n",
        "        s = prep(dfs.get(\"soil\"), \"soil\")\n",
        "        \n",
        "        df = t.merge(h, on=\"timestamp\", how=\"outer\").merge(s, on=\"timestamp\", how=\"outer\")\n",
        "        return df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate_ai_summary(df: pd.DataFrame) -> str:\n",
        "        \"\"\"Generate AI summary of sensor data.\"\"\"\n",
        "        if df.empty:\n",
        "            return \"No data available for analysis.\"\n",
        "        \n",
        "        # Get last 24 hours\n",
        "        try:\n",
        "            cutoff = df[\"timestamp\"].max() - timedelta(hours=24)\n",
        "            daily = df[df[\"timestamp\"] >= cutoff]\n",
        "        except:\n",
        "            daily = df.tail(100)\n",
        "        \n",
        "        if daily.empty:\n",
        "            daily = df.tail(50)\n",
        "        \n",
        "        # Calculate statistics\n",
        "        stats = {}\n",
        "        for col in [\"temperature\", \"humidity\", \"soil\"]:\n",
        "            if col in daily.columns:\n",
        "                stats[col] = {\n",
        "                    \"mean\": daily[col].mean(),\n",
        "                    \"min\": daily[col].min(),\n",
        "                    \"max\": daily[col].max(),\n",
        "                    \"std\": daily[col].std()\n",
        "                }\n",
        "        \n",
        "        prompt = f\"\"\"Generate a brief daily summary for a smart garden monitoring system.\n",
        "\n",
        "Sensor Statistics (last 24 hours):\n",
        "- Temperature: avg={stats.get('temperature', {}).get('mean', 0):.1f}Â°C, range={stats.get('temperature', {}).get('min', 0):.1f}-{stats.get('temperature', {}).get('max', 0):.1f}Â°C\n",
        "- Humidity: avg={stats.get('humidity', {}).get('mean', 0):.1f}%, range={stats.get('humidity', {}).get('min', 0):.1f}-{stats.get('humidity', {}).get('max', 0):.1f}%\n",
        "- Soil Moisture: avg={stats.get('soil', {}).get('mean', 0):.1f}%, range={stats.get('soil', {}).get('min', 0):.1f}-{stats.get('soil', {}).get('max', 0):.1f}%\n",
        "\n",
        "Healthy ranges: Temperature 18-32Â°C, Humidity 35-75%, Soil 20-60%\n",
        "\n",
        "Write 2-3 sentences summarizing plant health status and any recommendations.\"\"\"\n",
        "        \n",
        "        try:\n",
        "            return AIClients.generate_cerebras(prompt)\n",
        "        except:\n",
        "            return \"AI summary unavailable.\"\n",
        "    \n",
        "    @classmethod\n",
        "    def create_docx(cls, dfs: Dict, limit: int = 50) -> str:\n",
        "        \"\"\"Create DOCX report file. Returns file path.\"\"\"\n",
        "        df = cls.unify_sensor_dfs(dfs)\n",
        "        \n",
        "        if df.empty:\n",
        "            raise ValueError(\"No data available for report\")\n",
        "        \n",
        "        doc = Document()\n",
        "        \n",
        "        # Title\n",
        "        title = doc.add_heading(\"CloudGarden Daily Report\", 0)\n",
        "        title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "        \n",
        "        # Date\n",
        "        date_para = doc.add_paragraph()\n",
        "        date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "        date_para.add_run(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\").italic = True\n",
        "        \n",
        "        doc.add_paragraph()\n",
        "        \n",
        "        # AI Summary\n",
        "        doc.add_heading(\"Executive Summary\", level=1)\n",
        "        summary = cls.generate_ai_summary(df)\n",
        "        doc.add_paragraph(summary)\n",
        "        \n",
        "        # Statistics Table\n",
        "        doc.add_heading(\"Environmental Conditions\", level=1)\n",
        "        \n",
        "        table = doc.add_table(rows=1, cols=5)\n",
        "        table.style = \"Table Grid\"\n",
        "        \n",
        "        headers = [\"Metric\", \"Current\", \"Average\", \"Min\", \"Max\"]\n",
        "        for i, header in enumerate(headers):\n",
        "            table.rows[0].cells[i].text = header\n",
        "        \n",
        "        for col, unit in [(\"temperature\", \"Â°C\"), (\"humidity\", \"%\"), (\"soil\", \"%\")]:\n",
        "            if col in df.columns:\n",
        "                row = table.add_row()\n",
        "                row.cells[0].text = col.capitalize()\n",
        "                row.cells[1].text = f\"{df[col].iloc[-1]:.1f}{unit}\"\n",
        "                row.cells[2].text = f\"{df[col].mean():.1f}{unit}\"\n",
        "                row.cells[3].text = f\"{df[col].min():.1f}{unit}\"\n",
        "                row.cells[4].text = f\"{df[col].max():.1f}{unit}\"\n",
        "        \n",
        "        # Save\n",
        "        fd, path = tempfile.mkstemp(suffix=\".docx\")\n",
        "        os.close(fd)\n",
        "        doc.save(path)\n",
        "        \n",
        "        return path\n",
        "\n",
        "\n",
        "print(\"âœ… Report generator ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š 13. Dashboard Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# DASHBOARD FUNCTIONS - IoT visualization\n",
        "# =============================================================================\n",
        "\n",
        "def create_kpi_card(label: str, value: str, color: str, trend: str = \"\") -> str:\n",
        "    \"\"\"Create HTML KPI card.\"\"\"\n",
        "    trend_html = f\"<span class='trend-{'up' if 'â†‘' in trend else 'down'}'>{trend}</span>\" if trend else \"\"\n",
        "    return f\"\"\"\n",
        "    <div class='kpi-card' style='border-left-color:{color};'>\n",
        "        <div class='kpi-label'>{label}</div>\n",
        "        <div class='kpi-value'>{value}</div>\n",
        "        {trend_html}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def dashboard_screen():\n",
        "    \"\"\"Generate full dashboard visualizations.\"\"\"\n",
        "    df = sensor_service.load_from_firebase()\n",
        "    \n",
        "    if df.empty:\n",
        "        empty_msg = \"No data available. Please sync data first.\"\n",
        "        return empty_msg, empty_msg, None, empty_msg, None, empty_msg, None, empty_msg, None, empty_msg, None\n",
        "    \n",
        "    # KPI Cards\n",
        "    latest = df.iloc[-1]\n",
        "    kpi_html = f\"\"\"\n",
        "    <div style='display:flex; gap:20px; flex-wrap:wrap;'>\n",
        "        {create_kpi_card('Temperature', f\"{latest['temperature']:.1f}Â°C\", CONFIG.COLOR_TEMP)}\n",
        "        {create_kpi_card('Humidity', f\"{latest['humidity']:.1f}%\", CONFIG.COLOR_HUMIDITY)}\n",
        "        {create_kpi_card('Soil Moisture', f\"{latest['soil']:.1f}%\", CONFIG.COLOR_SOIL)}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    # Stats\n",
        "    stats_html = f\"\"\"\n",
        "    <div style='display:flex; gap:20px; flex-wrap:wrap;'>\n",
        "        <div style='padding:15px; background:#f5f5f5; border-radius:8px; flex:1;'>\n",
        "            <b>Temperature</b><br>\n",
        "            Avg: {df['temperature'].mean():.1f}Â°C | \n",
        "            Min: {df['temperature'].min():.1f}Â°C | \n",
        "            Max: {df['temperature'].max():.1f}Â°C\n",
        "        </div>\n",
        "        <div style='padding:15px; background:#f5f5f5; border-radius:8px; flex:1;'>\n",
        "            <b>Humidity</b><br>\n",
        "            Avg: {df['humidity'].mean():.1f}% | \n",
        "            Min: {df['humidity'].min():.1f}% | \n",
        "            Max: {df['humidity'].max():.1f}%\n",
        "        </div>\n",
        "        <div style='padding:15px; background:#f5f5f5; border-radius:8px; flex:1;'>\n",
        "            <b>Soil</b><br>\n",
        "            Avg: {df['soil'].mean():.1f}% | \n",
        "            Min: {df['soil'].min():.1f}% | \n",
        "            Max: {df['soil'].max():.1f}%\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    # Time Series Plot\n",
        "    fig_ts = make_subplots(rows=3, cols=1, shared_xaxes=True, subplot_titles=(\"Temperature\", \"Humidity\", \"Soil Moisture\"))\n",
        "    fig_ts.add_trace(go.Scatter(x=df['timestamp'], y=df['temperature'], name='Temp', line=dict(color=CONFIG.COLOR_TEMP)), row=1, col=1)\n",
        "    fig_ts.add_trace(go.Scatter(x=df['timestamp'], y=df['humidity'], name='Humidity', line=dict(color=CONFIG.COLOR_HUMIDITY)), row=2, col=1)\n",
        "    fig_ts.add_trace(go.Scatter(x=df['timestamp'], y=df['soil'], name='Soil', line=dict(color=CONFIG.COLOR_SOIL)), row=3, col=1)\n",
        "    fig_ts.update_layout(height=600, showlegend=False)\n",
        "    \n",
        "    # Correlation\n",
        "    corr = df[['temperature', 'humidity', 'soil']].corr()\n",
        "    fig_corr = px.imshow(corr, text_auto='.2f', color_continuous_scale='RdBu_r', title=\"Correlation Matrix\")\n",
        "    corr_html = \"<p>Shows relationships between sensor readings</p>\"\n",
        "    \n",
        "    # Hourly patterns\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    hourly = df.groupby('hour')[['temperature', 'humidity', 'soil']].mean()\n",
        "    fig_hourly = go.Figure()\n",
        "    fig_hourly.add_trace(go.Bar(x=hourly.index, y=hourly['temperature'], name='Temp'))\n",
        "    fig_hourly.add_trace(go.Bar(x=hourly.index, y=hourly['humidity'], name='Humidity'))\n",
        "    fig_hourly.update_layout(title=\"Average by Hour\", barmode='group')\n",
        "    hourly_html = \"<p>Average sensor values by hour of day</p>\"\n",
        "    \n",
        "    # Daily trends\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "    daily = df.groupby('date')[['temperature', 'humidity', 'soil']].mean()\n",
        "    fig_daily = go.Figure()\n",
        "    for col, color in [('temperature', CONFIG.COLOR_TEMP), ('humidity', CONFIG.COLOR_HUMIDITY), ('soil', CONFIG.COLOR_SOIL)]:\n",
        "        fig_daily.add_trace(go.Scatter(x=daily.index, y=daily[col], name=col.capitalize(), line=dict(color=color)))\n",
        "    fig_daily.update_layout(title=\"Daily Averages\")\n",
        "    daily_html = \"<p>Daily average trends</p>\"\n",
        "    \n",
        "    # Distribution\n",
        "    fig_dist = make_subplots(rows=1, cols=3, subplot_titles=(\"Temperature\", \"Humidity\", \"Soil\"))\n",
        "    fig_dist.add_trace(go.Histogram(x=df['temperature'], nbinsx=20, marker_color=CONFIG.COLOR_TEMP), row=1, col=1)\n",
        "    fig_dist.add_trace(go.Histogram(x=df['humidity'], nbinsx=20, marker_color=CONFIG.COLOR_HUMIDITY), row=1, col=2)\n",
        "    fig_dist.add_trace(go.Histogram(x=df['soil'], nbinsx=20, marker_color=CONFIG.COLOR_SOIL), row=1, col=3)\n",
        "    fig_dist.update_layout(height=300, showlegend=False)\n",
        "    dist_html = \"<p>Distribution of sensor readings</p>\"\n",
        "    \n",
        "    return kpi_html, stats_html, fig_ts, corr_html, fig_corr, hourly_html, fig_hourly, daily_html, fig_daily, dist_html, fig_dist\n",
        "\n",
        "\n",
        "def dashboard_moving_avg(variable: str):\n",
        "    \"\"\"Calculate moving averages.\"\"\"\n",
        "    df = sensor_service.load_from_firebase()\n",
        "    \n",
        "    if df.empty or variable not in df.columns:\n",
        "        return \"No data\", None\n",
        "    \n",
        "    df['MA_7'] = df[variable].rolling(window=7, min_periods=1).mean()\n",
        "    df['MA_24'] = df[variable].rolling(window=24, min_periods=1).mean()\n",
        "    \n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=df['timestamp'], y=df[variable], name='Raw', opacity=0.5))\n",
        "    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['MA_7'], name='MA-7'))\n",
        "    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['MA_24'], name='MA-24'))\n",
        "    fig.update_layout(title=f\"{variable.capitalize()} Moving Averages\")\n",
        "    \n",
        "    return f\"Moving averages for {variable}\", fig\n",
        "\n",
        "\n",
        "print(\"âœ… Dashboard functions ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŒ¿ 14. Plant Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# PLANT ANALYSIS - Disease detection and recommendations\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_plant_image(image, temp: float, humidity: float, soil: float):\n",
        "    \"\"\"Analyze plant image and sensor data.\"\"\"\n",
        "    # Run image classification\n",
        "    preds = plant_classifier(image)\n",
        "    top = preds[0]\n",
        "    label = top[\"label\"]\n",
        "    score = top[\"score\"]\n",
        "    \n",
        "    # Check sensor conditions\n",
        "    alerts = []\n",
        "    advice = []\n",
        "    \n",
        "    # Temperature checks\n",
        "    if temp < CONFIG.TEMP_MIN:\n",
        "        alerts.append(\"Low temperature\")\n",
        "        advice.append(\"Move plant to a warmer environment\")\n",
        "    elif temp > CONFIG.TEMP_MAX:\n",
        "        alerts.append(\"High temperature\")\n",
        "        advice.append(\"Move plant to a shaded area\")\n",
        "    \n",
        "    # Humidity checks\n",
        "    if humidity < CONFIG.HUMIDITY_MIN:\n",
        "        alerts.append(\"Low air humidity\")\n",
        "        advice.append(\"Increase humidity (misting)\")\n",
        "    elif humidity > CONFIG.HUMIDITY_MAX:\n",
        "        alerts.append(\"High air humidity\")\n",
        "        advice.append(\"Improve ventilation\")\n",
        "    \n",
        "    # Soil checks\n",
        "    if soil < CONFIG.SOIL_MIN:\n",
        "        alerts.append(\"Low soil moisture\")\n",
        "        advice.append(\"Water the plant\")\n",
        "    elif soil > CONFIG.SOIL_MAX:\n",
        "        alerts.append(\"High soil moisture\")\n",
        "        advice.append(\"Reduce watering\")\n",
        "    \n",
        "    # Determine status based on image\n",
        "    is_bad = \"healthy\" not in label.lower()\n",
        "    \n",
        "    status_html = f\"\"\"\n",
        "    <div style='padding:15px; border-radius:10px; \n",
        "         background:{'#ffdddd' if is_bad else '#ddffdd'};\n",
        "         border:2px solid {'#ff0000' if is_bad else '#00aa00'};\n",
        "         font-weight:bold; font-size:18px;'>\n",
        "        {'ðŸ”´ Plant Status: NEEDS ATTENTION' if is_bad else 'ðŸŸ¢ Plant Status: HEALTHY'}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    if not alerts:\n",
        "        alerts.append(\"All sensor readings normal\")\n",
        "    \n",
        "    return (\n",
        "        f\"Detected: {label} ({score:.1%} confidence)\",\n",
        "        status_html,\n",
        "        \"\\n\".join(f\"âš ï¸ {a}\" for a in alerts),\n",
        "        \"\\n\".join(f\"ðŸ’¡ {a}\" for a in advice) if advice else \"No immediate actions needed.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Gamified wrapper\n",
        "def analyze_plant_gamified(image, temp, humidity, soil):\n",
        "    \"\"\"Analyze plant with gamification reward.\"\"\"\n",
        "    result = analyze_plant_image(image, temp, humidity, soil)\n",
        "    \n",
        "    # Award mission completion\n",
        "    profile, earned = GamificationService.complete_mission(\"analyze_plant\", 15)\n",
        "    if earned:\n",
        "        print(f\"ðŸŽ® +15 points! Total: {profile['points']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"âœ… Plant analysis functions ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ 15. Gamified Action Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# GAMIFIED WRAPPERS - Add points to actions\n",
        "# =============================================================================\n",
        "\n",
        "def sync_screen_gamified():\n",
        "    \"\"\"Sync data with gamification.\"\"\"\n",
        "    msg, count = sensor_service.sync_to_firebase()\n",
        "    \n",
        "    if count > 0:\n",
        "        profile, earned = GamificationService.complete_mission(\"sync_data\", 10)\n",
        "        if earned:\n",
        "            msg += f\"\\n\\nðŸŽ® Mission Complete! +10 points (Total: {profile['points']})\"\n",
        "    \n",
        "    return msg\n",
        "\n",
        "\n",
        "def generate_report_screen_gamified(samples: int):\n",
        "    \"\"\"Generate report with gamification.\"\"\"\n",
        "    try:\n",
        "        # Fetch sensor data\n",
        "        dfs = {\n",
        "            \"temperature\": sensor_service.fetch_from_server(\"temperature\", samples),\n",
        "            \"humidity\": sensor_service.fetch_from_server(\"humidity\", samples),\n",
        "            \"soil\": sensor_service.fetch_from_server(\"soil\", samples),\n",
        "        }\n",
        "        \n",
        "        # Generate report\n",
        "        path = ReportGenerator.create_docx(dfs, samples)\n",
        "        \n",
        "        # Award mission\n",
        "        profile, earned = GamificationService.complete_mission(\"generate_report\", 20)\n",
        "        \n",
        "        status = f\"âœ… Report generated successfully!\"\n",
        "        if earned:\n",
        "            status += f\"\\nðŸŽ® Mission Complete! +20 points (Total: {profile['points']})\"\n",
        "        \n",
        "        return status, path\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {e}\", None\n",
        "\n",
        "\n",
        "print(\"âœ… Gamified wrappers ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ 16. Initialize RAG Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# INITIALIZE RAG INDEX\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading RAG index...\")\n",
        "rag_service.build_index(CONFIG.DOC_URLS)\n",
        "print(f\"âœ… RAG ready with {len(rag_service.documents)} documents\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¨ 17. UI Tab Builders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# UI TAB BUILDERS - Gradio interface components\n",
        "# =============================================================================\n",
        "\n",
        "def build_realtime_dashboard_tab():\n",
        "    \"\"\"Build the realtime dashboard tab.\"\"\"\n",
        "    gr.Markdown(\"### ðŸŒ¿ Real-Time Plant Status\")\n",
        "    \n",
        "    samples = gr.Slider(1, 200, value=20, step=1, label=\"Number of Samples\")\n",
        "    btn = gr.Button(\"Update Dashboard\", variant=\"primary\")\n",
        "    \n",
        "    status = gr.Textbox(label=\"Status\", lines=1)\n",
        "    details = gr.Textbox(label=\"Details\", lines=4)\n",
        "    \n",
        "    with gr.Row():\n",
        "        gr.Markdown(f\"\"\"\n",
        "        <div style='padding:14px; border:1px solid #ddd; border-radius:10px;'>\n",
        "            <h4>ðŸŒ¿ Status Legend</h4>\n",
        "            <span style='color:{CONFIG.COLOR_OK}; font-size:20px;'>â—</span> Healthy - All normal<br>\n",
        "            <span style='color:{CONFIG.COLOR_WARN}; font-size:20px;'>â—</span> Warning - Near limits<br>\n",
        "            <span style='color:{CONFIG.COLOR_BAD}; font-size:20px;'>â—</span> Not OK - Out of range\n",
        "        </div>\n",
        "        \"\"\")\n",
        "        \n",
        "        gr.Markdown(f\"\"\"\n",
        "        <div style='padding:14px; border:1px solid #ddd; border-radius:10px;'>\n",
        "            <h4>â„¹ï¸ Valid Ranges</h4>\n",
        "            <span style='color:{CONFIG.COLOR_TEMP}; font-size:20px;'>â—</span> Temperature: {CONFIG.TEMP_MIN}-{CONFIG.TEMP_MAX}Â°C<br>\n",
        "            <span style='color:{CONFIG.COLOR_HUMIDITY}; font-size:20px;'>â—</span> Humidity: {CONFIG.HUMIDITY_MIN}-{CONFIG.HUMIDITY_MAX}%<br>\n",
        "            <span style='color:{CONFIG.COLOR_SOIL}; font-size:20px;'>â—</span> Soil: {CONFIG.SOIL_MIN}-{CONFIG.SOIL_MAX}%\n",
        "        </div>\n",
        "        \"\"\")\n",
        "    \n",
        "    gr.Markdown(\"### ðŸ“ˆ Sensor Graphs\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        plot_temp = gr.Plot(label=\"Temperature\")\n",
        "        plot_hum = gr.Plot(label=\"Humidity\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        plot_soil = gr.Plot(label=\"Soil Moisture\")\n",
        "        plot_combined = gr.Plot(label=\"Combined\")\n",
        "    \n",
        "    def update_dashboard(limit):\n",
        "        dfs = {\n",
        "            \"temperature\": sensor_service.fetch_from_server(\"temperature\", limit),\n",
        "            \"humidity\": sensor_service.fetch_from_server(\"humidity\", limit),\n",
        "            \"soil\": sensor_service.fetch_from_server(\"soil\", limit),\n",
        "        }\n",
        "        \n",
        "        missing = [k for k, v in dfs.items() if v is None]\n",
        "        if missing:\n",
        "            return \"âš ï¸ Partial Data\", f\"Missing: {', '.join(missing)}\", None, None, None, None\n",
        "        \n",
        "        temp_val = float(dfs[\"temperature\"][\"value\"].iloc[-1])\n",
        "        hum_val = float(dfs[\"humidity\"][\"value\"].iloc[-1])\n",
        "        soil_val = float(dfs[\"soil\"][\"value\"].iloc[-1])\n",
        "        \n",
        "        status_text, color, details_list = sensor_service.check_plant_status(temp_val, hum_val, soil_val)\n",
        "        \n",
        "        details_text = f\"{'; '.join(details_list)}\\nLatest: temp={temp_val:.1f}, hum={hum_val:.1f}, soil={soil_val:.1f}\"\n",
        "        \n",
        "        # Create plots\n",
        "        fig_t = plt.figure(figsize=(7, 3))\n",
        "        plt.plot(dfs[\"temperature\"][\"created_at\"], dfs[\"temperature\"][\"value\"], color=CONFIG.COLOR_TEMP)\n",
        "        plt.title(\"Temperature\")\n",
        "        plt.grid(True)\n",
        "        plt.close()\n",
        "        \n",
        "        fig_h = plt.figure(figsize=(7, 3))\n",
        "        plt.plot(dfs[\"humidity\"][\"created_at\"], dfs[\"humidity\"][\"value\"], color=CONFIG.COLOR_HUMIDITY)\n",
        "        plt.title(\"Humidity\")\n",
        "        plt.grid(True)\n",
        "        plt.close()\n",
        "        \n",
        "        fig_s = plt.figure(figsize=(7, 3))\n",
        "        plt.plot(dfs[\"soil\"][\"created_at\"], dfs[\"soil\"][\"value\"], color=CONFIG.COLOR_SOIL)\n",
        "        plt.title(\"Soil Moisture\")\n",
        "        plt.grid(True)\n",
        "        plt.close()\n",
        "        \n",
        "        # Combined normalized\n",
        "        fig_c = plt.figure(figsize=(7, 3))\n",
        "        for name, df, color in [(\"Temp\", dfs[\"temperature\"], CONFIG.COLOR_TEMP),\n",
        "                                 (\"Humidity\", dfs[\"humidity\"], CONFIG.COLOR_HUMIDITY),\n",
        "                                 (\"Soil\", dfs[\"soil\"], CONFIG.COLOR_SOIL)]:\n",
        "            vals = df[\"value\"]\n",
        "            normalized = (vals - vals.min()) / (vals.max() - vals.min() + 1e-8)\n",
        "            plt.plot(df[\"created_at\"], normalized, label=name, color=color)\n",
        "        plt.legend()\n",
        "        plt.title(\"Combined (Normalized)\")\n",
        "        plt.grid(True)\n",
        "        plt.close()\n",
        "        \n",
        "        return status_text, details_text, fig_t, fig_h, fig_s, fig_c\n",
        "    \n",
        "    btn.click(update_dashboard, inputs=[samples], outputs=[status, details, plot_temp, plot_hum, plot_soil, plot_combined])\n",
        "\n",
        "\n",
        "def build_iot_dashboard_tab():\n",
        "    \"\"\"Build comprehensive IoT analytics dashboard.\"\"\"\n",
        "    gr.Markdown(\"### ðŸ“ˆ Comprehensive Sensor Analytics\")\n",
        "    \n",
        "    refresh_btn = gr.Button(\"ðŸ”„ Refresh All Data\", variant=\"primary\")\n",
        "    \n",
        "    gr.Markdown(\"#### ðŸ“Œ Current Readings\")\n",
        "    kpi_html = gr.HTML()\n",
        "    \n",
        "    gr.Markdown(\"#### ðŸ“Š Statistics\")\n",
        "    stats_html = gr.HTML()\n",
        "    \n",
        "    gr.Markdown(\"#### ðŸ“ˆ Time Series\")\n",
        "    ts_plot = gr.Plot()\n",
        "    \n",
        "    gr.Markdown(\"#### ðŸ”— Correlation\")\n",
        "    corr_card = gr.HTML()\n",
        "    corr_plot = gr.Plot()\n",
        "    \n",
        "    gr.Markdown(\"#### â° Hourly Patterns\")\n",
        "    hourly_card = gr.HTML()\n",
        "    hourly_plot = gr.Plot()\n",
        "    \n",
        "    gr.Markdown(\"#### ðŸ“… Daily Trends\")\n",
        "    daily_card = gr.HTML()\n",
        "    daily_plot = gr.Plot()\n",
        "    \n",
        "    gr.Markdown(\"#### ðŸ“Š Distribution\")\n",
        "    dist_card = gr.HTML()\n",
        "    dist_plot = gr.Plot()\n",
        "    \n",
        "    refresh_btn.click(\n",
        "        dashboard_screen,\n",
        "        outputs=[kpi_html, stats_html, ts_plot, corr_card, corr_plot, hourly_card, hourly_plot, daily_card, daily_plot, dist_card, dist_plot]\n",
        "    )\n",
        "    \n",
        "    gr.Markdown(\"#### ðŸ“‰ Moving Averages\")\n",
        "    with gr.Row():\n",
        "        ma_var = gr.Dropdown(choices=['temperature', 'humidity', 'soil'], value='temperature', label='Variable')\n",
        "        ma_btn = gr.Button(\"Calculate\")\n",
        "    ma_card = gr.HTML()\n",
        "    ma_plot = gr.Plot()\n",
        "    \n",
        "    ma_btn.click(dashboard_moving_avg, inputs=[ma_var], outputs=[ma_card, ma_plot])\n",
        "\n",
        "\n",
        "def build_sync_data_tab():\n",
        "    \"\"\"Build data sync tab.\"\"\"\n",
        "    gr.Markdown(\"### ðŸ”„ Sync Data from Server\")\n",
        "    gr.Markdown(\"Upload IoT sensor data to Firebase database\")\n",
        "    \n",
        "    sync_btn = gr.Button(\"ðŸ”„ Sync New Data\", variant=\"primary\")\n",
        "    sync_output = gr.Textbox(label=\"Status\", lines=5)\n",
        "    \n",
        "    sync_btn.click(sync_screen_gamified, outputs=[sync_output])\n",
        "\n",
        "\n",
        "def build_generate_report_tab():\n",
        "    \"\"\"Build report generation tab.\"\"\"\n",
        "    gr.Markdown(\"### ðŸ“„ Generate Report\")\n",
        "    gr.Markdown(\"Create a DOCX report with sensor data analysis\")\n",
        "    \n",
        "    samples = gr.Slider(5, 200, value=20, step=1, label=\"Samples per sensor\")\n",
        "    btn = gr.Button(\"ðŸ“¥ Generate Report\", variant=\"primary\")\n",
        "    status = gr.Textbox(label=\"Status\", lines=2)\n",
        "    file = gr.File(label=\"Download\")\n",
        "    \n",
        "    btn.click(generate_report_screen_gamified, inputs=[samples], outputs=[status, file])\n",
        "\n",
        "\n",
        "def build_plant_disease_tab():\n",
        "    \"\"\"Build plant disease detection tab.\"\"\"\n",
        "    gr.Markdown(\"### ðŸ–¼ï¸ Plant Disease Detection\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            image = gr.Image(type=\"filepath\", label=\"Upload Plant Image\")\n",
        "            temp = gr.Slider(0, 45, value=25, label=\"Temperature (Â°C)\")\n",
        "            humidity = gr.Slider(0, 100, value=50, label=\"Humidity (%)\")\n",
        "            soil = gr.Slider(0, 100, value=50, label=\"Soil Moisture (%)\")\n",
        "            btn = gr.Button(\"Analyze Plant\", variant=\"primary\")\n",
        "        \n",
        "        with gr.Column(scale=2):\n",
        "            diagnosis = gr.Textbox(label=\"Diagnosis\")\n",
        "            status = gr.HTML(label=\"Status\")\n",
        "            alerts = gr.Textbox(label=\"Alerts\", lines=4)\n",
        "            recommendations = gr.Textbox(label=\"Recommendations\", lines=4)\n",
        "    \n",
        "    btn.click(analyze_plant_gamified, inputs=[image, temp, humidity, soil], outputs=[diagnosis, status, alerts, recommendations])\n",
        "\n",
        "\n",
        "print(\"âœ… UI builders ready (part 1)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¨ 18. UI Tab Builders (Part 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# UI TAB BUILDERS - RAG, Chat, Rewards\n",
        "# =============================================================================\n",
        "\n",
        "def build_rag_chat_tab():\n",
        "    \"\"\"Build RAG-powered Q&A tab.\"\"\"\n",
        "    gr.Markdown(\"### ðŸ” RAG Q&A - Ask about Plant Science\")\n",
        "    gr.Markdown(\"Ask questions about the indexed research papers\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        question = gr.Textbox(label=\"Question\", placeholder=\"Ask about plant diseases, soil health, etc.\", lines=2)\n",
        "    \n",
        "    with gr.Row():\n",
        "        top_k = gr.Slider(1, 5, value=3, step=1, label=\"Top-K documents\")\n",
        "    \n",
        "    ask_btn = gr.Button(\"ðŸ” Search & Answer\", variant=\"primary\")\n",
        "    \n",
        "    answer = gr.Textbox(label=\"Answer\", lines=6)\n",
        "    sources = gr.Textbox(label=\"Sources & Ranking\", lines=10)\n",
        "    \n",
        "    def rag_search(q, k):\n",
        "        if not q.strip():\n",
        "            return \"Please enter a question.\", \"\"\n",
        "        \n",
        "        result = rag_service.answer_question(q, top_k=k)\n",
        "        \n",
        "        # Format sources\n",
        "        source_lines = []\n",
        "        if result.get(\"docs\"):\n",
        "            source_lines.append(\"ðŸ“Š Document Ranking:\")\n",
        "            for d in result[\"docs\"]:\n",
        "                source_lines.append(f\"  â€¢ Doc {d['doc_id']}: score={d['score']:.4f} | {d['title'][:50]}\")\n",
        "        \n",
        "        if result.get(\"sources\"):\n",
        "            source_lines.append(\"\\nðŸ“Ž Sources:\")\n",
        "            for url in result[\"sources\"]:\n",
        "                source_lines.append(f\"  â€¢ {url}\")\n",
        "        \n",
        "        if result.get(\"chunks\"):\n",
        "            source_lines.append(f\"\\nðŸ“„ Used {len(result['chunks'])} relevant chunks\")\n",
        "        \n",
        "        return result.get(\"answer\", \"No answer found.\"), \"\\n\".join(source_lines)\n",
        "    \n",
        "    ask_btn.click(rag_search, inputs=[question, top_k], outputs=[answer, sources])\n",
        "\n",
        "\n",
        "def build_gemini_chat_tab():\n",
        "    \"\"\"Build Gemini chat tab with database persistence.\"\"\"\n",
        "    gr.Markdown(\"### ðŸ’¬ AI Chat (with History)\")\n",
        "    gr.Markdown(\"Chat with AI - conversations are saved to database\")\n",
        "    \n",
        "    # Session management\n",
        "    with gr.Row():\n",
        "        session_dropdown = gr.Dropdown(choices=[], label=\"Load Session\", scale=3)\n",
        "        new_session_btn = gr.Button(\"New Session\", scale=1)\n",
        "        refresh_sessions_btn = gr.Button(\"ðŸ”„\", scale=1)\n",
        "    \n",
        "    session_id_display = gr.Textbox(label=\"Current Session\", interactive=False)\n",
        "    \n",
        "    chat = gr.Chatbot(label=\"Chat History\", height=400)\n",
        "    state = gr.State([])\n",
        "    \n",
        "    msg = gr.Textbox(label=\"Message\", placeholder=\"Type your message...\", lines=2)\n",
        "    temperature = gr.Slider(0.0, 1.0, value=0.7, step=0.05, label=\"Creativity\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Clear Session\")\n",
        "    \n",
        "    def refresh_sessions():\n",
        "        sessions = chat_service.get_all_sessions()\n",
        "        choices = [f\"{s['id']} ({s['message_count']} msgs)\" for s in sessions]\n",
        "        return gr.update(choices=choices)\n",
        "    \n",
        "    def new_session():\n",
        "        session_id = chat_service.start_new_session()\n",
        "        return session_id, [], []\n",
        "    \n",
        "    def load_session(selection):\n",
        "        if not selection:\n",
        "            return \"\", [], []\n",
        "        session_id = selection.split(\" (\")[0]\n",
        "        chat_service.load_session(session_id)\n",
        "        return session_id, chat_service.session_history, chat_service.session_history\n",
        "    \n",
        "    def send_message(message, history, temp):\n",
        "        if not message.strip():\n",
        "            return \"\", history, history\n",
        "        \n",
        "        answer, new_history = chat_service.chat(message, temp)\n",
        "        return \"\", new_history, new_history\n",
        "    \n",
        "    def clear_chat():\n",
        "        new_history = chat_service.clear_session()\n",
        "        return chat_service.current_session_id, [], []\n",
        "    \n",
        "    # Wire up events\n",
        "    refresh_sessions_btn.click(refresh_sessions, outputs=[session_dropdown])\n",
        "    new_session_btn.click(new_session, outputs=[session_id_display, chat, state])\n",
        "    session_dropdown.change(load_session, inputs=[session_dropdown], outputs=[session_id_display, chat, state])\n",
        "    send_btn.click(send_message, inputs=[msg, state, temperature], outputs=[msg, chat, state])\n",
        "    msg.submit(send_message, inputs=[msg, state, temperature], outputs=[msg, chat, state])\n",
        "    clear_btn.click(clear_chat, outputs=[session_id_display, chat, state])\n",
        "\n",
        "\n",
        "def build_rewards_tab():\n",
        "    \"\"\"Build gamification rewards tab.\"\"\"\n",
        "    profile = GamificationService.get_profile()\n",
        "    \n",
        "    gr.Markdown(\"### ðŸŽ® Farm Rewards\")\n",
        "    gr.Markdown(\"Complete missions to earn points and spins!\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        points_box = gr.Textbox(label=\"Points\", value=str(profile[\"points\"]), interactive=False)\n",
        "        spins_box = gr.Textbox(label=\"Spins Available\", value=str(profile[\"spins_available\"]), interactive=False)\n",
        "    \n",
        "    status_html = gr.HTML(value=GamificationService.get_status_html())\n",
        "    \n",
        "    spin_btn = gr.Button(\"ðŸŽ° Spin the Wheel!\", variant=\"primary\")\n",
        "    spin_result = gr.Textbox(label=\"Result\", lines=2)\n",
        "    \n",
        "    coupons_md = gr.Markdown(\"### Your Coupons\")\n",
        "    coupons_list = gr.Textbox(label=\"Coupons\", lines=4, value=\"No coupons yet\")\n",
        "    \n",
        "    refresh_btn = gr.Button(\"ðŸ”„ Refresh Stats\")\n",
        "    \n",
        "    def spin():\n",
        "        profile, msg = GamificationService.spin_wheel()\n",
        "        coupons_text = \"\\n\".join([f\"â€¢ {c['code']} - {c['label']}\" for c in profile.get(\"coupons\", [])]) or \"No coupons yet\"\n",
        "        return str(profile[\"points\"]), str(profile[\"spins_available\"]), msg, coupons_text, GamificationService.get_status_html()\n",
        "    \n",
        "    def refresh():\n",
        "        profile = GamificationService.get_profile()\n",
        "        coupons_text = \"\\n\".join([f\"â€¢ {c['code']} - {c['label']}\" for c in profile.get(\"coupons\", [])]) or \"No coupons yet\"\n",
        "        return str(profile[\"points\"]), str(profile[\"spins_available\"]), coupons_text, GamificationService.get_status_html()\n",
        "    \n",
        "    spin_btn.click(spin, outputs=[points_box, spins_box, spin_result, coupons_list, status_html])\n",
        "    refresh_btn.click(refresh, outputs=[points_box, spins_box, coupons_list, status_html])\n",
        "\n",
        "\n",
        "print(\"âœ… UI builders ready (part 2)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¨ 19. Custom CSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CUSTOM CSS\n",
        "# =============================================================================\n",
        "\n",
        "CUSTOM_CSS = \"\"\"\n",
        "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');\n",
        "\n",
        "* { font-family: 'Inter', sans-serif; }\n",
        "\n",
        ".kpi-card {\n",
        "    background: white;\n",
        "    padding: 24px;\n",
        "    border-radius: 12px;\n",
        "    box-shadow: 0 1px 3px rgba(0,0,0,0.12);\n",
        "    text-align: center;\n",
        "    border-left: 4px solid;\n",
        "    min-width: 150px;\n",
        "}\n",
        "\n",
        ".kpi-label { color: #6b7280; font-size: 14px; font-weight: 600; }\n",
        ".kpi-value { font-size: 36px; font-weight: 700; color: #1f2937; }\n",
        ".trend-up { color: #10b981; }\n",
        ".trend-down { color: #ef4444; }\n",
        "\n",
        ".gradio-container { max-width: 1400px !important; }\n",
        "\"\"\"\n",
        "\n",
        "print(\"âœ… CSS ready\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ 20. Launch Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# MAIN APPLICATION - Build and launch Gradio app\n",
        "# =============================================================================\n",
        "\n",
        "def build_app():\n",
        "    \"\"\"Build the complete Gradio application.\"\"\"\n",
        "    \n",
        "    with gr.Blocks(css=CUSTOM_CSS, title=\"CloudGarden\") as app:\n",
        "        \n",
        "        gr.Markdown(\"\"\"\n",
        "        # ðŸŒ± CloudGarden\n",
        "        ### Smart Plant Monitoring & Disease Detection System\n",
        "        \"\"\")\n",
        "        \n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"ðŸŒ¿ Realtime Dashboard\"):\n",
        "                build_realtime_dashboard_tab()\n",
        "            \n",
        "            with gr.Tab(\"ðŸ“Š IoT Analytics\"):\n",
        "                build_iot_dashboard_tab()\n",
        "            \n",
        "            with gr.Tab(\"ðŸ”„ Sync Data\"):\n",
        "                build_sync_data_tab()\n",
        "            \n",
        "            with gr.Tab(\"ðŸ“„ Generate Report\"):\n",
        "                build_generate_report_tab()\n",
        "            \n",
        "            with gr.Tab(\"ðŸ–¼ï¸ Disease Detection\"):\n",
        "                build_plant_disease_tab()\n",
        "            \n",
        "            with gr.Tab(\"ðŸ” RAG Q&A\"):\n",
        "                build_rag_chat_tab()\n",
        "            \n",
        "            with gr.Tab(\"ðŸ’¬ AI Chat\"):\n",
        "                build_gemini_chat_tab()\n",
        "            \n",
        "            with gr.Tab(\"ðŸŽ® Rewards\"):\n",
        "                build_rewards_tab()\n",
        "    \n",
        "    return app\n",
        "\n",
        "\n",
        "# Build and launch\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸš€ Starting CloudGarden Application...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "app = build_app()\n",
        "app.launch(share=True, debug=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}