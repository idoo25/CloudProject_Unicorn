{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/idoo25/CloudProject_Unicorn/blob/master/HW3_Unicorn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# \ud83c\udf3f CloudGarden - Smart Plant Care System\n\n**Layered Architecture:** Configuration \u2192 Data Access \u2192 Business Logic \u2192 Presentation \u2192 Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 1: Package Installation & Imports (Optimized for Colab)\n# ============================================================================\n# Only install packages NOT pre-installed in Colab\n!pip install -q gradio python-docx firebase-admin cerebras-cloud-sdk sentence-transformers fastapi uvicorn\n\n# === Standard Library (pre-installed) ===\nimport os\nimport sys\nimport re\nimport json\nimport math\nimport time\nimport random\nimport tempfile\nimport warnings\nimport subprocess\nfrom io import BytesIO\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom datetime import datetime, timedelta, timezone\nfrom zoneinfo import ZoneInfo\nfrom collections import Counter, defaultdict\nfrom urllib.parse import quote\n\nwarnings.filterwarnings('ignore')\n\n# === Data Science (pre-installed in Colab) ===\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport requests\nimport gdown\n\n# === NLP (pre-installed in Colab) ===\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nnltk.download(\"stopwords\", quiet=True)\n\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words(\"english\"))\n\n# === External APIs (need pip install) ===\nimport gradio as gr\nfrom transformers import pipeline\nfrom docx import Document\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\nfrom docx.shared import Inches, Pt, RGBColor\nfrom cerebras.cloud.sdk import Cerebras\nfrom sentence_transformers import SentenceTransformer\nfrom fastapi import FastAPI\nfrom fastapi.responses import Response\nfrom pydantic import BaseModel\nimport firebase_admin\nfrom firebase_admin import credentials, db\nfrom google.colab import drive\n\nprint(\"\u2705 All packages loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%writefile report_service.py\nfrom fastapi import FastAPI\nfrom fastapi.responses import Response\nfrom pydantic import BaseModel\n\nimport os\nimport tempfile\nfrom datetime import datetime, timedelta\nfrom zoneinfo import ZoneInfo\nfrom typing import List, Dict, Any\n\nimport pandas as pd\nfrom docx import Document\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\n\nfrom cerebras.cloud.sdk import Cerebras\n\napp = FastAPI(title=\"Generate Report Microservice (Cerebras + DOCX)\")\n\n# -------- API Schema --------\nclass ReportPayload(BaseModel):\n    limit: int\n    temperature: List[Dict[str, Any]]\n    humidity: List[Dict[str, Any]]\n    soil: List[Dict[str, Any]]\n\n@app.get(\"/health\")\ndef health():\n    return {\"ok\": True}\n\n# -------- Helpers --------\ndef records_to_df(records: List[Dict[str, Any]]) -> pd.DataFrame:\n    if not records:\n        return pd.DataFrame(columns=[\"created_at\", \"value\"])\n    df = pd.DataFrame(records)\n    if \"created_at\" not in df.columns or \"value\" not in df.columns:\n        return pd.DataFrame(columns=[\"created_at\", \"value\"])\n    return df\n\ndef unify_sensor_dfs(dfs: dict) -> pd.DataFrame:\n    def prep(df, col):\n        if df is None or df.empty:\n            return pd.DataFrame(columns=[\"timestamp\", col])\n\n        out = df.copy()\n        if \"timestamp\" not in out.columns and \"created_at\" in out.columns:\n            out = out.rename(columns={\"created_at\": \"timestamp\"})\n\n        if \"timestamp\" not in out.columns:\n            out = out.reset_index().rename(columns={\"index\": \"timestamp\"})\n\n        if \"timestamp\" not in out.columns or \"value\" not in out.columns:\n            return pd.DataFrame(columns=[\"timestamp\", col])\n\n        out = out[[\"timestamp\", \"value\"]]\n        ts = out[\"timestamp\"]\n\n        # numeric timestamp (s/ms) OR datetime string\n        if pd.api.types.is_numeric_dtype(ts) or ts.astype(str).str.fullmatch(r\"\\d+\").all():\n            ts_num = pd.to_numeric(ts, errors=\"coerce\")\n            unit = \"ms\" if ts_num.dropna().astype(int).astype(str).str.len().median() >= 13 else \"s\"\n            out[\"timestamp\"] = (\n                pd.to_datetime(ts_num, errors=\"coerce\", unit=unit, utc=True)\n                .dt.tz_convert(\"Asia/Jerusalem\")\n                .dt.tz_localize(None)\n            )\n        else:\n            out[\"timestamp\"] = (\n                pd.to_datetime(ts, errors=\"coerce\", utc=True)\n                .dt.tz_convert(\"Asia/Jerusalem\")\n                .dt.tz_localize(None)\n            )\n\n        out = out.dropna(subset=[\"timestamp\"])\n        out[\"value\"] = pd.to_numeric(out[\"value\"], errors=\"coerce\")\n        out = out.dropna(subset=[\"value\"])\n        out = out.rename(columns={\"value\": col})\n        return out\n\n    t = prep(dfs.get(\"temperature\"), \"temperature\")\n    h = prep(dfs.get(\"humidity\"), \"humidity\")\n    s = prep(dfs.get(\"soil\"), \"soil\")\n\n    df = t.merge(h, on=\"timestamp\", how=\"outer\").merge(s, on=\"timestamp\", how=\"outer\")\n    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n    return df\n\nclass AutomatedReportGenerator:\n    def __init__(self, cerebras_client, model_name: str):\n        self.client = cerebras_client\n        self.model_name = model_name\n\n    def generate_daily_report(self, df: pd.DataFrame) -> str:\n        if df.empty:\n            return \"No data available for report.\"\n\n        try:\n            cutoff = df[\"timestamp\"].max() - timedelta(hours=24)\n            daily = df[df[\"timestamp\"] > cutoff]\n            if daily.empty:\n                daily = df.tail(100)\n        except Exception:\n            daily = df.tail(100)\n\n        stats = {\n            \"date\": daily[\"timestamp\"].max().strftime(\"%Y-%m-%d\"),\n            \"readings\": len(daily),\n            \"temp_avg\": daily[\"temperature\"].mean(),\n            \"temp_min\": daily[\"temperature\"].min(),\n            \"temp_max\": daily[\"temperature\"].max(),\n            \"humidity_avg\": daily[\"humidity\"].mean(),\n            \"humidity_min\": daily[\"humidity\"].min(),\n            \"humidity_max\": daily[\"humidity\"].max(),\n            \"soil_avg\": daily[\"soil\"].mean(),\n            \"soil_min\": daily[\"soil\"].min(),\n            \"soil_max\": daily[\"soil\"].max(),\n        }\n\n        prompt = f\"\"\"Generate a professional daily plant health report based on this data:\n\nDATE: {stats['date']}\nREADINGS: {stats['readings']} sensor measurements\n\nENVIRONMENTAL CONDITIONS:\n- Temperature: {stats['temp_avg']:.1f}\u00b0C (range: {stats['temp_min']:.1f}-{stats['temp_max']:.1f}\u00b0C)\n- Humidity: {stats['humidity_avg']:.1f}% (range: {stats['humidity_min']:.1f}-{stats['humidity_max']:.1f}%)\n- Soil Moisture: {stats['soil_avg']:.1f}% (range: {stats['soil_min']:.1f}-{stats['soil_max']:.1f}%)\n\nGenerate a concise daily summary (3-4 paragraphs in English) covering:\n1) Overall environmental conditions\n2) Risks and potential stress/disease\n3) Practical care recommendations\n\"\"\"\n\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an agricultural consultant generating plant health reports in English.\"},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            temperature=0.3,\n            max_tokens=800,\n        )\n        return response.choices[0].message.content\n\n    def create_docx_report(self, df: pd.DataFrame, output_path: str) -> str:\n        doc = Document()\n\n        title = doc.add_heading(\"\ud83c\udf31 Daily Plant Health Report\", 0)\n        title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n\n        date_para = doc.add_paragraph()\n        date_run = date_para.add_run(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n        date_run.bold = True\n        date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n\n        doc.add_heading(\"Executive Summary\", 1)\n        doc.add_paragraph(self.generate_daily_report(df))\n\n        doc.add_heading(\"Statistical Summary\", 1)\n        stats_text = (\n            f\"Total Readings: {len(df)}\\n\"\n            f\"Time Period: {df['timestamp'].min().strftime('%Y-%m-%d')} to {df['timestamp'].max().strftime('%Y-%m-%d')}\\n\"\n            f\"Data Points: Temperature, Humidity, Soil Moisture\\n\"\n            f\"Quality: All sensors operational\"\n        )\n        doc.add_paragraph(stats_text)\n\n        doc.save(output_path)\n        return output_path\n\n@app.post(\"/generate-docx\")\ndef generate_docx(payload: ReportPayload):\n    api_key = os.environ.get(\"CEREBRAS_API_KEY\", \"\").strip()\n    model_name = os.environ.get(\"REPORT_MODEL_NAME\", \"llama3.1-8b\").strip()\n\n    if not api_key:\n        return Response(content=b\"Missing CEREBRAS_API_KEY (set in runtime env).\", status_code=500)\n\n    client = Cerebras(api_key=api_key)\n    report_gen = AutomatedReportGenerator(client, model_name)\n\n    dfs = {\n        \"temperature\": records_to_df(payload.temperature),\n        \"humidity\": records_to_df(payload.humidity),\n        \"soil\": records_to_df(payload.soil),\n    }\n\n    df = unify_sensor_dfs(dfs).dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n    if df.empty:\n        return Response(content=b\"No data available for report.\", status_code=400)\n\n    fd, path = tempfile.mkstemp(suffix=\".docx\", prefix=\"daily_report_\")\n    os.close(fd)\n\n    try:\n        report_gen.create_docx_report(df, output_path=path)\n        with open(path, \"rb\") as f:\n            docx_bytes = f.read()\n    finally:\n        try:\n            os.remove(path)\n        except Exception:\n            pass\n\n    return Response(\n        content=docx_bytes,\n        media_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n        headers={\"Content-Disposition\": \"attachment; filename=daily_report.docx\"},\n    )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 2: Firebase & API Configuration\n# ============================================================================\n#CEREBRAS cardetntial\nimport requests\n\nfile_id = \"1zOdWD70pxR_BKBW8vMU3FTN1MqL1fkYD\"\nurl = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n\nresponse = requests.get(url)\nCEREBRAS_API_KEY = response.text.strip()\n\n\n\n\nassert CEREBRAS_API_KEY, \"Key file is empty or not found\"\nREPORT_MODEL_NAME = \"llama3.1-8b\"\n\n# Firebase credentials\nFIREBASE_KEY_ID = '1ESnh8BIbGKrVEijA9nKNgNJNdD5kAaYC'\nfirebase_key_file = 'firebase_key.json'\n#add for the RAG & INDEX CODE#\nFIREBASE_URL = \"https://cloud-81451-default-rtdb.europe-west1.firebasedatabase.app/\"\n#-----#\nif os.path.exists(firebase_key_file):\n    os.remove(firebase_key_file)\n\nprint(' Downloading Firebase credentials...')\ntry:\n    url = f'https://drive.google.com/uc?id={FIREBASE_KEY_ID}'\n    gdown.download(url, firebase_key_file, quiet=False, fuzzy=True)\n    with open(firebase_key_file, 'r') as f:\n        creds = json.load(f)\n    print(f'\u2713 Project: {creds.get(\"project_id\")}')\nexcept Exception as e:\n    print(f'\u26a0\ufe0f Error: {e}')\n    from google.colab import files\n    uploaded = files.upload()\n    if uploaded:\n        os.rename(list(uploaded.keys())[0], firebase_key_file)\n\n# Initialize Firebase\nif not firebase_admin._apps:\n    firebase_admin.initialize_app(\n        credentials.Certificate(firebase_key_file),\n        {'databaseURL': 'https://cloud-81451-default-rtdb.europe-west1.firebasedatabase.app/'}\n    )\n    print(' Firebase initialized')\n\n# Server Configuration (already exists in Cell 6, but adding here for completeness)\nBATCH_LIMIT = 200\n\nprint(' Firebase configured')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 3: Data Access Layer - Sync & Loading Functions\n# ============================================================================\n# Sync Functions\n\ndef get_latest_timestamp_from_firebase():\n    try:\n        latest = db.reference('/sensor_data').order_by_child('created_at').limit_to_last(1).get()\n        return list(latest.values())[0]['created_at'] if latest else None\n    except:\n        return None\n\ndef fetch_batch_from_server(before_timestamp=None):\n    params = {\"feed\": FEED, \"limit\": BATCH_LIMIT}\n    if before_timestamp:\n        params[\"before_created_at\"] = before_timestamp\n    try:\n        return requests.get(f\"{BASE_URL}/history\", params=params, timeout=180).json()\n    except:\n        return {}\n\ndef save_sensor_data_to_firebase(data_list):\n    if not data_list:\n        return 0\n\n    ref = db.reference('/sensor_data')\n    saved = 0\n\n    for sample in data_list:\n        try:\n            vals = json.loads(sample['value'])\n            temperature = max(-50, min(100, float(vals['temperature'])))\n            humidity = max(0, min(100, float(vals['humidity'])))\n            soil = max(0, min(100, float(vals['soil'])))\n            timestamp_key = sample['created_at'].replace(':', '-').replace('.', '-')\n\n            ref.child(timestamp_key).set({\n                'created_at': sample['created_at'],\n                'temperature': temperature,\n                'humidity': humidity,\n                'soil': soil\n            })\n\n            saved += 1\n        except:\n            continue\n\n    return saved\n\ndef sync_new_data_from_server():\n    msgs = [\"Starting sync...\"]\n    latest = get_latest_timestamp_from_firebase()\n    msgs.append(f\"Latest: {latest}\" if latest else \"No existing data\")\n    resp = fetch_batch_from_server()\n\n    if \"data\" not in resp:\n        return \"\\n\".join(msgs + [\"Error fetching data\"]), 0\n\n    new = [s for s in resp[\"data\"] if not latest or s[\"created_at\"] > latest]\n\n    if new:\n        saved = save_sensor_data_to_firebase(new)\n        return \"\\n\".join(msgs + [f\"Found {len(new)} new\", f\"Saved {saved}!\"]), saved\n\n    return \"\\n\".join(msgs + [\"No new data\"]), 0\n\nprint('Sync functions loaded')\n\n\n# @title\n# Data Loading (YOUR ORIGINAL CODE)\n\ndef load_data_from_firebase():\n    data = db.reference('/sensor_data').get()\n    if not data:\n        return pd.DataFrame()\n    df = pd.DataFrame([{\n        'timestamp': pd.to_datetime(v['created_at']),\n        'temperature': float(v['temperature']),\n        'humidity': float(v['humidity']),\n        'soil': float(v['soil'])\n    } for v in data.values()])\n    df = df.sort_values('timestamp').reset_index(drop=True)\n    df['humidity'] = df['humidity'].clip(0, 100)\n    df['soil'] = df['soil'].clip(0, 100)\n    df['temperature'] = df['temperature'].clip(-50, 100)\n    return df\n\nprint('\u2705 Data loading ready')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 4: Data Ingestion & MapReduce Processing\n# ============================================================================\ndf = load_data_from_firebase() # NoSQL DB acting as a Data Lake for raw sensor data\nprint(df.shape)\ndf.head()\n\n# Data Lake: df already contains raw sensor data loaded from the DB (Firebase/NoSQL)\ndf['timestamp'] = pd.to_datetime(df['timestamp'])  # Data Lake -> processing raw timestamps\n\n# Map: create a time key for slicing (hourly buckets)\ndf['hour_bucket'] = df['timestamp'].dt.floor('H')  # Map phase (key = hour)\n\n# Reduce: aggregate (average) per hour\nagg_hourly = df.groupby('hour_bucket', as_index=False).agg(\n    avg_temperature=('temperature', 'mean'),\n    avg_humidity=('humidity', 'mean'),\n    avg_soil=('soil', 'mean')\n)  # Reduce phase (aggregation)\n\n# Interactive Analytics: plot one \"cut\" of your choice (example: avg temperature over time)\nplt.figure()\nplt.plot(agg_hourly['hour_bucket'], agg_hourly['avg_temperature'])  # Interactive Analytics (graph)\nplt.xlabel('Time (hourly)')\nplt.ylabel('Average Temperature')\nplt.title('Average Temperature per Hour')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Save the graph as an image for Word export\ngraph_path = \"avg_temperature_per_hour.png\"\nplt.savefig(graph_path)  # Value: exporting insight for report\nplt.show()\n\nprint(\"Graph saved to:\", graph_path)\nagg_hourly.head()\n\n# Create Word document\ndoc = Document()\ndoc.add_heading('Big Data Sensor Analysis \u2013 Time-based Aggregation', level=1)\n\ndoc.add_paragraph(\n    'The following graph presents an hourly aggregation of sensor data collected '\n    'and stored in a NoSQL database acting as a Data Lake. '\n    'The analysis was performed using a MapReduce-based approach, '\n    'where data was grouped by time and aggregated to extract insights.'\n)\n\n# Add the graph\ndoc.add_picture('avg_temperature_per_hour.png', width=Inches(6))\n\n# Save document\ndoc_path = 'sensor_data_analysis_report.docx'\ndoc.save(doc_path)\n\ndoc_path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 5: Global Configuration & Realtime Dashboard Logic\n# ============================================================================\n# Global Configuration\n\n# ============================================================================\n# SERVER & FEED CONFIGURATION (YOUR ORIGINAL SETTINGS)\n# ============================================================================\nFEED = \"json\"  # Your feed name - CHANGE THIS if different\nBASE_URL = \"https://server-cloud-v645.onrender.com/\"\nBATCH_LIMIT = 200\n\n# ============================================================================\n# APP CONFIGURATION\n# ============================================================================\nAPP_TITLE = \"\ud83c\udf31 CloudGarden\"\nAPP_SUBTITLE = \"Smart Plant Disease Detection System\"\n\n# --- Colors for Friend's Realtime Dashboard ---\nCOLOR_TEMP = \"#1f77b4\"   # blue\nCOLOR_HUM  = \"#ff7f0e\"   # orange\nCOLOR_SOIL = \"#2ca02c\"   # green\n\nSTATUS_OK_COLOR = \"#2ca02c\"      # green\nSTATUS_WARN_COLOR = \"#ffbf00\"    # yellow\nSTATUS_BAD_COLOR = \"#d62728\"     # red\n\n# --- ML Model Configuration ---\nMODEL_NAME = \"linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification\"\nclf = pipeline(\"image-classification\", model=MODEL_NAME)\n\n# ============================================================================\n# CSS STYLING FOR IOT DASHBOARD\n# ============================================================================\n\nCOLORS = {\n    'temperature': {'color': '#ef4444'},\n    'humidity': {'color': '#3b82f6'},\n    'soil': {'color': '#8b5cf6'}\n}\n\nCUSTOM_CSS = \"\"\"\n@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');\n* { font-family: 'Inter', sans-serif; }\n\n.kpi-card {\n    background: white;\n    padding: 24px;\n    border-radius: 12px;\n    box-shadow: 0 1px 3px rgba(0,0,0,0.12);\n    text-align: center;\n    border-left: 4px solid;\n}\n.kpi-label { color: #6b7280; font-size: 14px; font-weight: 600; }\n.kpi-value { font-size: 48px; font-weight: 700; color: #1f2937; }\n.trend-up { color: #10b981; }\n.trend-down { color: #ef4444; }\n\n/* Tooltip styles */\n.info-icon {\n    position: relative;\n    display: inline-flex;\n    cursor: help;\n}\n\n.info-icon .tooltip-text {\n    visibility: hidden;\n    width: 200px;\n    background-color: #1f2937;\n    color: white;\n    text-align: center;\n    border-radius: 6px;\n    padding: 8px;\n    position: absolute;\n    z-index: 1000;\n    bottom: 125%;\n    left: 50%;\n    margin-left: -100px;\n    opacity: 0;\n    transition: opacity 0.3s;\n    font-size: 11px;\n    line-height: 1.4;\n    box-shadow: 0 2px 8px rgba(0,0,0,0.2);\n}\n\n.info-icon .tooltip-text::after {\n    content: \"\";\n    position: absolute;\n    top: 100%;\n    left: 50%;\n    margin-left: -5px;\n    border-width: 5px;\n    border-style: solid;\n    border-color: #1f2937 transparent transparent transparent;\n}\n\n.info-icon:hover .tooltip-text {\n    visibility: visible;\n    opacity: 1;\n}\n\"\"\"\n\nprint('\u2705 Configuration loaded')\nprint(f'\ud83d\udce1 Server: {BASE_URL}')\nprint(f'\ud83d\udcfb Feed: {FEED}')\nprint(f'\ud83d\udce6 Batch limit: {BATCH_LIMIT}')\n\n\n# ---------- Core Data Fetch ----------\ndef load_iot_data(feed: str, limit: int) -> pd.DataFrame | None:\n    resp = requests.get(\n        f\"{BASE_URL}/history\",\n        params={\"feed\": feed, \"limit\": limit},\n        timeout=30\n    )\n    data = resp.json()\n    if \"data\" not in data or not data[\"data\"]:\n        return None\n\n    df = pd.DataFrame(data[\"data\"])\n    if \"created_at\" not in df.columns or \"value\" not in df.columns:\n        return None\n\n    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n    df = df.dropna(subset=[\"created_at\", \"value\"]).sort_values(\"created_at\")\n\n    return None if df.empty else df\n\n\n# ---------- Helpers ----------\ndef normalize(series: pd.Series) -> pd.Series:\n    mn, mx = float(series.min()), float(series.max())\n    if mx - mn == 0:\n        return series * 0.0\n    return (series - mn) / (mx - mn)\n\n\n# ---------- Plant Status + Plots ----------\ndef plant_dashboard(limit: int):\n    try:\n        dfs = {\n            \"temperature\": load_iot_data(\"temperature\", limit),\n            \"humidity\": load_iot_data(\"humidity\", limit),\n            \"soil\": load_iot_data(\"soil\", limit),\n        }\n\n        missing = [k for k, v in dfs.items() if v is None]\n        if missing:\n            return \"\u26a0\ufe0f Partial Data\", f\"Missing sensors or empty history: {', '.join(missing)}\", None, None, None, None\n\n        temp = float(dfs[\"temperature\"][\"value\"].iloc[-1])\n        hum = float(dfs[\"humidity\"][\"value\"].iloc[-1])\n        soil = float(dfs[\"soil\"][\"value\"].iloc[-1])\n\n        issues, warnings = [], []\n\n        checks = [\n            (\"Temperature\", temp, 18, 32, 1),\n            (\"Air humidity\", hum, 35, 75, 3),\n            (\"Soil moisture\", soil, 20, 60, 3),\n        ]\n\n        for name, value, low, high, margin in checks:\n            if not (low <= value <= high):\n                issues.append(f\"{name} out of range ({value:.1f})\")\n            elif value <= low + margin or value >= high - margin:\n                warnings.append(f\"{name} near limit ({value:.1f})\")\n\n        if issues:\n            status = \"\ud83d\udd34 Plant Status: Not OK\"\n            details_main = \" ; \".join(issues)\n\n        elif warnings:\n            status = \"\ud83d\udfe1 Plant Status: Warning\"\n            details_main = \" ; \".join(warnings)\n\n        else:\n            status = \"\ud83d\udfe2 Plant Status: OK\"\n            details_main = \"All sensors are within valid ranges\"\n\n        details = (\n    f\"{details_main}\\n\"\n    f\"Latest values:\\n\"\n    f\"temp={temp:.1f}\\n\"\n    f\"humidity={hum:.1f}\\n\"\n    f\"soil={soil:.1f}\"\n\n        )\n\n        df_t, df_h, df_s = dfs[\"temperature\"], dfs[\"humidity\"], dfs[\"soil\"]\n\n        fig_t = plt.figure(figsize=(7, 3.2))\n        plt.plot(df_t[\"created_at\"], df_t[\"value\"], marker=\"o\", color=COLOR_TEMP)\n        plt.title(\"Temperature History\")\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"\u00b0C\")\n        plt.grid(True)\n\n        fig_h = plt.figure(figsize=(7, 3.2))\n        plt.plot(df_h[\"created_at\"], df_h[\"value\"], marker=\"o\", color=COLOR_HUM)\n        plt.title(\"Air Humidity History\")\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"%\")\n        plt.grid(True)\n\n        fig_s = plt.figure(figsize=(7, 3.2))\n        plt.plot(df_s[\"created_at\"], df_s[\"value\"], marker=\"o\", color=COLOR_SOIL)\n        plt.title(\"Soil Moisture History\")\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"%\")\n        plt.grid(True)\n\n        fig_c = plt.figure(figsize=(10, 3.4))\n        plt.plot(df_t[\"created_at\"], normalize(df_t[\"value\"]), marker=\"o\", label=\"Temperature (norm)\", color=COLOR_TEMP)\n        plt.plot(df_h[\"created_at\"], normalize(df_h[\"value\"]), marker=\"o\", label=\"Humidity (norm)\", color=COLOR_HUM)\n        plt.plot(df_s[\"created_at\"], normalize(df_s[\"value\"]), marker=\"o\", label=\"Soil (norm)\", color=COLOR_SOIL)\n        plt.title(\"Combined Trend (Normalized)\")\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Normalized Value (0\u20131)\")\n        plt.grid(True)\n        plt.legend()\n\n        return status, details, fig_t, fig_h, fig_s, fig_c\n\n    except Exception:\n        return \"\u274c Error\", \"Failed to fetch data from server. Please try again.\", None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================================================\n# CELL 6: Dashboard, Report & Plant Disease Tabs (GUI + Logic)\n# ============================================================================\ndef build_realtime_dashboard_tab():\n    gr.Markdown(\n    \"<h3 style='margin:0; font-size:22px;'>\ud83c\udf3f Overall Plant Status (Real-Time)</h3>\"\n)\n\n\n    samples = gr.Slider(1, 200, value=20, step=1, label=\"Number of Samples (used for all graphs)\")\n    overall_btn = gr.Button(\"Update Plant Dashboard\", variant=\"primary\")\n\n\n\n    overall_status = gr.Textbox(\n        label=\"Overall Status\",\n        lines=1,\n        placeholder=\"Click 'Update Plant Dashboard' to evaluate plant status\"\n    )\n    overall_info = gr.Textbox(\n        label=\"Status Details\",\n        lines=4,\n        placeholder=\"Detailed plant analysis will appear here\"\n    )\n\n    with gr.Row():\n        gr.Markdown(f\"\"\"\n<div class=\"legend-card\" style=\"margin-top:14px;padding:14px;border:1px solid var(--border-color-primary)\n;border-radius:10px;\">\n\n\n\n\n  <h4 style=\"margin-bottom:10px; font-size:20px; font-weight:600;\">\n\ud83c\udf3f Plant Status\n</h4>\n\n\n  <span style=\"color:{STATUS_OK_COLOR};font-size:26px;\">\u25cf</span>\n  <b>Healthy</b> \u2013 All sensor values within normal ranges<br>\n\n  <span style=\"color:{STATUS_WARN_COLOR};font-size:26px;\">\u25cf</span>\n  <b>Warning</b> \u2013 At least one value near threshold<br>\n\n  <span style=\"color:{STATUS_BAD_COLOR};font-size:26px;\">\u25cf</span>\n  <b>Not OK</b> \u2013 One or more values out of range<br><br>\n\n  <span>Status is calculated automatically from sensor data</span>\n</div>\n        \"\"\")\n\n        gr.Markdown(f\"\"\"\n<div class=\"legend-card\" style=\"margin-top:14px;padding:14px;border:1px solid var(--border-color-primary)\n;border-radius:10px;\">\n\n\n\n\n  <h4 style=\"margin-bottom:10px; font-size:20px; font-weight:600;\">\n\u2139\ufe0f Valid Value Ranges\n</h4>\n\n\n  <span style=\"color:{COLOR_TEMP};font-size:26px;\">\u25cf</span>\n  \ud83c\udf21\ufe0f <b>Temperature</b>: 18\u201332\u00b0C<br>\n\n  <span style=\"color:{COLOR_HUM};font-size:26px;\">\u25cf</span>\n  \ud83d\udca7 <b>Air Humidity</b>: 35\u201375%<br>\n\n  <span style=\"color:{COLOR_SOIL};font-size:26px;\">\u25cf</span>\n  \ud83c\udf31 <b>Soil Moisture</b>: 20\u201360%<br><br>\n\n  <span>\u26a0\ufe0f Values outside these ranges are considered abnormal</span>\n</div>\n        \"\"\")\n\n    gr.Markdown(\"\"\"\n<h2 style=\"text-align:center; margin-top:22px; font-size:26px; font-weight:600;\">\n\ud83d\udcc8 Plant Sensor Graphs\n</h2>\n\"\"\")\n\n    # Full-width graphs stacked vertically (column layout)\n    plot_temp = gr.Plot(label=\"\ud83c\udf21\ufe0f Temperature\")\n    plot_hum = gr.Plot(label=\"\ud83d\udca7 Air Humidity\")\n    plot_soil = gr.Plot(label=\"\ud83c\udf31 Soil Moisture\")\n    plot_combined = gr.Plot(label=\"\ud83d\udcca Combined (Normalized)\")\n\n    overall_btn.click(\n        fn=plant_dashboard,\n        inputs=[samples],\n        outputs=[overall_status, overall_info, plot_temp, plot_hum, plot_soil, plot_combined]\n    )\n\n# =========================================================\n# TAB 3 \u2014 Generate Report (LOGIC ONLY) | CLIENT (NO AI HERE)\n# =========================================================\n\nREPORT_SERVICE_URL = \"http://127.0.0.1:8001/generate-docx\"\n\ndef df_to_records(df):\n    \"\"\"\n    Convert DataFrame with columns [created_at, value] into JSON-friendly list.\n    \"\"\"\n    if df is None or df.empty:\n        return []\n    out = df[[\"created_at\", \"value\"]].copy()\n    out[\"created_at\"] = out[\"created_at\"].astype(str)\n    return out.to_dict(\"records\")\n\ndef call_report_microservice(dfs: dict, limit: int) -> str:\n    \"\"\"\n    Send sensor data to microservice and save returned DOCX to temp file.\n    Returns local path for Gradio download.\n    \"\"\"\n    payload = {\n        \"limit\": int(limit),\n        \"temperature\": df_to_records(dfs.get(\"temperature\")),\n        \"humidity\": df_to_records(dfs.get(\"humidity\")),\n        \"soil\": df_to_records(dfs.get(\"soil\")),\n    }\n\n    r = requests.post(REPORT_SERVICE_URL, json=payload, timeout=120)\n    if not r.ok:\n        raise RuntimeError(f\"Microservice error: {r.status_code} | {r.text}\")\n\n    fd, path = tempfile.mkstemp(suffix=\".docx\", prefix=\"daily_report_\")\n    os.close(fd)\n    with open(path, \"wb\") as f:\n        f.write(r.content)\n\n    return path\n\ndef generate_report_screen(limit: int):\n    \"\"\"\n    Gradio button handler (same signature as before).\n    Returns: (status_text, file_path_or_None)\n    \"\"\"\n    try:\n        dfs = {\n            \"temperature\": load_iot_data(\"temperature\", limit),\n            \"humidity\": load_iot_data(\"humidity\", limit),\n            \"soil\": load_iot_data(\"soil\", limit),\n        }\n\n        if all(df is None or df.empty for df in dfs.values()):\n            return \"No data available to generate a report.\", None\n\n        out_path = call_report_microservice(dfs, limit)\n        return \"\u2705 Report generated successfully. Download below:\", out_path\n\n    except Exception as e:\n        return f\"\u274c Error generating report: {str(e)}\", None\n\n# =========================================================\n# TAB 2 \u2014 Generate Report (GUI ONLY)\n# =========================================================\n\ndef build_generate_report_tab():\n    gr.Markdown(\"## \ud83d\udcc4 Generate Report\")\n    gr.Markdown(\"Generate a Word (DOCX) report based on sensor data: temperature, humidity, and soil moisture. (English AI summary)\")\n\n    report_samples = gr.Slider(\n        minimum=5,\n        maximum=200,\n        value=20,\n        step=1,\n        label=\"Number of samples per sensor\"\n    )\n\n    report_btn = gr.Button(\"\ud83d\udce5 Generate & Download Report\", variant=\"primary\")\n    report_status = gr.Textbox(label=\"Status\", lines=2)\n    report_file = gr.File(label=\"Download DOCX\")\n\n    report_btn.click(\n        fn=generate_report_screen_gamified,   # \u05de\u05d2\u05d9\u05e2 \u05de\u05d4-LOGIC tab\n        inputs=[report_samples],\n        outputs=[report_status, report_file]\n    )\n\n\ndef analyze_plant(image, temp, humidity, soil):\n    \"\"\"\n    Focused Plant Disease Detection - Concise alerts, not raw data dumps.\n    Returns: diagnosis, status_html, alerts, recommendations\n    \"\"\"\n    # 1. ML Model Prediction\n    preds = clf(image)\n    top = preds[0]\n    label = top[\"label\"]\n    score = top[\"score\"]\n    disease_name = label.replace(\"_\", \" \").title()\n    is_bad = (\"healthy\" not in label.lower())\n    \n    alerts_list = []\n    recommendations_list = []\n    sensor_issues = []\n    \n    # 2. Get current sensor data from cache (pre-loaded)\n    try:\n        df = get_cached_sensor_data() if 'get_cached_sensor_data' in dir() else load_data_from_firebase()\n        if df is not None and not df.empty:\n            latest = df.iloc[-1]\n            temp = latest.get('temperature', temp)\n            humidity = latest.get('humidity', humidity)\n            soil = latest.get('soil', soil)\n    except:\n        pass\n    \n    # 3. Analyze sensors - only report issues\n    if temp < 18:\n        sensor_issues.append(f\"\ud83c\udf21\ufe0f Low temp ({temp:.0f}\u00b0C) - cold stress risk\")\n        recommendations_list.append(\"\ud83c\udf21\ufe0f Move to warmer location (18-32\u00b0C)\")\n    elif temp > 32:\n        sensor_issues.append(f\"\ud83c\udf21\ufe0f High temp ({temp:.0f}\u00b0C) - heat stress risk\")\n        recommendations_list.append(\"\ud83c\udf21\ufe0f Move to shade, improve ventilation\")\n    \n    if humidity < 35:\n        sensor_issues.append(f\"\ud83d\udca7 Low humidity ({humidity:.0f}%) - leaf drying risk\")\n        recommendations_list.append(\"\ud83d\udca7 Mist leaves or use humidifier\")\n    elif humidity > 75:\n        sensor_issues.append(f\"\ud83d\udca7 High humidity ({humidity:.0f}%) - fungal risk\")\n        recommendations_list.append(\"\ud83d\udca8 Improve ventilation\")\n    \n    if soil < 20:\n        sensor_issues.append(f\"\ud83c\udf31 Dry soil ({soil:.0f}%) - needs water\")\n        recommendations_list.append(\"\ud83c\udf31 Water the plant\")\n    elif soil > 60:\n        sensor_issues.append(f\"\ud83c\udf31 Wet soil ({soil:.0f}%) - root rot risk\")\n        recommendations_list.append(\"\ud83c\udf31 Reduce watering, check drainage\")\n    \n    # 4. Build focused alert summary\n    if is_bad:\n        alerts_list.append(f\"\ud83d\udd34 DISEASE: {disease_name}\")\n        alerts_list.append(f\"   Confidence: {score:.0%}\")\n    else:\n        alerts_list.append(f\"\ud83d\udfe2 HEALTHY: {disease_name} ({score:.0%})\")\n    \n    # Show sensor status summary\n    if sensor_issues:\n        alerts_list.append(\"\\n\u26a0\ufe0f SENSOR ALERTS:\")\n        for issue in sensor_issues:\n            alerts_list.append(f\"   {issue}\")\n    else:\n        alerts_list.append(f\"\\n\u2705 Sensors OK: {temp:.0f}\u00b0C | {humidity:.0f}% RH | {soil:.0f}% soil\")\n    \n    # 5. Get RAG insight (one line summary only)\n    try:\n        query = f\"{disease_name} plant disease\"\n        q_terms, ranked = bm25_rank(query, k=1)\n        if ranked and is_bad:\n            doc_id, bm25_score = ranked[0]\n            alerts_list.append(f\"\\n\ud83d\udcda Research found (relevance: {bm25_score:.1f})\")\n    except:\n        pass\n    \n    # 6. AI recommendations for disease\n    if is_bad:\n        try:\n            prompt = f\"Plant disease: {disease_name}. Give 3 brief treatment tips (one line each).\"\n            ai_response = llm_generate(prompt, temperature=0.3, max_output_tokens=200)\n            if ai_response:\n                recommendations_list.append(f\"\\n\ud83e\udd16 Treatment for {disease_name}:\")\n                recommendations_list.append(ai_response)\n        except:\n            pass\n    else:\n        if not recommendations_list:\n            recommendations_list.append(\"\u2705 Continue current care routine\")\n    \n    # 7. Build status HTML\n    status_html = (\n        \"<div style='padding:12px;border-radius:10px;\"\n        f\"background:{'#ffdddd' if is_bad else '#ddffdd'};\"\n        f\"border:2px solid {'#ff0000' if is_bad else '#00aa00'};\"\n        \"font-weight:700;font-size:16px;'>\"\n        f\"{'\ud83d\udd34 NEEDS ATTENTION' if is_bad else '\ud83d\udfe2 HEALTHY'}\"\n        f\"<br><small>{disease_name} ({score:.0%})</small>\"\n        \"</div>\"\n    )\n    \n    return (\n        f\"\ud83d\udd2c {disease_name} ({score:.0%})\",\n        status_html,\n        \"\\n\".join(alerts_list),\n        \"\\n\".join(recommendations_list) if recommendations_list else \"No issues found.\"\n    )\n\ndef build_plant_disease_detection_tab():\n    gr.Markdown(\"## \ud83d\uddbc\ufe0f Plant Disease Detection\")\n\n    with gr.Row():\n\n        # -------- LEFT SIDE --------\n        with gr.Column(scale=2):\n\n            image = gr.Image(\n                type=\"filepath\",\n                label=\"Upload plant image\",\n                sources=[\"upload\"]\n            )\n\n            temp = gr.Slider(0, 45, value=25, label=\"Temperature (\u00b0C)\")\n            humidity = gr.Slider(0, 100, value=50, label=\"Humidity (%)\")\n            soil = gr.Slider(0, 100, value=50, label=\"Soil Moisture (%)\")\n\n            run_btn = gr.Button(\"Analyze Plant\", variant=\"primary\")\n\n        # -------- RIGHT SIDE --------\n        with gr.Column(scale=2):\n\n            diagnosis = gr.Textbox(\n                label=\"Diagnosis\",\n                placeholder=\"Plant disease diagnosis will appear here\"\n            )\n\n            status = gr.HTML(label=\"Status\")\n\n            alerts = gr.Textbox(\n                label=\"Alerts\",\n                lines=5,\n                placeholder=\"Sensor alerts and warnings will appear here\"\n            )\n\n            recommendations = gr.Textbox(\n                label=\"Recommendations\",\n                lines=5,\n                placeholder=\"Care and treatment recommendations will appear here\"\n            )\n\n    run_btn.click(\n        fn=analyze_plant_gamified,\n        inputs=[image, temp, humidity, soil],\n        outputs=[diagnosis, status, alerts, recommendations]\n    )"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n# ============================================================================\n# VECTOR EMBEDDINGS FOR FAST RAG (Pre-computed during indexing)\n# ============================================================================\n\n# Initialize embedding model (lightweight, fast)\nEMBED_MODEL = None\nDOC_EMBEDDINGS = {}  # {doc_id: embedding}\nCHUNK_EMBEDDINGS = {}  # {doc_id: [(chunk_text, embedding), ...]}\n\ndef get_embed_model():\n    global EMBED_MODEL\n    if EMBED_MODEL is None:\n        print(\"\ud83d\udce5 Loading embedding model...\")\n        EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')  # Small, fast model\n        print(\"\u2705 Embedding model loaded\")\n    return EMBED_MODEL\n\ndef compute_embeddings(texts):\n    \"\"\"Compute embeddings for a list of texts.\"\"\"\n    model = get_embed_model()\n    return model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n\ndef chunk_text(text, chunk_size=500, overlap=50):\n    \"\"\"Split text into overlapping chunks.\"\"\"\n    words = text.split()\n    chunks = []\n    for i in range(0, len(words), chunk_size - overlap):\n        chunk = ' '.join(words[i:i + chunk_size])\n        if chunk.strip():\n            chunks.append(chunk)\n    return chunks\n\ndef build_and_save_embeddings(doc_text_map, embed_path=\"indexes/embeddings\"):\n    \"\"\"Pre-compute and save embeddings for all document chunks.\"\"\"\n    global CHUNK_EMBEDDINGS\n    \n    print(\"\ud83d\udd04 Computing document embeddings...\")\n    all_chunks = []\n    chunk_mapping = []  # (doc_id, chunk_idx, chunk_text)\n    \n    for doc_id, text in doc_text_map.items():\n        chunks = chunk_text(text)\n        for idx, chunk in enumerate(chunks):\n            all_chunks.append(chunk)\n            chunk_mapping.append((str(doc_id), idx, chunk))\n    \n    if not all_chunks:\n        print(\"\u26a0\ufe0f No chunks to embed\")\n        return {}\n    \n    # Compute all embeddings at once (efficient batch processing)\n    embeddings = compute_embeddings(all_chunks)\n    \n    # Organize by document\n    embed_data = {}\n    for i, (doc_id, chunk_idx, chunk_text) in enumerate(chunk_mapping):\n        if doc_id not in embed_data:\n            embed_data[doc_id] = []\n        embed_data[doc_id].append({\n            'chunk': chunk_text[:200],  # Store truncated for reference\n            'embedding': embeddings[i].tolist()  # Convert to list for JSON\n        })\n    \n    # Save to Firebase\n    save_to_firebase(embed_data, embed_path)\n    print(f\"\u2705 Saved embeddings for {len(doc_text_map)} documents ({len(all_chunks)} chunks)\")\n    \n    # Cache locally\n    CHUNK_EMBEDDINGS = embed_data\n    return embed_data\n\ndef load_embeddings(embed_path=\"indexes/embeddings\"):\n    \"\"\"Load pre-computed embeddings from Firebase.\"\"\"\n    global CHUNK_EMBEDDINGS\n    \n    if CHUNK_EMBEDDINGS:\n        return CHUNK_EMBEDDINGS\n    \n    try:\n        embed_data = firebase_get(embed_path)\n        if embed_data:\n            CHUNK_EMBEDDINGS = embed_data\n            print(f\"\u2705 Loaded embeddings for {len(embed_data)} documents\")\n            return embed_data\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Could not load embeddings: {e}\")\n    \n    return {}\n\ndef semantic_search(query, top_k=5):\n    \"\"\"Fast semantic search using pre-computed embeddings.\"\"\"\n    embeddings = load_embeddings()\n    if not embeddings:\n        return []\n    \n    # Compute query embedding\n    query_emb = compute_embeddings([query])[0]\n    \n    # Calculate similarities\n    results = []\n    for doc_id, chunks in embeddings.items():\n        for chunk_data in chunks:\n            chunk_emb = np.array(chunk_data['embedding'])\n            # Cosine similarity\n            sim = np.dot(query_emb, chunk_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(chunk_emb) + 1e-9)\n            results.append((doc_id, chunk_data['chunk'], float(sim)))\n    \n    # Sort by similarity\n    results.sort(key=lambda x: x[2], reverse=True)\n    return results[:top_k]\n\n\n# ============================================================================\n# CELL 7: RAG System - Indexing, Retrieval & LLM Integration\n# ============================================================================\nDOC_URLS = [\n    \"https://doi.org/10.1038/s41598-025-20629-y\",\n    \"https://doi.org/10.3389/fpls.2016.01419\",\n    \"https://doi.org/10.1038/s41598-025-05102-0\",\n    \"https://doi.org/10.1038/s41598-025-04758-y\",\n    \"https://doi.org/10.2174/0118743315321139240627092707\",\n]\n\n# =========================\n# Cell 2: HTTP / Session setup\n# =========================\n# Purpose: Shared HTTP session + helper functions to fetch HTML/PDF and query academic APIs (Semantic Scholar/OpenAlex/Unpaywall).\n\n\n\n# Reuse a single session for performance and consistent headers/cookies.\nsession = requests.Session()\n\n# Browser-like headers to improve compatibility with some sites.\nBROWSER_HEADERS = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n}\n\n# Normalize DOI input (accept DOI or doi.org URL).\ndef _normalize_doi(doi_or_url: str) -> str:\n    s = (doi_or_url or \"\").strip()\n    s = s.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\")\n    return s.strip()\n\n# ---------- Added: text quality post-processing helpers ----------\n\n_STOP_SECTION_TITLES = {\n    \"references\", \"reference\", \"bibliography\",\n    \"acknowledgements\", \"acknowledgments\",\n    \"author information\", \"ethics declarations\",\n    \"additional information\", \"supplementary information\",\n    \"rights and permissions\", \"about this article\",\n    \"availability of data and materials\", \"data availability\",\n    \"publisher's note\"\n}\n\n_SKIP_LINE_RES = [\n    re.compile(r\"(?i)^\\s*cite this article\\s*$\"),\n    re.compile(r\"(?i)^\\s*article\\s+google\\s+scholar\\s*$\"),\n    re.compile(r\"(?i)^\\s*google\\s+scholar\\s*$\"),\n    re.compile(r\"(?i)^\\s*pubmed\\s*$\"),\n    re.compile(r\"(?i)^\\s*pubmed\\s+central\\s*$\"),\n    re.compile(r\"(?i)^\\s*ads\\s*$\"),\n    re.compile(r\"(?i)^\\s*cas\\s*$\"),\n    re.compile(r\"(?i)creative\\s+commons\"),\n    re.compile(r\"(?i)springer\\s+nature\"),\n    re.compile(r\"(?i)sharedit\"),\n    re.compile(r\"(?i)correspondence\\s+to\"),\n    re.compile(r\"(?i)the\\s+authors\\s+declare\\s+no\\s+competing\\s+interests\"),\n    re.compile(r\"(?i)view\\s+author\\s+publications\"),\n    re.compile(r\"(?i)search\\s+author\\s+on\"),\n    re.compile(r\"(?i)anyone\\s+you\\s+share\\s+the\\s+following\\s+link\"),\n    re.compile(r\"(?i)version\\s+of\\s+record\"),\n    re.compile(r\"(?i)copyright\"),\n    re.compile(r\"(?i)rights\\s+reserved\"),\n]\n\ndef normalize_text_keep_newlines(text: str) -> str:\n    \"\"\"Normalize whitespace while preserving newlines.\"\"\"\n    if not text:\n        return \"\"\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    # Collapse spaces/tabs inside lines\n    text = \"\\n\".join(re.sub(r\"[ \\t]+\", \" \", ln).strip() for ln in text.split(\"\\n\"))\n    # Collapse too many blank lines\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n    return text\n\ndef looks_like_blocked_or_boilerplate(text: str) -> bool:\n    \"\"\"Detect common anti-bot/paywall/cookie boilerplate that inflates text.\"\"\"\n    s = (text or \"\").strip().lower()\n    if not s:\n        return False\n    bad_phrases = [\n        \"verify you are human\",\n        \"enable javascript\",\n        \"turn on javascript\",\n        \"access denied\",\n        \"unusual traffic\",\n        \"cookie\",\n        \"cookies\",\n        \"privacy policy\",\n        \"terms of use\",\n        \"we value your privacy\",\n        \"sign in to continue\",\n        \"subscribe to\",\n        \"accept all cookies\",\n        \"manage cookies\",\n    ]\n    return any(p in s for p in bad_phrases)\n\ndef _should_skip_line(line: str) -> bool:\n    s = (line or \"\").strip()\n    if not s:\n        return False\n    if len(s) <= 3:\n        return True\n    for rx in _SKIP_LINE_RES:\n        if rx.search(s):\n            return True\n    return False\n\ndef postprocess_document_text(text: str, max_chars: int = 50000) -> str:\n    \"\"\"\n    Normalize + remove boilerplate + cut references + de-duplicate paragraphs.\n    Keeps 'TITLE:' / 'ABSTRACT:' markers if present.\n    \"\"\"\n    text = normalize_text_keep_newlines(text or \"\")\n    if not text:\n        return \"\"\n\n    # Remove noisy single lines\n    lines = []\n    for ln in text.splitlines():\n        if _should_skip_line(ln):\n            continue\n        if looks_like_blocked_or_boilerplate(ln):\n            continue\n        lines.append(ln.rstrip())\n    text = \"\\n\".join(lines).strip()\n    if not text:\n        return \"\"\n\n    # Cut everything after an explicit references/bibliography header line\n    m = re.search(r\"(?im)^\\s*(references|bibliography)\\s*$\", text)\n    if m:\n        text = text[:m.start()].rstrip()\n\n    # Keep only the first TITLE: line if repeated\n    out_lines = []\n    title_seen = False\n    for ln in text.splitlines():\n        if ln.startswith(\"TITLE:\"):\n            if title_seen:\n                continue\n            title_seen = True\n        out_lines.append(ln)\n    text = \"\\n\".join(out_lines).strip()\n\n    # Paragraph de-duplication (removes duplicates anywhere, not only consecutive)\n    paras = [p.strip() for p in re.split(r\"\\n{2,}\", text) if p.strip()]\n    seen = set()\n    deduped = []\n    for p in paras:\n        key = re.sub(r\"\\W+\", \"\", p.lower())[:900]\n        if key in seen:\n            continue\n        seen.add(key)\n        deduped.append(p)\n\n    text = \"\\n\\n\".join(deduped).strip()\n\n    # Hard cap (avoid storing extremely large junk)\n    if len(text) > max_chars:\n        text = text[:max_chars].rstrip()\n\n    return text\n\n# Fetch HTML and return (html_text, final_url, status_code).\ndef fetch_html(url: str, timeout: int = 25):\n    r = session.get(\n        url,\n        headers={**BROWSER_HEADERS, \"Accept\": \"text/html,*/*;q=0.8\"},\n        timeout=timeout,\n        allow_redirects=True\n    )\n    return (r.text or \"\"), r.url, r.status_code\n\n# Extract readable main text from HTML (title/description/body).\ndef extract_main_text_from_html(html: str) -> str:\n    try:\n        from bs4 import BeautifulSoup\n    except ImportError:\n        raise ImportError(\"Install bs4 + lxml: pip install beautifulsoup4 lxml\")\n\n    soup = BeautifulSoup(html, \"lxml\")\n\n    # Remove obvious non-content areas\n    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"iframe\"]):\n        tag.decompose()\n    for tag in soup.find_all([\"header\", \"footer\", \"nav\", \"aside\", \"form\"]):\n        tag.decompose()\n\n    root = soup.find(\"main\") or soup.find(\"article\") or soup\n\n    title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n    desc = \"\"\n    m = soup.find(\"meta\", attrs={\"name\": \"description\"})\n    if m and m.get(\"content\"):\n        desc = m[\"content\"].strip()\n\n    chunks = []\n    for el in root.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"]):\n        t = el.get_text(\" \", strip=True)\n\n        # Stop when reaching tail sections (References, etc.)\n        if el.name in [\"h1\", \"h2\", \"h3\"]:\n            if (t or \"\").strip().lower() in _STOP_SECTION_TITLES:\n                break\n\n        if not t or len(t) < 30:\n            continue\n        if looks_like_blocked_or_boilerplate(t):\n            continue\n        if _should_skip_line(t):\n            continue\n\n        chunks.append(t)\n\n    # Fallback if structure is weak\n    if len(chunks) < 3:\n        t = re.sub(r\"\\s+\", \" \", root.get_text(\" \", strip=True)).strip()\n        chunks = [t] if t else []\n\n    # De-duplicate keeping order\n    chunks = list(dict.fromkeys(chunks))\n\n    # IMPORTANT: keep paragraph boundaries so paragraph de-dup works later\n    body = \"\\n\\n\".join(chunks).strip()\n\n    # Drop DESCRIPTION if it's basically contained in the body\n    if desc:\n        norm_desc = re.sub(r\"\\W+\", \"\", desc.lower())\n        norm_body_head = re.sub(r\"\\W+\", \"\", body.lower()[: max(2000, len(desc) * 3)])\n        if norm_desc and norm_desc in norm_body_head:\n            desc = \"\"\n\n    parts = []\n    if title:\n        parts.append(f\"TITLE: {title}\")\n    if desc:\n        parts.append(f\"DESCRIPTION: {desc}\")\n    if body:\n        parts.append(body)\n\n    return postprocess_document_text(\"\\n\\n\".join(parts).strip())\n\n\n# API lookup: Semantic Scholar (metadata + possible openAccessPdf).\ndef semantic_scholar_lookup(doi: str):\n    \"\"\"Return dict with title/abstract and possibly openAccessPdf url.\"\"\"\n    # No key required for basic use, but rate-limited.\n    url = f\"https://api.semanticscholar.org/graph/v1/paper/DOI:{quote(doi, safe='')}\"\n    params = {\"fields\": \"title,abstract,openAccessPdf,url\"}\n    r = session.get(url, params=params, headers=BROWSER_HEADERS, timeout=25)\n    if r.status_code != 200:\n        return None\n    return r.json()\n\n# API lookup: OpenAlex (metadata + locations + inverted-index abstract).\ndef openalex_lookup(doi: str):\n    url = f\"https://api.openalex.org/works/doi:{quote(doi, safe='')}\"\n    r = session.get(url, headers=BROWSER_HEADERS, timeout=25)\n    if r.status_code != 200:\n        return None\n    return r.json()\n\n# API lookup: Unpaywall (OA locations; requires email parameter).\ndef unpaywall_lookup(doi: str, email: str = \"test@example.com\"):\n    # Unpaywall requires an email parameter (your real email is best).\n    url = f\"https://api.unpaywall.org/v2/{quote(doi, safe='')}\"\n    r = session.get(url, params={\"email\": email}, headers=BROWSER_HEADERS, timeout=25)\n    if r.status_code != 200:\n        return None\n    return r.json()\n\n# Select the best PDF URL from available sources (Semantic Scholar/OpenAlex/Unpaywall).\ndef _pick_pdf_url_from_sources(ss=None, oa=None, up=None) -> str:\n    # 1) Semantic Scholar openAccessPdf\n    if ss and isinstance(ss, dict):\n        oap = ss.get(\"openAccessPdf\") or {}\n        if isinstance(oap, dict) and oap.get(\"url\"):\n            return oap[\"url\"]\n\n    # 2) OpenAlex primary_location / open_access\n    if oa and isinstance(oa, dict):\n        pl = oa.get(\"primary_location\") or {}\n        if isinstance(pl, dict):\n            pdf = pl.get(\"pdf_url\")\n            if pdf:\n                return pdf\n            landing = pl.get(\"landing_page_url\")\n            if landing and landing.lower().endswith(\".pdf\"):\n                return landing\n        oa2 = oa.get(\"open_access\") or {}\n        if isinstance(oa2, dict):\n            oa_url = oa2.get(\"oa_url\")\n            if oa_url and oa_url.lower().endswith(\".pdf\"):\n                return oa_url\n\n    # 3) Unpaywall best_oa_location\n    if up and isinstance(up, dict):\n        bol = up.get(\"best_oa_location\") or {}\n        if isinstance(bol, dict):\n            pdf = bol.get(\"url_for_pdf\")\n            if pdf:\n                return pdf\n            url = bol.get(\"url\")\n            if url and url.lower().endswith(\".pdf\"):\n                return url\n\n    return \"\"\n\n# Download a PDF and extract text from the first pages (bounded by max_pages).\ndef extract_text_from_pdf_url(pdf_url: str, max_pages: int = 8) -> str:\n    \"\"\"Download PDF and extract text from first max_pages pages.\"\"\"\n    r = session.get(\n        pdf_url,\n        headers={**BROWSER_HEADERS, \"Accept\": \"application/pdf,*/*;q=0.8\"},\n        timeout=40,\n        allow_redirects=True\n    )\n    if r.status_code != 200 or not r.content:\n        return \"\"\n\n    data = r.content\n\n    # pypdf\n    try:\n        from pypdf import PdfReader\n        import io\n        reader = PdfReader(io.BytesIO(data))\n        out = []\n        n = min(len(reader.pages), max_pages)\n        for i in range(n):\n            out.append(reader.pages[i].extract_text() or \"\")\n        text = \"\\n\".join(out).strip()\n        text = re.sub(r\"\\s+\", \" \", text)\n        return text.strip()\n    except Exception:\n        pass\n\n    # pdfminer\n    try:\n        from pdfminer.high_level import extract_text\n        import io\n        text = extract_text(io.BytesIO(data), maxpages=max_pages) or \"\"\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n    except Exception:\n        return \"\"\n\n# Utility: reconstruct OpenAlex inverted-index abstract into ordered text.\ndef _reconstruct_abstract_from_inverted_index(inv: dict) -> str:\n    \"\"\"\n    OpenAlex returns abstracts as an inverted index:\n    {word: [pos1, pos2, ...], ...}\n    This reconstructs the abstract text in the correct order.\n    \"\"\"\n    if not isinstance(inv, dict) or not inv:\n        return \"\"\n\n    # Find max position to size the token list\n    max_pos = -1\n    for positions in inv.values():\n        if isinstance(positions, list) and positions:\n            mp = max(positions)\n            if mp > max_pos:\n                max_pos = mp\n\n    if max_pos < 0:\n        return \"\"\n\n    tokens = [\"\"] * (max_pos + 1)\n\n    for word, positions in inv.items():\n        if not isinstance(positions, list):\n            continue\n        for p in positions:\n            if isinstance(p, int) and 0 <= p < len(tokens):\n                tokens[p] = word\n\n    # Join; remove empties\n    text = \" \".join(t for t in tokens if t)\n    return text.strip()\n\n\n# =========================\n# 3. DOI Metadata fallback (only if HTML is short)\n# =========================\n# Purpose: Fetch usable document text from a URL with fallbacks:\n# HTML -> Open-Access PDF (via APIs) -> Abstract metadata -> Title-only.\n\ndef get_document_text(url: str, min_chars: int = 800, unpaywall_email: str = \"test@example.com\", debug: bool = False) -> str:\n    \"\"\"\n    Returns the best available text we can legally obtain.\n    Order:\n      1) HTML extraction (if enough)\n      2) OA PDF extraction via Semantic Scholar / OpenAlex / Unpaywall (if available)\n      3) Abstract via Semantic Scholar/OpenAlex (fallback)\n      4) Title only\n    \"\"\"\n\n    # 1) Try extracting text directly from the landing HTML page.\n    html, final_url, status = fetch_html(url)\n    text_html_raw = extract_main_text_from_html(html) if html else \"\"\n    text_html = postprocess_document_text(text_html_raw)\n\n    # Optional debug printing for tracing which path is used.\n    if debug:\n        print(\"final_url:\", final_url)\n        print(\"html_status:\", status, \"| html_text_chars_raw:\", len(text_html_raw), \"| html_text_chars_clean:\", len(text_html))\n\n    # If cleaned HTML content is sufficient, return it immediately.\n    if len(text_html) >= min_chars:\n        return text_html\n\n    # 2) If input is a DOI link or a DOI string, try Open-Access routes (PDF).\n    u = (url or \"\").strip()\n    doi = \"\"\n    if \"doi.org/\" in u:\n        doi = _normalize_doi(u)\n    elif re.match(r\"^10\\.\\d{4,9}/\\S+$\", u):\n        doi = _normalize_doi(u)\n\n    ss = oa = up = None\n\n    if doi:\n        # Query multiple services to maximize chances of finding an OA PDF.\n        try:\n            ss = semantic_scholar_lookup(doi)\n        except Exception:\n            ss = None\n        try:\n            oa = openalex_lookup(doi)\n        except Exception:\n            oa = None\n        try:\n            up = unpaywall_lookup(doi, email=unpaywall_email)\n        except Exception:\n            up = None\n\n        # Pick the best PDF URL from the available sources (if any).\n        pdf_url = _pick_pdf_url_from_sources(ss=ss, oa=oa, up=up)\n        if debug:\n            print(\"pdf_url:\", pdf_url or \"(none)\")\n\n        # If we found a PDF, extract text from the first pages.\n        if pdf_url:\n            pdf_text_raw = extract_text_from_pdf_url(pdf_url, max_pages=8)\n            # Optionally prepend a title header when available.\n            title = (ss or {}).get(\"title\") or (oa or {}).get(\"title\") or \"\"\n            title = title.get(\"display_name\") if isinstance(title, dict) else title\n            header = f\"TITLE: {title}\".strip() if title else \"\"\n\n            merged = (header + \"\\n\" + pdf_text_raw).strip() if header else (pdf_text_raw or \"\").strip()\n            pdf_text = postprocess_document_text(merged)\n\n            if debug:\n                print(\"pdf_text_chars_raw:\", len(pdf_text_raw), \"| pdf_text_chars_clean:\", len(pdf_text))\n\n            if len(pdf_text) >= min_chars:\n                return pdf_text\n\n        # 3) If PDF is missing/short, fall back to abstract metadata (when present).\n        title = \"\"\n        abstract = \"\"\n\n        if ss and isinstance(ss, dict):\n            title = (ss.get(\"title\") or \"\").strip()\n            abstract = (ss.get(\"abstract\") or \"\").strip()\n\n        if (not abstract) and oa and isinstance(oa, dict):\n            t = oa.get(\"title\") or \"\"\n            title = title or t\n            # OpenAlex abstract can be stored as an inverted index.\n            inv = oa.get(\"abstract_inverted_index\")\n            if inv:\n                abstract = _reconstruct_abstract_from_inverted_index(inv)\n\n        # Return any metadata we managed to obtain.\n        if title or abstract:\n            parts = []\n            if title:\n                parts.append(f\"TITLE: {title}\")\n            if abstract:\n                parts.append(f\"ABSTRACT: {abstract}\")\n            return postprocess_document_text(\"\\n\".join(parts).strip())\n\n    # 4) Last resort: return thin HTML (title/description) if present.\n    if text_html:\n        return postprocess_document_text(text_html.strip())\n\n    # Final fallback: return the URL as a minimal title marker.\n    return postprocess_document_text(f\"TITLE: {url}\".strip())\n\n\n# =========================\n# 4. NLP Preprocessing\n# =========================\n# Purpose: Standardize text into comparable tokens for indexing/search.\n# Pipeline: tokenize -> stopword removal -> stemming.\n\ndef tokenize(text):\n    \"\"\"Convert text to a list of lowercase word tokens.\"\"\"\n    return re.findall(r\"\\w+\", (text or \"\").lower())\n\ndef remove_stopwords(tokens, stop_words):\n    \"\"\"Remove stop words from a list of tokens.\"\"\"\n    return [t for t in tokens if t not in stop_words]\n\ndef apply_stemming(tokens):\n    \"\"\"Apply Porter stemming to a list of tokens.\"\"\"\n    return [stemmer.stem(t) for t in tokens]\n\n\ndef preprocess_query(query: str):\n    # Goal: Apply the same normalization steps used for documents to the user query.\n    tokens = tokenize(query)\n    tokens = remove_stopwords(tokens, stop_words)\n    tokens = apply_stemming(tokens)\n    return tokens\n\n\n# =========================\n# 5. Document Text Access\n# =========================\n# Purpose: Fetch and store raw text for each document URL so it can be indexed later.\n# Output: doc_text dict mapping doc_id -> extracted text (may be empty on failure).\n\ndef build_doc_text_map(doc_urls, min_chars=800, unpaywall_email=\"test@example.com\", debug=False):\n    doc_text = {}\n    for i, url in enumerate(doc_urls):\n        # Try to extract the best available text (HTML/PDF/abstract fallback).\n        try:\n            doc_text[i] = get_document_text(\n                url,\n                min_chars=min_chars,\n                unpaywall_email=unpaywall_email,\n                debug=debug\n            ) or \"\"\n        except Exception as e:\n            # Keep the pipeline running even if a document fails to fetch.\n            print(f\"Failed to fetch document {i}: {e}\")\n            doc_text[i] = \"\"\n    return doc_text\n\n\n# Placeholder / initialization for the document-text mapping (doc_id -> text).\ndoc_text = {}\n\n\n# =========================\n# 6. Index Construction\n# =========================\n# Purpose: Build an inverted index for fast keyword-based retrieval.\n# - Input: raw document texts (doc_text) and preprocessing settings (stop_words).\n# - Output:\n#   1) inverted: term -> list of doc_ids containing the term\n#   2) doc_map: doc_id -> original URL\n\ndef build_inverted_index(urls, stop_words, doc_text):\n    inverted = defaultdict(set)  # term -> set(doc_ids)\n    doc_map = {i: url for i, url in enumerate(urls)}  # DocID -> URL\n\n    for doc_id in range(len(urls)):\n        # Get the stored text for this document (support int or string keys).\n        text = doc_text.get(doc_id) or doc_text.get(str(doc_id)) or \"\"\n\n        # Normalize document text into searchable terms.\n        tokens = tokenize(text)\n        tokens = remove_stopwords(tokens, stop_words)\n        tokens = apply_stemming(tokens)\n\n        # Add each unique term to the index for this document.\n        for term in set(tokens):\n            inverted[term].add(doc_id)\n\n    # Convert sets to sorted lists for stable output/serialization.\n    inverted = {term: sorted(list(ids)) for term, ids in inverted.items()}\n    return inverted, doc_map\n\n\n# =========================\n# 7. Firebase I/O (Updated: Only index new/missing papers)\n# =========================\n# Purpose: Persist and reuse the document store in Firebase.\n# Stored objects:\n# - doc_text: doc_id -> extracted text\n# - public_index: inverted index (term -> doc_ids)\n# - doc_map: doc_id -> source URL\n#\n# Indexing is done ONCE and saved to database. Only re-index if:\n# - Paper is new (not in doc_map)\n# - Paper text is missing (not in doc_text)\n\ndef save_to_firebase(data, path):\n    \"\"\"Write JSON data to a Firebase Realtime Database path using HTTP PUT.\"\"\"\n    base = FIREBASE_URL.rstrip(\"/\")\n    url = f\"{base}/{path}.json\"\n    r = requests.put(url, json=data, timeout=30)\n    print(\"PUT\", path, \"status:\", r.status_code, \"| resp:\", r.text[:200])\n    if r.status_code != 200:\n        raise RuntimeError(f\"PUT {path} failed: {r.status_code} | {r.text[:400]}\")\n    return r.status_code, r.text\n\n\ndef firebase_get(path):\n    \"\"\"Read JSON data from a Firebase Realtime Database path using HTTP GET.\"\"\"\n    base = FIREBASE_URL.rstrip(\"/\")\n    url = f\"{base}/{path}.json\"\n    r = requests.get(url, timeout=30)\n    if r.status_code != 200:\n        raise RuntimeError(f\"GET {path} failed: {r.status_code} | {r.text[:400]}\")\n    return r.json()\n\n\ndef check_existing_index():\n    \"\"\"Check if index already exists in Firebase.\"\"\"\n    try:\n        existing_map = firebase_get(MAP_PATH)\n        existing_index = firebase_get(INDEX_PATH)\n        if existing_map and existing_index and len(existing_map) > 0:\n            print(f\"\u2705 Existing index found: {len(existing_map)} papers indexed\")\n            return existing_map, existing_index\n    except Exception as e:\n        print(f\"No existing index found: {e}\")\n    return None, None\n\n\ndef get_missing_papers(urls, existing_map):\n    \"\"\"Find papers that need to be indexed (not in existing index).\"\"\"\n    if not existing_map:\n        return urls  # All need indexing\n    \n    existing_urls = set(existing_map.values()) if isinstance(existing_map, dict) else set(existing_map)\n    missing = [url for url in urls if url not in existing_urls]\n    \n    print(f\"\ud83d\udcca Papers status: {len(urls)} total, {len(urls) - len(missing)} already indexed, {len(missing)} new\")\n    return missing\n\n\ndef smart_build_and_save_index(\n    urls,\n    stop_words,\n    index_path=\"indexes/public_index\",\n    map_path=\"indexes/doc_map\",\n    text_path=\"indexes/doc_text\",\n    force_rebuild=False):\n    \"\"\"\n    Smart indexing: Only index new papers, merge with existing index.\n    Set force_rebuild=True to completely rebuild the index.\n    \"\"\"\n    \n    # Check for existing index\n    existing_map, existing_index = check_existing_index()\n    \n    if not force_rebuild and existing_map and existing_index:\n        missing_urls = get_missing_papers(urls, existing_map)\n        \n        if not missing_urls:\n            print(\"\u2705 All papers already indexed. No action needed.\")\n            return existing_index, existing_map\n        \n        print(f\"\ud83d\udd04 Indexing {len(missing_urls)} new papers...\")\n        urls_to_index = missing_urls\n    else:\n        print(\"\ud83d\udd04 Building complete index from scratch...\")\n        urls_to_index = urls\n        existing_map = {}\n        existing_index = {}\n    \n    # Build doc_text for new papers\n    doc_text_local = build_doc_text_map(urls_to_index)\n    \n    # Merge with existing text if any\n    if existing_map:\n        try:\n            existing_text = firebase_get(text_path) or {}\n            if isinstance(existing_text, dict):\n                # Start ID from after existing docs\n                start_id = max([int(k) for k in existing_text.keys()], default=-1) + 1\n                doc_text_local = {str(start_id + i): v for i, v in enumerate(doc_text_local.values())}\n                doc_text_local.update(existing_text)\n        except:\n            pass\n    \n    # Save doc_text\n    doc_text_to_save = {str(k): v for k, v in doc_text_local.items()}\n    save_to_firebase(doc_text_to_save, text_path)\n    print(\"\u2705 doc_text saved\")\n    \n    # Pre-compute vector embeddings for fast retrieval\n    try:\n        build_and_save_embeddings(doc_text_to_save, \"indexes/embeddings\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Embedding generation skipped: {e}\")\n\n    # Build and merge index\n    inv_index, doc_map = build_inverted_index(urls, stop_words, doc_text_local)\n    \n    # Save index and map\n    save_to_firebase(inv_index, index_path)\n    print(\"\u2705 index saved\")\n    \n    doc_map_json = {str(k): v for k, v in doc_map.items()}\n    save_to_firebase(doc_map_json, map_path)\n    print(\"\u2705 doc_map saved\")\n\n    return inv_index, doc_map\n\n\n# Legacy function for backwards compatibility\ndef build_and_save_index(\n    urls,\n    stop_words,\n    index_path=\"indexes/public_index\",\n    map_path=\"indexes/doc_map\",\n    text_path=\"indexes/doc_text\"):\n    \"\"\"\n    Original function - now redirects to smart indexing.\n    Only indexes new papers, skips already indexed ones.\n    \"\"\"\n    return smart_build_and_save_index(urls, stop_words, index_path, map_path, text_path, force_rebuild=False)\n\n\n# =========================\n# 8 Load store from Firebase\n# =========================\n# Purpose: Load previously saved index artifacts from Firebase into runtime globals.\n# Loads: public_index (inverted index), doc_map (doc_id -> URL), and optionally doc_text.\n\nINDEX_PATH = \"indexes/public_index\"\nMAP_PATH   = \"indexes/doc_map\"\nTEXT_PATH  = \"indexes/doc_text\"\n\npublic_index = None\ndoc_map = None\ndoc_text = None\n\ndef load_store_from_firebase(load_text: bool = False):\n    # Load index + doc_map from Firebase into globals\n    global public_index, doc_map, doc_text\n\n    # Always load index and doc_map (use empty dict if missing).\n    public_index = firebase_get(INDEX_PATH) or {}\n    doc_map = firebase_get(MAP_PATH) or {}\n\n    # Normalize doc_map shape if Firebase returns a JSON array instead of an object.\n    if isinstance(doc_map, list):\n      doc_map = {str(i): v for i, v in enumerate(doc_map)}\n\n    # Optionally load the full document texts (can be large).\n    if load_text:\n        doc_text = firebase_get(TEXT_PATH) or {}\n\n    # Basic summary for quick sanity-check.\n    print(\"Loaded:\",\n          \"terms=\", len(public_index),\n          \"| docs=\", len(doc_map) if hasattr(doc_map, \"__len__\") else type(doc_map))\n\n    return public_index, doc_map, doc_text\n\n\n# =========================\n# 9. Retrieval (uses data loaded from Firebase)\n# =========================\n# Purpose: Perform simple keyword-based retrieval using the inverted index.\n# Method: preprocess query -> count matching terms per document -> return top-k docs.\n\ndef search_top_k(query: str, k: int = 3):\n    # Ensure index and doc_map are available in memory (load if missing).\n    if public_index is None or doc_map is None:\n        load_store_from_firebase(load_text=False)\n\n    # Convert the raw query into normalized terms (tokenize/stopwords/stemming).\n    q_terms = preprocess_query(query)\n    scores = defaultdict(int)\n\n    # Score documents by how many query terms they contain.\n    for term in q_terms:\n        for doc_id in (public_index.get(term, []) or []):\n            scores[int(doc_id)] += 1\n\n    # Rank by score and keep the top-k.\n    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n\n    # Build a compact result list with doc id, score, and source URL.\n    results = []\n    for doc_id, score in ranked:\n        url = doc_map.get(str(doc_id)) if isinstance(doc_map, dict) else None\n        results.append({\"doc_id\": doc_id, \"score\": score, \"url\": url})\n\n    return q_terms, results\n\n\n# English comments only\ndrive_link = \"https://drive.google.com/file/d/1vuCquKjZPwCHVNbRoLfAay7MxI7MjOru/view?usp=sharing\"\nm = re.search(r\"/d/([a-zA-Z0-9_-]+)\", drive_link) or re.search(r\"id=([a-zA-Z0-9_-]+)\", drive_link)\nif not m:\n    raise RuntimeError(\"Could not extract file id from the link\")\n\nfile_id = m.group(1)\ndownload_url = f\"https://drive.google.com/uc?id={file_id}\"\n\nout_path = \"/content/my_text.txt\"\ngdown.download(download_url, out_path, quiet=False)\n\nwith open(out_path, \"r\", encoding=\"utf-8\") as f:\n    drive_key_1 = f.read().strip()\n\n# =========================================================\n# 10. Ranking (BM25) + Cerebras LLM Bridge\n# =========================================================\nimport math\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom collections import Counter, defaultdict\nfrom cerebras.cloud.sdk import Cerebras\nimport json\nimport re\nimport time\n\n# --- Cerebras Initialization ---\n# Ensure drive_key_1 exists from the credential download cell\nif 'drive_key_1' not in globals():\n    raise NameError(\"Variable 'drive_key_1' not found. Please run the Drive download cell first.\")\n\n# Initializing the Cerebras client with the user-provided key\nc_client = Cerebras(api_key=drive_key_1.strip())\n\n# Using Llama 3.1 8B as it is the standard for this project's microservices\nMODEL_ID = \"llama3.1-8b\"\n\n_DOC_TEXT_CACHE: Dict[int, str] = {}\n\ndef _ensure_stores_loaded():\n    \"\"\"Checks if the Firebase index and map are loaded.\"\"\"\n    if globals().get(\"doc_map\") is None or globals().get(\"public_index\") is None:\n        load_store_from_firebase(load_text=False)\n\ndef get_doc_text(doc_id: int) -> str:\n    \"\"\"Retrieves paper content from Firebase path 'indexes/doc_text'.\"\"\"\n    if doc_id in _DOC_TEXT_CACHE:\n        return _DOC_TEXT_CACHE[doc_id]\n\n    key = str(int(doc_id))\n    try:\n        # Fetching directly from the NoSQL data lake structure\n        v = firebase_get(f\"indexes/doc_text/{key}\")\n        if v:\n            _DOC_TEXT_CACHE[doc_id] = str(v).strip()\n            return _DOC_TEXT_CACHE[doc_id]\n    except Exception as e:\n        print(f\"Firebase Retrieval Error (Doc {doc_id}): {e}\")\n\n    return \"\"\n\ndef _all_doc_ids() -> List[int]:\n    \"\"\"Identifies all valid document IDs in the system.\"\"\"\n    _ensure_stores_loaded()\n    dm = globals().get(\"doc_map\")\n    if isinstance(dm, list):\n        return list(range(len(dm)))\n    if isinstance(dm, dict):\n        return [int(k) for k in dm.keys() if str(k).isdigit()]\n    return []\n\ndef _get_source_url(doc_id: int) -> str:\n    \"\"\"Returns the original URL/DOI for a given document.\"\"\"\n    dm = globals().get(\"doc_map\")\n    key = str(doc_id)\n    if isinstance(dm, dict):\n        return str(dm.get(key, dm.get(int(doc_id), \"\")))\n    if isinstance(dm, list) and 0 <= doc_id < len(dm):\n        return str(dm[doc_id])\n    return \"\"\n\ndef bm25_rank(query: str, k: int = 3) -> Tuple[List[str], List[Tuple[int, float]]]:\n    \"\"\"Ranks documents based on the BM25 algorithm using query terms.\"\"\"\n    _ensure_stores_loaded()\n    q_terms = preprocess_query(query)\n    doc_ids = _all_doc_ids()\n\n    # BM25 Hyperparameters\n    N = len(doc_ids)\n    avgdl = 1000\n    k1, b = 1.5, 0.75\n    scores = defaultdict(float)\n\n    for did in doc_ids:\n        text = get_doc_text(did)\n        if not text: continue\n\n        tokens = preprocess_query(text)\n        tf = Counter(tokens)\n        dl = len(tokens)\n\n        for term in q_terms:\n            if term in tf:\n                # BM25 Scoring formula implementation\n                scores[did] += tf[term] * (k1 + 1) / (tf[term] + k1 * (1 - b + b * dl / avgdl))\n\n    ranked = sorted(scores.items(), key=lambda x: -x[1])[:k]\n    return q_terms, ranked\n\ndef llm_generate(prompt: str, temperature: float = 0.0, max_output_tokens: int = 512) -> str:\n    \"\"\"Calls Cerebras API with retries to avoid connection drops.\"\"\"\n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            response = c_client.chat.completions.create(\n                model=MODEL_ID,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a specialized plant pathologist assistant.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=temperature,\n                max_tokens=max_output_tokens,\n            )\n            return response.choices[0].message.content.strip()\n        except Exception as e:\n            err = str(e)\n            if (\"429\" in err or \"connection\" in err.lower()) and attempt < max_retries - 1:\n                time.sleep(5 * (attempt + 1))\n                continue\n            return \"\"\n    return \"\"\n\n# =========================================================\n# 11. RAG Core (Updated: Returns BM25 Scores + Citation Excerpts)\n# =========================================================\n\ndef _chunk_text(text: str, max_chars: int = 4000) -> List[str]:\n    \"\"\"\n    Splits text into overlapping chunks.\n    Overlap helps preserve context between chunks.\n    \"\"\"\n    if not text: return []\n    overlap = 400\n    chunks = []\n    for i in range(0, len(text), max_chars - overlap):\n        chunks.append(text[i : i + max_chars])\n    return chunks\n\ndef _extract_evidence_from_chunk(question: str, doc_id: int, chunk_id: int, chunk_text: str) -> Dict[str, Any]:\n    \"\"\"\n    Uses the LLM to find evidence.\n    Includes robust JSON parsing and debug logging.\n    Returns evidence with citation excerpt.\n    \"\"\"\n    prompt = (\n        \"You are an expert academic researcher. Analyze the TEXT CHUNK to answer the QUESTION.\\n\"\n        \"Return ONLY a valid JSON object with these fields:\\n\"\n        \"- 'found': boolean (true if the answer is explicitly or implicitly in this text)\\n\"\n        \"- 'answer': string (technical summary of findings)\\n\"\n        \"- 'evidence': array of strings (supporting quotes - SHORT, max 2 sentences each)\\n\"\n        \"- 'citation_excerpt': string (1-2 sentence excerpt showing where this info appears)\\n\\n\"\n        f\"QUESTION: {question}\\n\\n\"\n        f\"TEXT CHUNK:\\n{chunk_text}\"\n    )\n\n    raw_response = llm_generate(prompt, temperature=0.0)\n\n    # Robust JSON extraction using regex\n    match = re.search(r\"\\{.*\\}\", raw_response, flags=re.DOTALL)\n    if match:\n        try:\n            res = json.loads(match.group(0))\n            # Normalize keys to lowercase for safety\n            res = {k.lower(): v for k, v in res.items()}\n            res.update({\"doc_id\": doc_id, \"chunk_id\": chunk_id})\n            return res\n        except:\n            pass\n    return {\"found\": False, \"doc_id\": doc_id, \"chunk_id\": chunk_id}\n\ndef _final_answer_from_evidence(question: str, evidence_packets: List[Dict[str, Any]]) -> str:\n    \"\"\"\n    Synthesizes the final response.\n    If evidence was found, it creates a professional answer.\n    \"\"\"\n    valid_evidence = [p for p in evidence_packets if p.get(\"found\") == True]\n\n    if not valid_evidence:\n        return \"I'm sorry, but I couldn't find specific information regarding this in the current database of papers.\"\n\n    context_blocks = []\n    for p in valid_evidence:\n        block = f\"Source {p['doc_id']} (Chunk {p['chunk_id']}): {p.get('answer', '')}\\n\"\n        if p.get(\"evidence\"):\n            block += f\"Quotes: {p['evidence']}\"\n        context_blocks.append(block)\n\n    combined_context = \"\\n---\\n\".join(context_blocks)\n\n    prompt = (\n        \"You are a professional agricultural scientist. Use the CONTEXT below to answer the QUESTION.\\n\"\n        \"Synthesize the information into a clear, technical response in English.\\n\\n\"\n        f\"CONTEXT:\\n{combined_context}\\n\\n\"\n        f\"QUESTION: {question}\\n\\n\"\n        \"RESPONSE:\"\n    )\n\n    return llm_generate(prompt, temperature=0.2)\n\ndef rag_answer_with_model(query: str, k: int = 3) -> Dict[str, Any]:\n    \"\"\"\n    Main RAG pipeline - now returns BM25 scores and citation excerpts for each paper.\n    \"\"\"\n    # 1. Retrieval with BM25 scores\n    q_terms, ranked = bm25_rank(query, k=k)\n    print(f\"Retrieval complete. Scanning top {len(ranked)} documents...\")\n\n    # 2. Evidence gathering with citation excerpts\n    all_evidence = []\n    paper_details = []  # Store detailed info for each paper\n    \n    for doc_id, bm25_score in ranked:\n        text = get_doc_text(doc_id)\n        source_url = _get_source_url(doc_id)\n        \n        paper_info = {\n            \"doc_id\": doc_id,\n            \"bm25_score\": round(bm25_score, 4),\n            \"url\": source_url,\n            \"citation_excerpts\": [],\n            \"found_evidence\": False\n        }\n        \n        if not text:\n            paper_details.append(paper_info)\n            continue\n\n        chunks = _chunk_text(text, max_chars=4500)\n        \n        for i, chunk in enumerate(chunks[:6]):\n            res = _extract_evidence_from_chunk(query, doc_id, i+1, chunk)\n            if res.get(\"found\"):\n                all_evidence.append(res)\n                paper_info[\"found_evidence\"] = True\n                # Add citation excerpt if found\n                excerpt = res.get(\"citation_excerpt\", res.get(\"answer\", \"\"))[:200]\n                if excerpt:\n                    paper_info[\"citation_excerpts\"].append(excerpt)\n\n        if paper_info[\"found_evidence\"]:\n            print(f\"  \u2713 Evidence found in Doc {doc_id} (Score: {bm25_score:.4f})\")\n        else:\n            print(f\"  \u2717 No evidence in Doc {doc_id} (Score: {bm25_score:.4f})\")\n        \n        paper_details.append(paper_info)\n\n    # 3. Final Synthesis\n    final_ans = _final_answer_from_evidence(query, all_evidence)\n\n    return {\n        \"answer\": final_ans,\n        \"sources\": [_get_source_url(did) for did, _ in ranked],\n        \"doc_ids\": [did for did, _ in ranked],\n        \"paper_details\": paper_details  # NEW: includes scores and citations\n    }\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 8: RAG Chat GUI\n# ============================================================================\n# =========================================================\n# 12. UI Adapter for Gradio (Updated: Shows Scores + Citations)\n# =========================================================\n\ndef rag_ui(query: str, k: int = 3):\n    \"\"\"Gradio-friendly bridge for the RAG system - now shows scores and citations.\"\"\"\n    if not query.strip():\n        return \"Please enter a question.\", \"\"\n\n    try:\n        result = rag_answer_with_model(query, k=int(k))\n        answer = result.get(\"answer\", \"NO_ANSWER_FOUND\")\n\n        # Format sources with BM25 scores and citation excerpts\n        paper_details = result.get(\"paper_details\", [])\n        source_lines = [\"\ud83d\udcda **Paper Details (BM25 Ranking):**\\n\"]\n        \n        for i, paper in enumerate(paper_details, 1):\n            score = paper.get(\"bm25_score\", 0)\n            url = paper.get(\"url\", \"Unknown\")\n            found = \"\u2713\" if paper.get(\"found_evidence\") else \"\u2717\"\n            excerpts = paper.get(\"citation_excerpts\", [])\n            \n            source_lines.append(f\"**{i}. {found} Score: {score:.4f}**\")\n            source_lines.append(f\"   URL: {url}\")\n            \n            if excerpts:\n                source_lines.append(f\"   \ud83d\udcdd Citation: _{excerpts[0][:150]}..._\")\n            source_lines.append(\"\")\n        \n        source_text = \"\\n\".join(source_lines)\n        return answer, source_text\n    except Exception as e:\n        return f\"Error: {str(e)}\", \"\"\n\ndef build_rag_chat_tab():\n    \"\"\"Constructs the RAG interface within the main app - now shows scores and citations.\"\"\"\n    gr.Markdown(\"### \ud83d\udd0d Plant Disease Research Assistant\")\n    gr.Markdown(\"_Returns BM25 relevance scores and citation excerpts for each paper_\")\n\n    with gr.Row():\n        with gr.Column():\n            q_input = gr.Textbox(label=\"Question\", placeholder=\"Ask about plant pathology papers...\", lines=2)\n            k_slider = gr.Slider(1, 5, value=3, step=1, label=\"Search Depth (Number of Papers)\")\n            ask_btn = gr.Button(\"Analyze Documents\", variant=\"primary\")\n\n    with gr.Row():\n        ans_out = gr.Textbox(label=\"Analysis Result\", lines=10, interactive=False)\n        src_out = gr.Markdown(label=\"Paper Scores & Citations\")\n\n    ask_btn.click(fn=rag_ui, inputs=[q_input, k_slider], outputs=[ans_out, src_out])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 9: Smart Chat (Cerebras + Sensors + RAG)\n# ============================================================================\n# Smart Chat - Uses Cerebras LLM connected to sensor data and RAG knowledge base\n\n# =========================================================\n# Smart Chat Configuration - Cerebras with Sensor + RAG Integration\n# =========================================================\n\n# Using the same Cerebras client (c_client) already initialized in Cell 51\nSMART_CHAT_MODEL = \"llama3.1-8b\"\n\ndef get_current_sensor_summary() -> str:\n    \"\"\"Fetches current sensor data and creates a summary for the chat context.\"\"\"\n    try:\n        df = get_cached_sensor_data() if \"get_cached_sensor_data\" in dir() else load_data_from_firebase()\n        if df.empty:\n            return \"No sensor data available.\"\n        \n        # Get latest readings\n        latest = df.iloc[-1]\n        \n        # Get averages from last 24 hours\n        recent = df.tail(min(100, len(df)))\n        \n        summary = f\"\"\"\nCurrent Sensor Readings (Latest):\n- Temperature: {latest['temperature']:.1f}\u00b0C\n- Humidity: {latest['humidity']:.1f}%\n- Soil Moisture: {latest['soil']:.1f}%\n\n24-Hour Statistics:\n- Avg Temperature: {recent['temperature'].mean():.1f}\u00b0C (Range: {recent['temperature'].min():.1f} - {recent['temperature'].max():.1f})\n- Avg Humidity: {recent['humidity'].mean():.1f}% (Range: {recent['humidity'].min():.1f} - {recent['humidity'].max():.1f})\n- Avg Soil Moisture: {recent['soil'].mean():.1f}% (Range: {recent['soil'].min():.1f} - {recent['soil'].max():.1f})\n\"\"\"\n        return summary\n    except Exception as e:\n        return f\"Error fetching sensor data: {str(e)}\"\n\ndef get_rag_context(query: str, k: int = 2) -> str:\n    \"\"\"Fetches relevant RAG context for the query.\"\"\"\n    try:\n        q_terms, ranked = bm25_rank(query, k=k)\n        if not ranked:\n            return \"\"\n        \n        context_parts = []\n        for doc_id, score in ranked:\n            text = get_doc_text(doc_id)\n            if text:\n                # Get first 500 chars as context\n                excerpt = text[:500].replace(\"\\n\", \" \")\n                url = _get_source_url(doc_id)\n                context_parts.append(f\"Paper (Score {score:.2f}): {excerpt}... [Source: {url}]\")\n        \n        return \"\\n\\n\".join(context_parts) if context_parts else \"\"\n    except Exception:\n        return \"\"\n\ndef build_smart_system_prompt(sensor_data: str, rag_context: str) -> str:\n    \"\"\"Builds a comprehensive system prompt with sensor and RAG context.\"\"\"\n    prompt = \"\"\"You are an expert agricultural consultant and plant pathologist assistant.\nYou have access to real-time IoT sensor data from a smart garden and a knowledge base of scientific papers on plant diseases.\n\nCURRENT SENSOR DATA:\n{sensors}\n\nRELEVANT SCIENTIFIC KNOWLEDGE:\n{rag}\n\nUse this information to provide professional, data-driven advice about plant care, disease identification, and environmental optimization.\nAlways cite specific sensor readings when relevant. If the user asks about plant diseases, reference the scientific knowledge when applicable.\nBe helpful, accurate, and professional.\"\"\"\n    \n    return prompt.format(\n        sensors=sensor_data if sensor_data else \"No sensor data available\",\n        rag=rag_context if rag_context else \"No specific scientific context retrieved for this query\"\n    )\n\n\ndef cerebras_smart_turn(user_message: str, history: list, temperature: float = 0.7):\n    \"\"\"\n    Smart chat turn using Cerebras LLM with sensor data and RAG context.\n    \"\"\"\n    user_message = (user_message or \"\").strip()\n    if not user_message:\n        return \"\", history, history\n\n    # Gather context\n    sensor_summary = get_current_sensor_summary()\n    rag_context = get_rag_context(user_message, k=2)\n    system_prompt = build_smart_system_prompt(sensor_summary, rag_context)\n\n    # Build conversation messages\n    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n    \n    # Add conversation history\n    for msg in history:\n        role = msg.get(\"role\", \"user\")\n        content = (msg.get(\"content\") or \"\").strip()\n        if content:\n            messages.append({\"role\": role, \"content\": content})\n    \n    # Add current user message\n    messages.append({\"role\": \"user\", \"content\": user_message})\n\n    try:\n        response = c_client.chat.completions.create(\n            model=SMART_CHAT_MODEL,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=800,\n        )\n        answer = response.choices[0].message.content.strip()\n    except Exception as e:\n        answer = f\"Error: {str(e)}\"\n\n    if not answer:\n        answer = \"I couldn't generate a response. Please try again.\"\n\n    new_history = history + [\n        {\"role\": \"user\", \"content\": user_message},\n        {\"role\": \"assistant\", \"content\": answer},\n    ]\n    return \"\", new_history, new_history\n\n\ndef build_smart_chat_tab():\n    \"\"\"Smart Chat powered by Cerebras with sensor data and RAG integration.\"\"\"\n    gr.Markdown(\"## \ud83d\udcac Smart Chat (Cerebras AI)\")\n    gr.Markdown(\"_Connected to sensor database and scientific papers. Ask about plant care, diseases, or your current readings._\")\n\n    chat = gr.Chatbot(label=\"Chat\")\n    state = gr.State([])\n\n    msg = gr.Textbox(label=\"Message\", placeholder=\"Ask about your plants, sensor data, or plant diseases...\", lines=2)\n    temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.7, step=0.05, label=\"Creativity\")\n\n    with gr.Row():\n        send_btn = gr.Button(\"Send\", variant=\"primary\")\n        clear_btn = gr.Button(\"Clear\")\n\n    send_btn.click(\n        fn=cerebras_smart_turn,\n        inputs=[msg, state, temperature],\n        outputs=[msg, chat, state],\n    )\n\n    msg.submit(\n        fn=cerebras_smart_turn,\n        inputs=[msg, state, temperature],\n        outputs=[msg, chat, state],\n    )\n\n    def clear_chat():\n        return [], []\n\n    clear_btn.click(fn=clear_chat, inputs=[], outputs=[chat, state])\n\n# Keep old function name for backwards compatibility\ndef build_gemini_chat_tab():\n    return build_smart_chat_tab()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 10: Gamification & Tab Registry\n# ============================================================================\n# =========================================================\n# LOGIC \u2014 Mission Rewards System (Gamification)\n# =========================================================\n\n# ---------------------------------------------------------\n# 1) Configuration + Firebase Storage Path\n#    - Where we store the profile (global, no username)\n# ---------------------------------------------------------\n\nTZ_NAME = \"Asia/Jerusalem\"\nGAMIFICATION_REF = db.reference(\"gamification/global\")\n\n\n# ---------------------------------------------------------\n# 2) Time Helpers\n#    - Used for \"once per day\" missions and timestamps\n# ---------------------------------------------------------\n\ndef _today_key():\n    from datetime import datetime\n    from zoneinfo import ZoneInfo\n    return datetime.now(ZoneInfo(TZ_NAME)).strftime(\"%Y-%m-%d\")\n\ndef _now_iso():\n    from datetime import datetime\n    from zoneinfo import ZoneInfo\n    return datetime.now(ZoneInfo(TZ_NAME)).isoformat()\n\n\n# ---------------------------------------------------------\n# 3) Default Profile Model\n#    - The schema we expect to exist in Firebase\n# ---------------------------------------------------------\n\nDEFAULT_PROFILE = {\n    \"points\": 0,\n    \"spins_available\": 0,  # 1 spin token per completed mission\n    \"missions\": {\n        \"sync_data\": {\"last_completed\": None, \"total_completed\": 0},\n        \"analyze_plant\": {\"last_completed\": None, \"total_completed\": 0},\n        \"generate_report\": {\"last_completed\": None, \"total_completed\": 0},\n    },\n    \"coupons\": []  # list of dicts: {code,label,created_at,redeemed}\n}\n\n\n# ---------------------------------------------------------\n# 4) Profile I/O (Read / Write)\n#    - Loads from Firebase and merges missing keys safely\n# ---------------------------------------------------------\n\ndef _get_profile():\n    p = GAMIFICATION_REF.get() or {}\n\n    prof = {\n        \"points\": int(p.get(\"points\", DEFAULT_PROFILE[\"points\"])),\n        \"spins_available\": int(p.get(\"spins_available\", DEFAULT_PROFILE[\"spins_available\"])),\n        \"missions\": p.get(\"missions\", {}) or {},\n        \"coupons\": p.get(\"coupons\", []) or [],\n    }\n\n    # Merge default missions safely (ensures all mission keys exist)\n    merged = {}\n    for mid, base in DEFAULT_PROFILE[\"missions\"].items():\n        m = (prof[\"missions\"].get(mid) or {})\n        merged[mid] = {\n            \"last_completed\": m.get(\"last_completed\", base[\"last_completed\"]),\n            \"total_completed\": int(m.get(\"total_completed\", base[\"total_completed\"])),\n        }\n    prof[\"missions\"] = merged\n    return prof\n\ndef _save_profile(prof: dict):\n    GAMIFICATION_REF.set(prof)\n\n\n# ---------------------------------------------------------\n# 5) Mission Completion Rule (Once per Day)\n#    - Adds points + 1 spin if not completed today\n# ---------------------------------------------------------\n\ndef complete_mission(mission_id: str, points: int):\n    \"\"\"\n    Returns: (prof, earned_today: bool)\n    Logic: once-per-day per mission to prevent spam clicking.\n    Reward: +points AND +1 spin token.\n    \"\"\"\n    prof = _get_profile()\n    today = _today_key()\n\n    m = prof[\"missions\"].get(mission_id, {\"last_completed\": None, \"total_completed\": 0})\n    if m.get(\"last_completed\") == today:\n        return prof, False\n\n    prof[\"points\"] = int(prof.get(\"points\", 0)) + int(points)\n    prof[\"spins_available\"] = int(prof.get(\"spins_available\", 0)) + 1\n\n    m[\"last_completed\"] = today\n    m[\"total_completed\"] = int(m.get(\"total_completed\", 0)) + 1\n    prof[\"missions\"][mission_id] = m\n\n    _save_profile(prof)\n    return prof, True\n\n\n# ---------------------------------------------------------\n# 6) Wheel Rewards (Spin Logic)\n#    - Consumes 1 spin token and grants points/coupon\n# ---------------------------------------------------------\n\nWHEEL_REWARDS = [\n    (\"+5 points\",  {\"points\": 5}),\n    (\"+10 points\", {\"points\": 10}),\n    (\"+20 points\", {\"points\": 20}),\n    (\"Coupon: 5% off\",  {\"coupon\": {\"base\": \"CG-5OFF\", \"label\": \"5% off\"}}),\n    (\"Coupon: 10% off\", {\"coupon\": {\"base\": \"CG-10OFF\",\"label\": \"10% off\"}}),\n]\n\ndef spin_wheel():\n    \"\"\"\n    Consumes 1 spin token and gives either points or a coupon.\n    Returns: (message, prof)\n    \"\"\"\n    prof = _get_profile()\n    spins = int(prof.get(\"spins_available\", 0))\n    if spins <= 0:\n        return \"No spins available. Complete a mission to earn a spin!\", prof\n\n    prof[\"spins_available\"] = spins - 1\n\n    label, payload = random.choice(WHEEL_REWARDS)\n\n    if \"points\" in payload:\n        prof[\"points\"] = int(prof.get(\"points\", 0)) + int(payload[\"points\"])\n\n    if \"coupon\" in payload:\n        # Make coupon code unique (so you can redeem multiple coupons)\n        from datetime import datetime\n        from zoneinfo import ZoneInfo\n        suffix = datetime.now(ZoneInfo(TZ_NAME)).strftime(\"%H%M%S\")\n        code = f\"{payload['coupon']['base']}-{suffix}\"\n\n        prof[\"coupons\"].append({\n            \"code\": code,\n            \"label\": payload[\"coupon\"][\"label\"],\n            \"created_at\": _now_iso(),\n            \"redeemed\": False,\n        })\n\n    _save_profile(prof)\n    return f\"\ud83c\udfa1 You got: {label}\", prof\n\n\n# ---------------------------------------------------------\n# 7) Voucher Redemption by Points (Redeem Logic)\n#    - User chooses a tier, points are deducted, voucher created\n# ---------------------------------------------------------\n\nREDEEM_TIERS = {\n    \"5\u20aa Voucher (100 pts)\": 100,\n    \"10\u20aa Voucher (200 pts)\": 200,\n    \"20\u20aa Voucher (350 pts)\": 350,\n}\n\ndef redeem_voucher(tier_label: str):\n    \"\"\"\n    On Redeem click:\n    - If enough points: subtract cost, create voucher (coupon) and save to Firebase.\n    - Else: return message only.\n    Returns: (message, prof)\n    \"\"\"\n    prof = _get_profile()\n\n    if tier_label not in REDEEM_TIERS:\n        return \"Please select a voucher tier.\", prof\n\n    cost = int(REDEEM_TIERS[tier_label])\n    if int(prof.get(\"points\", 0)) < cost:\n        missing = cost - int(prof.get(\"points\", 0))\n        return f\"Not enough points. You need {missing} more.\", prof\n\n    # Create a simple unique voucher code\n    suffix = datetime.now(ZoneInfo(TZ_NAME)).strftime(\"%H%M%S\")\n    code = f\"VOUCH-{suffix}\"\n\n    prof[\"points\"] = int(prof.get(\"points\", 0)) - cost\n    prof[\"coupons\"].append({\n        \"code\": code,\n        \"label\": tier_label.split(\" (\")[0],  # \"5\u20aa Voucher\"\n        \"created_at\": _now_iso(),\n        \"redeemed\": True,  # since user redeemed it now\n    })\n\n    _save_profile(prof)\n    return f\"\u2705 Voucher created! Code: {code}\", prof\n\n\n# ---------------------------------------------------------\n# 8) Formatting Helpers (for UI display)\n#    - Converts profile data into nice text/markdown\n# ---------------------------------------------------------\n\ndef _format_missions_md(prof: dict) -> str:\n    today = _today_key()\n    m = prof.get(\"missions\", {}) or {}\n\n    def row(mid, title):\n        last = (m.get(mid, {}) or {}).get(\"last_completed\")\n        total = (m.get(mid, {}) or {}).get(\"total_completed\", 0)\n        done = (last == today)\n        return f\"- **{title}**: {'\u2705 Done today' if done else '\u2b1c Not done today'} (total: {total})\"\n\n    lines = [\n        \"### Daily Missions\",\n        row(\"sync_data\", \"Sync New Data\"),\n        row(\"analyze_plant\", \"Analyze a Plant Image\"),\n        row(\"generate_report\", \"Generate a Report\"),\n    ]\n    return \"\\n\".join(lines)\n\ndef _format_coupons_text(prof: dict) -> str:\n    coupons = prof.get(\"coupons\", []) or []\n    if not coupons:\n        return \"No coupons yet.\"\n\n    lines = []\n    for c in coupons[-15:][::-1]:\n        status = \"REDEEMED\" if c.get(\"redeemed\") else \"ACTIVE\"\n        lines.append(f\"{c.get('code')} | {c.get('label','Coupon')} | {status}\")\n    return \"\\n\".join(lines)\n\n\n# ---------------------------------------------------------\n# 9) UI Handlers (Gradio outputs)\n#    - Functions that return values matching Gradio outputs\n# ---------------------------------------------------------\n\ndef rewards_refresh():\n    prof = _get_profile()\n    return (\n        str(prof.get(\"points\", 0)),\n        str(prof.get(\"spins_available\", 0)),\n        _format_missions_md(prof),\n        _format_coupons_text(prof),\n    )\n\ndef rewards_spin():\n    msg, _ = spin_wheel()\n    pts, spins, missions_md, coupons_txt = rewards_refresh()\n    return msg, pts, spins, missions_md, coupons_txt\n\ndef rewards_redeem(tier_label: str):\n    msg, _ = redeem_voucher(tier_label)\n    pts, spins, missions_md, coupons_txt = rewards_refresh()\n    return msg, pts, spins, missions_md, coupons_txt\n\n\n# ---------------------------------------------------------\n# 10) Mission Wrappers (connect app actions -> mission rewards)\n#     - Wrap existing app functions and award missions if valid\n# ---------------------------------------------------------\n\ndef sync_screen_gamified():\n    msg, count = sync_new_data_from_server()\n    # Mission completed only if new rows were actually saved\n    if count and count > 0:\n        prof, earned = complete_mission(\"sync_data\", points=10)\n        if earned:\n            msg += f\"\\n\ud83c\udfae Rewards: +10 points, +1 spin (Spins now: {prof['spins_available']})\"\n    return msg\n\ndef analyze_plant_gamified(image, temp, humidity, soil):\n    diagnosis, status_html, alerts, recommendations = analyze_plant(image, temp, humidity, soil)\n\n    # Mission completed only if an image was actually provided\n    if image:\n        prof, earned = complete_mission(\"analyze_plant\", points=8)\n        if earned:\n            recommendations = (recommendations or \"\")\n            recommendations += f\"\\n\\n\ud83c\udfae Rewards: +8 points, +1 spin (Spins now: {prof['spins_available']})\"\n\n    return diagnosis, status_html, alerts, recommendations\n\ndef generate_report_screen_gamified(limit: int):\n    status, file_path = generate_report_screen(limit)\n    # Mission completed only if report file was actually generated\n    if file_path:\n        prof, earned = complete_mission(\"generate_report\", points=12)\n        if earned:\n            status += f\" | \ud83c\udfae +12 points, +1 spin (Spins now: {prof['spins_available']})\"\n    return status, file_path\n\n\ndef build_placeholder_tab(title: str, note: str = \"\u05db\u05d0\u05df \u05d9\u05d9\u05db\u05e0\u05e1 \u05d4\u05e7\u05d5\u05d3 \u05d1\u05d4\u05de\u05e9\u05da\"):\n    gr.Markdown(f\"## {title}\")\n    gr.Markdown(note)\n\n\n# -----------------------------\n# TABS (EMPTY PLACEHOLDERS)\n# -----------------------------\n\ndef build_iot_dashboard_tab():\n    \"\"\"Complete IoT Dashboard with all visualizations\"\"\"\n\n    gr.Markdown('### \ud83d\udcc8 Comprehensive Sensor Analytics')\n\n    refresh_btn = gr.Button('\ud83d\udd04 Refresh All Data', variant='primary', size='lg')\n\n    # KPI Cards\n    gr.Markdown('#### \ud83d\udccc Current Readings')\n    kpi_html = gr.HTML()\n\n    # Statistics Cards\n    gr.Markdown('#### \ud83d\udcca Statistical Summary')\n    stats_html = gr.HTML()\n\n    # Main Time Series\n    gr.Markdown('#### \ud83d\udcc8 Time Series Overview')\n    ts_plot = gr.Plot()\n\n    # Correlation Analysis\n    gr.Markdown('#### \ud83d\udd17 Correlation Analysis')\n    corr_card = gr.HTML()\n    corr_plot = gr.Plot()\n\n    # Hourly Patterns\n    gr.Markdown('#### \u23f0 Hourly Patterns')\n    hourly_card = gr.HTML()\n    hourly_plot = gr.Plot()\n\n    # Daily Trends\n    gr.Markdown('#### \ud83d\udcc5 Daily Trends')\n    daily_card = gr.HTML()\n    daily_plot = gr.Plot()\n\n    # Distribution Analysis\n    gr.Markdown('#### \ud83d\udcca Distribution Analysis (Histograms)')\n    dist_card = gr.HTML()\n    dist_plot = gr.Plot()\n\n    # Moving Averages\n    gr.Markdown('#### \ud83d\udcc9 Moving Averages')\n    with gr.Row():\n        ma_variable = gr.Dropdown(\n            choices=['temperature', 'humidity', 'soil'],\n            value='temperature',\n            label='Select Variable'\n        )\n        ma_btn = gr.Button('Generate Moving Average')\n    ma_card = gr.HTML()\n    ma_plot = gr.Plot()\n\n    # Wire up refresh button (11 outputs - without scatter)\n    refresh_btn.click(\n        dashboard_screen,\n        outputs=[\n            kpi_html, stats_html, ts_plot,\n            corr_card, corr_plot,\n            hourly_card, hourly_plot,\n            daily_card, daily_plot,\n            dist_card, dist_plot\n        ]\n    )\n\n    # Wire up moving average\n    ma_btn.click(\n        dashboard_moving_avg,\n        inputs=ma_variable,\n        outputs=[ma_card, ma_plot]\n    )\n\n\ndef build_search_engine_tab():\n    build_placeholder_tab(\"\ud83d\udd0d Search Engine\")\n\n#def build_rag_chat_tab():\n #   build_placeholder_tab(\"\ud83d\udcac RAG Chat\")\n\ndef build_sync_data_tab():\n    \"\"\"Sync data from server to Firebase\"\"\"\n\n    gr.Markdown('Sync Data from to Server')\n    gr.Markdown('Upload IoT Data to FireBase')\n\n    sync_btn = gr.Button('\ud83d\udd04 Sync New Data', variant='primary', size='lg')\n    sync_output = gr.Textbox(label='Status', lines=5)\n\n    sync_btn.click(sync_screen_gamified, outputs=sync_output)\n\ndef build_rewards_tab():\n    # ---------------------------------------------------------\n    # 1) Load current profile snapshot (initial values)\n    # ---------------------------------------------------------\n    prof = _get_profile()\n\n    # ---------------------------------------------------------\n    # 2) Title + short explanation\n    # ---------------------------------------------------------\n    gr.Markdown(\"## \ud83c\udfae Farm Rewards\")\n    gr.Markdown(\n        \"Complete daily missions to earn points and **1 spin per mission**. \"\n        \"Spin the wheel to win bonus points or vouchers (demo).\"\n    )\n\n    # ---------------------------------------------------------\n    # 3) KPI section (Points + Spins)\n    # ---------------------------------------------------------\n    with gr.Row():\n        points_box = gr.Textbox(\n            label=\"Points\",\n            value=str(prof.get(\"points\", 0)),\n            interactive=False\n        )\n        spins_box = gr.Textbox(\n            label=\"Spins available\",\n            value=str(prof.get(\"spins_available\", 0)),\n            interactive=False\n        )\n\n    # ---------------------------------------------------------\n    # 4) Missions + Vouchers display\n    # ---------------------------------------------------------\n    missions_md = gr.Markdown(_format_missions_md(prof))\n    coupons_txt = gr.Textbox(\n        label=\"Vouchers\",\n        value=_format_coupons_text(prof),\n        lines=7,\n        interactive=False\n    )\n\n    # ---------------------------------------------------------\n    # 5) Actions: Refresh + Spin\n    # ---------------------------------------------------------\n    with gr.Row():\n        refresh_btn = gr.Button(\"\ud83d\udd04 Refresh\", variant=\"secondary\")\n        spin_btn = gr.Button(\"\ud83c\udfa1 Spin the Wheel\", variant=\"primary\")\n\n    spin_result = gr.Textbox(\n        label=\"Spin result\",\n        lines=2,\n        interactive=False\n    )\n\n    # ---------------------------------------------------------\n    # 6) Redeem voucher by points (tier selection)\n    # ---------------------------------------------------------\n    gr.Markdown(\"### Redeem a Voucher\")\n\n    tier = gr.Dropdown(\n        choices=list(REDEEM_TIERS.keys()),\n        label=\"Select voucher tier\",\n        value=\"5\u20aa Voucher (100 pts)\"\n    )\n\n    redeem_btn = gr.Button(\"\ud83c\udf9f\ufe0f Redeem\", variant=\"primary\")\n\n    redeem_result = gr.Textbox(\n        label=\"Redeem status\",\n        lines=2,\n        interactive=False\n    )\n\n    # ---------------------------------------------------------\n    # 7) Wiring (connect buttons -> LOGIC handlers)\n    # ---------------------------------------------------------\n    refresh_btn.click(\n        fn=rewards_refresh,\n        outputs=[points_box, spins_box, missions_md, coupons_txt]\n    )\n\n    spin_btn.click(\n        fn=rewards_spin,\n        outputs=[spin_result, points_box, spins_box, missions_md, coupons_txt]\n    )\n\n    redeem_btn.click(\n        fn=rewards_redeem,\n        inputs=[tier],\n        outputs=[redeem_result, points_box, spins_box, missions_md, coupons_txt]\n    )\n\n    # ---------------------------------------------------------\n    # 8) Return component refs (for build_app auto-refresh)\n    # ---------------------------------------------------------\n    return points_box, spins_box, missions_md, coupons_txt\n\n\n\n\n\n\n# \u2705 Tab Registry - Single place to add/remove tabs\nTABS = [\n    (\"\ud83c\udf31 Realtime Dashboard\", build_realtime_dashboard_tab),\n    (\"\ud83d\udcca Analistic Dashboard\", build_iot_dashboard_tab),\n    (\"\ud83d\udcc4 Generate Report\", build_generate_report_tab),\n    (\"\ud83d\uddbc\ufe0f Plant Disease Detection\", build_plant_disease_detection_tab),\n    (\"\ud83d\udcac RAG Chat\", build_rag_chat_tab),\n    (\"\ud83d\udcac Smart Chat\", build_smart_chat_tab),  # Cerebras AI with Sensors + RAG\n    (\"\ud83d\udd04 Sync Data\", build_sync_data_tab),\n    (\"\ud83c\udfae Farm Rewards\", build_rewards_tab),\n]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 11: Visualization Functions\n# ============================================================================\n# @title\n# Cell 7: Complete Visualization Functions (WITHOUT SCATTER ANALYSIS)\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport numpy as np\n\n# Sensor configuration\nSENSORS = [\n    ('temperature', '\u00b0C', COLORS['temperature']['color'], COLORS['temperature']['color'], 'TEMPERATURE'),\n    ('humidity', '%', COLORS['humidity']['color'], COLORS['humidity']['color'], 'HUMIDITY'),\n    ('soil', '%', COLORS['soil']['color'], COLORS['soil']['color'], 'SOIL MOISTURE')\n]\nprint('\u2713 Sensor config loaded')\n\n# ============================================================================\n# COMPONENT FUNCTIONS\n# ============================================================================\ndef create_kpi_card(label, value, unit, change, change_label, trend=\"up\", border_color=None):\n    \"\"\"Create KPI card HTML.\"\"\"\n    bc = border_color or COLORS['temperature']['color']\n    icon = \"\u2191\" if trend == \"up\" else (\"\u2193\" if trend == \"down\" else \"\u2192\")\n    return f'''<div class=\"kpi-card\" style=\"border-left-color: {bc};\">\n        <p class=\"kpi-label\">{label}</p>\n        <p class=\"kpi-value\">{value}<span style=\"font-size: 24px;\">{unit}</span></p>\n        <p class=\"kpi-change trend-{trend}\"><span>{icon}</span><span>{change} {change_label}</span></p>\n    </div>'''\n\ndef create_status_badge(text=\"LIVE\", pulse=True):\n    \"\"\"Create status badge HTML.\"\"\"\n    pulse_dot = \"<span class='status-dot'></span>\" if pulse else \"\"\n    return f'<span class=\"status-badge\">{pulse_dot}{text}</span>'\n\ndef create_explanation_card(title, description, interpretation, gradient=None):\n    \"\"\"Create explanation card HTML.\"\"\"\n    grad = gradient or COLORS['temperature']['color']\n    return f'''<div class=\"explanation-card\" style=\"background: {grad};\">\n        <h3>\ud83d\udcca {title}</h3><p><strong>What it shows:</strong> {description}</p>\n        <p><strong>How to interpret:</strong> {interpretation}</p></div>'''\n\nprint('\u2713 Component functions loaded')\n\n# ============================================================================\n# STATISTICS CARDS\n# ============================================================================\ndef create_stat_cards_html(df):\n    \"\"\"Create comprehensive statistics cards for all sensors.\"\"\"\n    if len(df) == 0:\n        return \"<p>No data available</p>\"\n\n    explanations = {\n        'Mean': 'Average value. Sum \u00f7 count.',\n        'Median': 'Middle value. 50% above, 50% below.',\n        'Std Dev': 'Variability around mean. Low=consistent, High=variable.',\n        'Min': 'Lowest recorded value.',\n        'Max': 'Highest recorded value.',\n        'Q25': '25th percentile. 25% below this.',\n        'Q75': '75th percentile. 75% below this.',\n        'IQR': 'Interquartile range (Q75-Q25).'\n    }\n\n    sensor_colors = {\n        'TEMPERATURE': {'bg': '#fee2e2', 'text': '#991b1b', 'border': '#ef4444'},\n        'HUMIDITY': {'bg': '#dbeafe', 'text': '#1e40af', 'border': '#3b82f6'},\n        'SOIL MOISTURE': {'bg': '#e9d5ff', 'text': '#6b21a8', 'border': '#8b5cf6'}\n    }\n\n    html = '<div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin: 20px 0;\">'\n\n    for var, unit, _, _, name in SENSORS:\n        stats = {k: round(v, 2) for k, v in {\n            'Mean': df[var].mean(),\n            'Median': df[var].median(),\n            'Std Dev': df[var].std(),\n            'Min': df[var].min(),\n            'Max': df[var].max(),\n            'Q25': df[var].quantile(0.25),\n            'Q75': df[var].quantile(0.75),\n            'IQR': df[var].quantile(0.75) - df[var].quantile(0.25)\n        }.items()}\n\n        colors = sensor_colors[name]\n\n        html += f'''\n        <div style=\"\n            background: {colors['bg']};\n            border: 3px solid {colors['border']};\n            border-radius: 12px;\n            padding: 20px;\n            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n        \">\n            <h2 style=\"\n                color: {colors['text']};\n                margin: 0 0 16px 0;\n                font-size: 20px;\n                font-weight: 700;\n                text-align: center;\n            \">{name}</h2>\n            <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 12px;\">\n        '''\n\n        for stat_name, stat_val in stats.items():\n            html += f'''\n            <div style=\"\n                background: white;\n                border-radius: 8px;\n                padding: 12px;\n                box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n            \">\n                <div style=\"\n                    display: flex;\n                    justify-content: space-between;\n                    align-items: center;\n                    margin-bottom: 4px;\n                \">\n                    <div style=\"\n                        font-size: 12px;\n                        font-weight: 600;\n                        color: {colors['text']};\n                    \">{stat_name}</div>\n                    <div class=\"info-icon\" style=\"\n                        width: 18px;\n                        height: 18px;\n                        border-radius: 50%;\n                        background: {colors['border']};\n                        color: white;\n                        display: flex;\n                        align-items: center;\n                        justify-content: center;\n                        font-size: 11px;\n                        font-weight: 700;\n                    \">i\n                        <span class=\"tooltip-text\">{explanations[stat_name]}</span>\n                    </div>\n                </div>\n                <div style=\"\n                    font-size: 24px;\n                    font-weight: 700;\n                    color: {colors['text']};\n                \">{stat_val}{unit}</div>\n            </div>\n            '''\n\n        html += '</div></div>'\n\n    html += '</div>'\n    return html\n\nprint('\u2713 Statistics functions loaded')\n\n# ============================================================================\n# CHART STYLING\n# ============================================================================\ndef apply_chart_styling(fig, title=\"\", xaxis_title=\"\", yaxis_title=\"\", height=400):\n    \"\"\"Apply consistent styling to all charts.\"\"\"\n    fig.update_layout(\n        title=dict(text=title, font=dict(size=20)),\n        xaxis_title=xaxis_title,\n        yaxis_title=yaxis_title,\n        font=dict(family=\"Inter, sans-serif\", size=14),\n        plot_bgcolor='white',\n        paper_bgcolor='white',\n        height=height,\n        hovermode='x unified'\n    )\n    fig.update_xaxes(showgrid=False, title_font=dict(size=14, color='#6b7280'), tickfont=dict(size=12))\n    fig.update_yaxes(showgrid=True, gridcolor='#E5E7EB', title_font=dict(size=14, color='#6b7280'), tickfont=dict(size=12))\n    return fig\n\nprint('\u2713 Chart styling loaded')\n\n# ============================================================================\n# PLOT FUNCTIONS\n# ============================================================================\ndef time_series_overview(df):\n    \"\"\"Time series for all sensors.\"\"\"\n    fig = go.Figure()\n    for col, unit, color, _, _ in SENSORS:\n        fig.add_trace(go.Scatter(\n            x=df['timestamp'],\n            y=df[col],\n            name=col.capitalize(),\n            mode='lines',\n            line=dict(color=color, width=2),\n            hovertemplate=f'%{{y:.1f}}{unit}<extra></extra>'\n        ))\n    apply_chart_styling(fig, \"Sensor Data Time Series\", \"Time\", \"Measurement (\u00b0C / %)\", 500)\n    return create_explanation_card(\n        \"Time Series Overview\",\n        \"All sensor measurements over time.\",\n        \"Look for trends, cycles, and sudden changes.\"\n    ), fig\n\ndef calculate_correlations(df):\n    \"\"\"Correlation matrix between sensors.\"\"\"\n    corr = df[['temperature', 'humidity', 'soil']].corr()\n    fig = px.imshow(\n        corr,\n        labels=dict(color=\"Correlation\"),\n        x=['Temperature', 'Humidity', 'Soil'],\n        y=['Temperature', 'Humidity', 'Soil'],\n        color_continuous_scale='RdBu_r',\n        zmin=-1,\n        zmax=1,\n        aspect=\"auto\"\n    )\n    apply_chart_styling(fig, \"Correlation Matrix\", \"Variables\", \"Variables\", 500)\n\n    # Add correlation values\n    for i in range(len(corr)):\n        for j in range(len(corr)):\n            fig.add_annotation(\n                x=j, y=i,\n                text=str(round(corr.iloc[i, j], 3)),\n                showarrow=False,\n                font=dict(size=14, color='black' if abs(corr.iloc[i, j]) < 0.5 else 'white', weight=600)\n            )\n\n    return create_explanation_card(\n        \"Correlation Analysis\",\n        \"Linear relationships between sensors. +1=perfect positive, -1=perfect negative, 0=no relationship.\",\n        \"High correlations indicate sensors respond together.\",\n        COLORS['humidity']['color']\n    ), fig\n\ndef hourly_patterns(df):\n    \"\"\"Average values by hour of day.\"\"\"\n    df_copy = df.copy()\n    df_copy['hour'] = df_copy['timestamp'].dt.hour\n    hourly = df_copy.groupby('hour')[['temperature', 'humidity', 'soil']].mean()\n\n    fig = go.Figure()\n    for col, _, color, _, _ in SENSORS:\n        fig.add_trace(go.Scatter(\n            x=hourly.index,\n            y=hourly[col],\n            name=col.capitalize(),\n            mode='lines+markers',\n            line=dict(color=color, width=2.5),\n            marker=dict(size=8)\n        ))\n\n    apply_chart_styling(fig, \"Average Values by Hour\", \"Hour (0-23)\", \"Average Measurement (\u00b0C / %)\", 450)\n    return create_explanation_card(\n        \"Hourly Patterns\",\n        \"Average values per hour showing daily cycles.\",\n        \"Look for peaks and valleys that repeat daily.\",\n        COLORS['soil']['color']\n    ), fig\n\ndef daily_patterns(df):\n    \"\"\"Daily trends with min-max ranges.\"\"\"\n    df_copy = df.copy()\n    df_copy['date'] = df_copy['timestamp'].dt.date\n    daily = df_copy.groupby('date')[['temperature', 'humidity', 'soil']].agg(['mean', 'min', 'max'])\n\n    fig = make_subplots(\n        rows=3, cols=1,\n        subplot_titles=('Temperature (\u00b0C)', 'Humidity (%)', 'Soil (%)'),\n        vertical_spacing=0.08\n    )\n\n    for idx, (var, _, color, _, _) in enumerate(SENSORS, 1):\n        dates = [str(d) for d in daily.index]\n        r, g, b = int(color[1:3], 16), int(color[3:5], 16), int(color[5:7], 16)\n\n        # Min-max range\n        fig.add_trace(go.Scatter(x=dates, y=daily[var]['max'], mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip'), row=idx, col=1)\n        fig.add_trace(go.Scatter(x=dates, y=daily[var]['min'], mode='lines', line=dict(width=0),\n                                fill='tonexty', fillcolor=f\"rgba({r},{g},{b},0.2)\", showlegend=False, hoverinfo='skip'), row=idx, col=1)\n\n        # Mean line\n        fig.add_trace(go.Scatter(x=dates, y=daily[var]['mean'], mode='lines+markers',\n                                line=dict(color=color, width=2.5), marker=dict(size=6), name='Mean', showlegend=(idx==1)), row=idx, col=1)\n\n    fig.update_xaxes(title_text=\"Date\", row=3, col=1)\n    fig.update_yaxes(title_text=\"Temperature (\u00b0C)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Humidity (%)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Soil Moisture (%)\", row=3, col=1)\n    fig.update_layout(height=900)\n\n    return create_explanation_card(\n        \"Daily Trends\",\n        \"Daily means with min-max ranges (shaded).\",\n        \"Wider shading = more variability. Look for trends and unusual days.\"\n    ), fig\n\ndef distribution_analysis(df):\n    \"\"\"Histograms showing distribution of sensor values.\"\"\"\n    fig = make_subplots(\n        rows=1, cols=3,\n        subplot_titles=('Temperature (\u00b0C)', 'Humidity (%)', 'Soil Moisture (%)')\n    )\n\n    # Temperature histogram\n    temp_data = df['temperature'].values\n    temp_min, temp_max = temp_data.min(), temp_data.max()\n    temp_padding = (temp_max - temp_min) * 0.1\n    temp_bins = np.linspace(temp_min - temp_padding, temp_max + temp_padding, 31)\n    temp_counts, temp_edges = np.histogram(temp_data, bins=temp_bins)\n    temp_centers = (temp_edges[:-1] + temp_edges[1:]) / 2\n\n    fig.add_trace(go.Bar(\n        x=temp_centers,\n        y=temp_counts,\n        name='Temperature',\n        marker_color=COLORS['temperature']['color'],\n        width=(temp_max - temp_min) / 30 * 0.9,\n        hovertemplate='%{x:.1f}\u00b0C: %{y} readings<extra></extra>'\n    ), row=1, col=1)\n\n    # Humidity histogram\n    humidity_data = df['humidity'].values\n    humidity_min, humidity_max = humidity_data.min(), humidity_data.max()\n    humidity_padding = (humidity_max - humidity_min) * 0.1\n    humidity_bins = np.linspace(humidity_min - humidity_padding, humidity_max + humidity_padding, 31)\n    humidity_counts, humidity_edges = np.histogram(humidity_data, bins=humidity_bins)\n    humidity_centers = (humidity_edges[:-1] + humidity_edges[1:]) / 2\n\n    fig.add_trace(go.Bar(\n        x=humidity_centers,\n        y=humidity_counts,\n        name='Humidity',\n        marker_color=COLORS['humidity']['color'],\n        width=(humidity_max - humidity_min) / 30 * 0.9,\n        hovertemplate='%{x:.1f}%: %{y} readings<extra></extra>'\n    ), row=1, col=2)\n\n    # Soil histogram\n    soil_data = df['soil'].values\n    soil_min, soil_max = soil_data.min(), soil_data.max()\n    soil_padding = (soil_max - soil_min) * 0.1\n    soil_bins = np.linspace(soil_min - soil_padding, soil_max + soil_padding, 31)\n    soil_counts, soil_edges = np.histogram(soil_data, bins=soil_bins)\n    soil_centers = (soil_edges[:-1] + soil_edges[1:]) / 2\n\n    fig.add_trace(go.Bar(\n        x=soil_centers,\n        y=soil_counts,\n        name='Soil Moisture',\n        marker_color=COLORS['soil']['color'],\n        width=(soil_max - soil_min) / 30 * 0.9,\n        hovertemplate='%{x:.1f}%: %{y} readings<extra></extra>'\n    ), row=1, col=3)\n\n    # Configure axes\n    fig.update_xaxes(title_text=\"Temperature (\u00b0C)\", row=1, col=1)\n    fig.update_xaxes(title_text=\"Humidity (%)\", row=1, col=2)\n    fig.update_xaxes(title_text=\"Soil Moisture (%)\", row=1, col=3)\n    fig.update_yaxes(title_text=\"Number of Readings\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Number of Readings\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Number of Readings\", row=1, col=3)\n\n    fig.update_layout(height=400, showlegend=False, plot_bgcolor='white', paper_bgcolor='white')\n\n    return create_explanation_card(\n        \"Distribution Analysis\",\n        \"Frequency of sensor values. Tall bars = common values, short bars = rare values.\",\n        \"Look for the shape: bell curve = normal, multiple peaks = different patterns.\",\n        COLORS['humidity']['color']\n    ), fig\n\ndef time_series_decomposition(df, variable='temperature'):\n    \"\"\"Moving averages showing smoothed trends.\"\"\"\n    df_s = df.sort_values('timestamp').copy()\n\n    # Calculate moving averages with different windows\n    for window in [3, 10, 30]:\n        df_s[f'MA_{window}'] = df_s[variable].rolling(window, center=True).mean()\n\n    fig = go.Figure()\n\n    # Raw data\n    fig.add_trace(go.Scatter(\n        x=df_s['timestamp'],\n        y=df_s[variable],\n        name='Raw',\n        mode='lines',\n        line=dict(width=1, color='#4B5563'),\n        opacity=0.6\n    ))\n\n    # Moving averages\n    for ma, color, width in [('MA_3', '#10b981', 1.5), ('MA_10', '#3b82f6', 2.5), ('MA_30', '#ef4444', 3.5)]:\n        fig.add_trace(go.Scatter(\n            x=df_s['timestamp'],\n            y=df_s[ma],\n            name=ma,\n            line=dict(width=width, color=color)\n        ))\n\n    unit = '\u00b0C' if variable == 'temperature' else '%'\n    apply_chart_styling(fig, f'Moving Averages - {variable.capitalize()}', 'Time', f'{variable.capitalize()} ({unit})', 450)\n\n    return create_explanation_card(\n        \"Moving Averages\",\n        \"Smoothed trends at different scales (3, 10, 30 measurements).\",\n        \"Thicker lines=longer windows=smoother trends.\"\n    ), fig\n\n# ============================================================================\n# DASHBOARD FUNCTIONS FOR GRADIO\n# ============================================================================\n\ndef create_kpi_cards(df):\n    \"\"\"Create simple KPI cards for dashboard.\"\"\"\n    if df.empty:\n        return \"<div style='padding: 20px; text-align: center;'>\u05d0\u05d9\u05df \u05e0\u05ea\u05d5\u05e0\u05d9\u05dd \u05d6\u05de\u05d9\u05e0\u05d9\u05dd</div>\"\n\n    latest = df.iloc[-1]\n\n    # Calculate trends\n    if len(df) > 10:\n        prev = df.iloc[-10]\n        temp_trend = \"up\" if latest['temperature'] > prev['temperature'] else \"down\" if latest['temperature'] < prev['temperature'] else \"stable\"\n        hum_trend = \"up\" if latest['humidity'] > prev['humidity'] else \"down\" if latest['humidity'] < prev['humidity'] else \"stable\"\n        soil_trend = \"up\" if latest['soil'] > prev['soil'] else \"down\" if latest['soil'] < prev['soil'] else \"stable\"\n\n        temp_change = f\"{abs(latest['temperature'] - prev['temperature']):.1f}\"\n        hum_change = f\"{abs(latest['humidity'] - prev['humidity']):.1f}\"\n        soil_change = f\"{abs(latest['soil'] - prev['soil']):.1f}\"\n    else:\n        temp_trend = hum_trend = soil_trend = \"stable\"\n        temp_change = hum_change = soil_change = \"0.0\"\n\n    # Create HTML\n    html = f\"\"\"\n    <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 24px; margin: 20px 0;\">\n        {create_kpi_card('\ud83c\udf21\ufe0f Temperature', f'{latest[\"temperature\"]:.1f}', '\u00b0C', temp_change, 'from last 10', temp_trend, COLORS['temperature']['color'])}\n        {create_kpi_card('\ud83d\udca7 Humidity', f'{latest[\"humidity\"]:.1f}', '%', hum_change, 'from last 10', hum_trend, COLORS['humidity']['color'])}\n        {create_kpi_card('\ud83c\udf31 Soil Moisture', f'{latest[\"soil\"]:.1f}', '%', soil_change, 'from last 10', soil_trend, COLORS['soil']['color'])}\n    </div>\n    \"\"\"\n    return html\n\ndef create_time_series_plot(df):\n    \"\"\"Create time series plot for dashboard.\"\"\"\n    if df.empty:\n        fig = go.Figure()\n        fig.add_annotation(\n            text=\"No data available\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False,\n            font=dict(size=20)\n        )\n        fig.update_layout(height=500)\n        return fig\n\n    fig = make_subplots(\n        rows=3, cols=1,\n        subplot_titles=('\ud83c\udf21\ufe0f Temperature (\u00b0C)', '\ud83d\udca7 Humidity (%)', '\ud83c\udf31 Soil Moisture (%)'),\n        vertical_spacing=0.08\n    )\n\n    # Temperature\n    fig.add_trace(\n        go.Scatter(\n            x=df['timestamp'],\n            y=df['temperature'],\n            name='Temperature',\n            line=dict(color=COLORS['temperature']['color'], width=2),\n            fill='tozeroy',\n            fillcolor=f\"rgba(239, 68, 68, 0.1)\"\n        ),\n        row=1, col=1\n    )\n\n    # Humidity\n    fig.add_trace(\n        go.Scatter(\n            x=df['timestamp'],\n            y=df['humidity'],\n            name='Humidity',\n            line=dict(color=COLORS['humidity']['color'], width=2),\n            fill='tozeroy',\n            fillcolor=f\"rgba(59, 130, 246, 0.1)\"\n        ),\n        row=2, col=1\n    )\n\n    # Soil\n    fig.add_trace(\n        go.Scatter(\n            x=df['timestamp'],\n            y=df['soil'],\n            name='Soil',\n            line=dict(color=COLORS['soil']['color'], width=2),\n            fill='tozeroy',\n            fillcolor=f\"rgba(139, 92, 246, 0.1)\"\n        ),\n        row=3, col=1\n    )\n\n    # Update layout\n    fig.update_xaxes(showgrid=True, gridcolor='#E5E7EB')\n    fig.update_yaxes(showgrid=True, gridcolor='#E5E7EB')\n\n    fig.update_layout(\n        height=800,\n        showlegend=False,\n        plot_bgcolor='white',\n        paper_bgcolor='white',\n        font=dict(family=\"Inter, sans-serif\")\n    )\n\n    return fig\n\nprint('\u2705 ALL visualization functions loaded!')\nprint('   \u2713 Time Series')\nprint('   \u2713 Correlations')\nprint('   \u2713 Hourly/Daily Patterns')\nprint('   \u2713 Histograms (Distributions)')\nprint('   \u2713 Moving Averages')\nprint('   \u2713 Statistics Cards')\nprint('   \u2713 Dashboard functions')\nprint('   \u274c Scatter Plots (REMOVED)')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 12: Screen Functions\n# ============================================================================\n# @title\n# Cell 8: Screen Functions (WITHOUT SCATTER ANALYSIS)\n\ndef sync_screen():\n    \"\"\"Sync data screen.\"\"\"\n    msg, count = sync_new_data_from_server()\n    return msg\n\ndef dashboard_screen():\n    \"\"\"Load all data and return comprehensive dashboard (WITHOUT SCATTER).\"\"\"\n    df = load_data_from_firebase()\n\n    if df.empty:\n        empty_msg = \"<div style='padding: 20px; text-align: center;'>\u05d0\u05d9\u05df \u05e0\u05ea\u05d5\u05e0\u05d9\u05dd. \u05dc\u05d7\u05e5 \u05e2\u05dc Sync Data!</div>\"\n        return empty_msg, None, None, None, None, None, None, None, None, None, None\n\n    # Generate all visualizations (WITHOUT SCATTER)\n    kpi = create_kpi_cards(df)\n    stats = create_stat_cards_html(df)\n    ts = create_time_series_plot(df)\n    corr_card, corr_plot = calculate_correlations(df)\n    hourly_card, hourly_plot = hourly_patterns(df)\n    daily_card, daily_plot = daily_patterns(df)\n    dist_card, dist_plot = distribution_analysis(df)\n\n    return kpi, stats, ts, corr_card, corr_plot, hourly_card, hourly_plot, daily_card, daily_plot, dist_card, dist_plot\n\ndef dashboard_moving_avg(variable):\n    \"\"\"Generate moving average plot for selected variable.\"\"\"\n    df = load_data_from_firebase()\n    if df.empty:\n        return None, \"\u05d0\u05d9\u05df \u05e0\u05ea\u05d5\u05e0\u05d9\u05dd\"\n    ma_card, ma_plot = time_series_decomposition(df, variable)\n    return ma_card, ma_plot\n\nprint('\u2705 All screen functions loaded!')\nprint('   \ud83d\udcca Dashboard (without scatter)')\nprint('   \ud83d\udcc9 Moving Averages')\nprint('   \ud83d\udd04 Data Sync')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# COMPREHENSIVE PRE-INITIALIZATION SYSTEM\n# All heavy operations run ONCE before Gradio launches\n# ============================================================================\n\n# Global caches for pre-loaded data\nCACHE = {\n    'firebase_initialized': False,\n    'sensor_data': None,\n    'rag_index': None,\n    'doc_map': None,\n    'doc_text': None,\n    'embeddings': None,\n    'embed_model': None,\n    'gamification_profile': None,\n    'ml_model': None,\n}\n\ndef initialize_firebase():\n    \"\"\"Initialize Firebase connection (run once).\"\"\"\n    if CACHE['firebase_initialized']:\n        return True\n    try:\n        # Firebase should already be initialized from config cell\n        print(\"\u2705 Firebase connection ready\")\n        CACHE['firebase_initialized'] = True\n        return True\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Firebase init: {e}\")\n        return False\n\ndef preload_sensor_data():\n    \"\"\"Pre-load latest sensor data from Firebase.\"\"\"\n    try:\n        df = load_data_from_firebase()\n        if df is not None and not df.empty:\n            CACHE['sensor_data'] = df\n            print(f\"\u2705 Pre-loaded {len(df)} sensor records\")\n            return df\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Sensor data preload: {e}\")\n    return None\n\ndef preload_rag_index():\n    \"\"\"Pre-load RAG index, doc_map, doc_text from Firebase.\"\"\"\n    try:\n        # Load index\n        index = firebase_get(\"indexes/public_index\")\n        if index:\n            CACHE['rag_index'] = index\n            print(f\"\u2705 Pre-loaded RAG index ({len(index)} terms)\")\n        \n        # Load doc_map\n        doc_map = firebase_get(\"indexes/doc_map\")\n        if doc_map:\n            CACHE['doc_map'] = doc_map\n            print(f\"\u2705 Pre-loaded doc_map ({len(doc_map)} documents)\")\n        \n        # Load doc_text\n        doc_text = firebase_get(\"indexes/doc_text\")\n        if doc_text:\n            CACHE['doc_text'] = doc_text\n            print(f\"\u2705 Pre-loaded doc_text ({len(doc_text)} documents)\")\n        \n        return True\n    except Exception as e:\n        print(f\"\u26a0\ufe0f RAG index preload: {e}\")\n    return False\n\ndef preload_embeddings():\n    \"\"\"Pre-load vector embeddings from Firebase.\"\"\"\n    try:\n        embeddings = firebase_get(\"indexes/embeddings\")\n        if embeddings:\n            CACHE['embeddings'] = embeddings\n            # Also update global CHUNK_EMBEDDINGS if it exists\n            global CHUNK_EMBEDDINGS\n            CHUNK_EMBEDDINGS = embeddings\n            print(f\"\u2705 Pre-loaded embeddings ({len(embeddings)} documents)\")\n            return True\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Embeddings preload: {e}\")\n    return False\n\ndef preload_embed_model():\n    \"\"\"Pre-load embedding model for semantic search.\"\"\"\n    try:\n        model = get_embed_model()\n        if model:\n            CACHE['embed_model'] = model\n            print(\"\u2705 Pre-loaded embedding model\")\n            return True\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Embed model preload: {e}\")\n    return False\n\ndef preload_ml_model():\n    \"\"\"Pre-load ML model for plant disease detection.\"\"\"\n    try:\n        # clf is already loaded globally from imports\n        global clf\n        if clf:\n            CACHE['ml_model'] = clf\n            print(\"\u2705 Pre-loaded plant disease ML model\")\n            return True\n    except Exception as e:\n        print(f\"\u26a0\ufe0f ML model preload: {e}\")\n    return False\n\ndef preload_gamification():\n    \"\"\"Pre-load gamification profile from Firebase.\"\"\"\n    try:\n        profile = _get_profile()\n        if profile:\n            CACHE['gamification_profile'] = profile\n            print(f\"\u2705 Pre-loaded gamification (points: {profile.get('points', 0)})\")\n            return True\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Gamification preload: {e}\")\n    return False\n\ndef build_index_if_missing():\n    \"\"\"Build RAG index if not exists in Firebase.\"\"\"\n    try:\n        if not CACHE['rag_index'] or not CACHE['doc_map']:\n            print(\"\ud83d\udd04 Building RAG index (first run)...\")\n            smart_build_and_save_index(DOC_URLS, STOP_WORDS)\n            # Reload after building\n            preload_rag_index()\n            preload_embeddings()\n            return True\n        return True\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Index build: {e}\")\n    return False\n\ndef initialize_all():\n    \"\"\"\n    MASTER INITIALIZATION - Run ALL setup before Gradio launches.\n    This ensures fast response times during user interaction.\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"\ud83d\ude80 INITIALIZING CLOUD GARDEN SYSTEM\")\n    print(\"=\" * 60)\n    \n    steps = [\n        (\"Firebase Connection\", initialize_firebase),\n        (\"Sensor Data\", preload_sensor_data),\n        (\"RAG Index\", preload_rag_index),\n        (\"Vector Embeddings\", preload_embeddings),\n        (\"Embedding Model\", preload_embed_model),\n        (\"Plant Disease ML\", preload_ml_model),\n        (\"Gamification\", preload_gamification),\n        (\"Build Missing Index\", build_index_if_missing),\n    ]\n    \n    success_count = 0\n    for step_name, step_func in steps:\n        try:\n            print(f\"\\n\ud83d\udccd {step_name}...\")\n            if step_func():\n                success_count += 1\n        except Exception as e:\n            print(f\"   \u26a0\ufe0f {step_name} failed: {e}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(f\"\u2705 INITIALIZATION COMPLETE ({success_count}/{len(steps)} components)\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    return success_count == len(steps)\n\n# Helper to get cached data (faster than Firebase reads)\ndef get_cached_sensor_data():\n    \"\"\"Get sensor data from cache or reload.\"\"\"\n    if CACHE['sensor_data'] is not None:\n        return CACHE['sensor_data']\n    return preload_sensor_data()\n\ndef get_cached_index():\n    \"\"\"Get RAG index from cache.\"\"\"\n    return CACHE['rag_index']\n\ndef get_cached_doc_map():\n    \"\"\"Get doc_map from cache.\"\"\"\n    return CACHE['doc_map']\n\ndef get_cached_doc_text():\n    \"\"\"Get doc_text from cache.\"\"\"\n    return CACHE['doc_text']\n\n\n# ============================================================================\n# CELL 13: App Builder\n# ============================================================================\ndef build_app():\n    with gr.Blocks(css=CUSTOM_CSS, title=\"Cloud Garden - IoT & AI\") as demo:\n        gr.Markdown(\"# \ud83c\udf3f Cloud Garden - IoT & AI\")\n\n        rewards_tab_ref = None\n        rewards_outputs = None\n\n        with gr.Tabs():\n            for tab_name, tab_builder in TABS:\n                if tab_name.startswith(\"\ud83c\udfae\"):\n                    with gr.Tab(tab_name) as rewards_tab_ref:\n                        rewards_outputs = tab_builder()\n                else:\n                    with gr.Tab(tab_name):\n                        tab_builder()\n\n        # auto-refresh rewards on load + on tab select\n        if rewards_tab_ref and rewards_outputs:\n            points_box, spins_box, missions_md, coupons_txt = rewards_outputs\n\n            demo.load(fn=rewards_refresh, outputs=[points_box, spins_box, missions_md, coupons_txt])\n            rewards_tab_ref.select(fn=rewards_refresh, outputs=[points_box, spins_box, missions_md, coupons_txt])\n        \n        # Auto-load dashboard data on startup\n        demo.load(fn=lambda: plant_dashboard(20), outputs=[])\n\n    return demo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 14: Start Report Microservice\n# ============================================================================\nimport os, time, requests, subprocess, sys\n\n# 1) Fetch Cerebras key from Google Drive \"uc\" URL (YOUR EXISTING METHOD)\nfile_id = \"1zOdWD70pxR_BKBW8vMU3FTN1MqL1fkYD\"\nurl = f\"https://drive.google.com/uc?id={file_id}&export=download\"\napi_key = requests.get(url, timeout=60).text.strip()\n\nif not api_key:\n    raise RuntimeError(\"Cerebras key download returned empty text.\")\n\n# 2) Inject key + model into runtime environment (NO .env needed)\nos.environ[\"CEREBRAS_API_KEY\"] = api_key\nos.environ[\"REPORT_MODEL_NAME\"] = \"llama3.1-8b\"\n\n# 3) Start uvicorn (background)\nPORT = 8001\nHOST = \"127.0.0.1\"\n\n# Kill previous process if you re-run this cell (best effort)\n# (optional, safe in colab: just start a new one; old one might hold the port)\ncmd = [sys.executable, \"-m\", \"uvicorn\", \"report_service:app\", \"--host\", HOST, \"--port\", str(PORT)]\nproc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n\n# 4) Health check loop\nhealth_url = f\"http://{HOST}:{PORT}/health\"\nok = False\nfor _ in range(30):\n    try:\n        r = requests.get(health_url, timeout=2)\n        if r.ok and r.json().get(\"ok\") is True:\n            ok = True\n            break\n    except Exception:\n        pass\n    time.sleep(0.5)\n\nif not ok:\n    # Print last lines to understand why it failed\n    try:\n        out = proc.stdout.read().splitlines()[-60:]\n        print(\"\\n\".join(out))\n    except Exception:\n        pass\n    raise RuntimeError(\"Microservice did not start (health check failed)\")\n\nprint(\"\u2705 Report microservice is UP:\", health_url)\n\n\n# \u05d1\u05d3\u05d9\u05e7\u05ea \u05d0\u05d5\u05e8\u05da \u05d4\u05d8\u05e7\u05e1\u05d8 \u05d4\u05e9\u05de\u05d5\u05e8 \u05dc\u05db\u05dc \u05de\u05e1\u05de\u05da\nfor did in range(5):\n    txt = get_doc_text(did)\n    print(f\"Document ID {did}: {len(txt)} characters\")\n    if len(txt) < 2000:\n        print(f\"   \u26a0\ufe0f Warning: Text is very short. Likely only an abstract.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CELL 15: Launch Application\n# ============================================================================\nif __name__ == \"__main__\":\n    # ================================================\n    # PRE-INITIALIZE ALL COMPONENTS BEFORE GRADIO\n    # ================================================\n    print(\"\\n\ud83c\udf31 Cloud Garden Starting...\\n\")\n    \n    # Run all initialization (first-run scenario)\n    init_success = initialize_all()\n    \n    if not init_success:\n        print(\"\u26a0\ufe0f Some components failed to initialize, but continuing...\")\n    \n    # Build and launch Gradio app\n    print(\"\\n\ud83c\udfa8 Building Gradio Interface...\\n\")\n    app = build_app()\n    \n    print(\"\\n\ud83d\ude80 Launching Application...\\n\")\n    app.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "uxU3y_ahSBCr",
        "dXiHxUFpa556",
        "WU5J3oqaCSV3",
        "ZSPgJzDMOn_M",
        "j3VvQJgmOu6H",
        "CqiJW_6S-UZW",
        "lJqc9RlyNbcN",
        "zNQNqiEsWqfZ",
        "xbBhAJeHXGSH",
        "diLmmpb6NPNq",
        "pVEFwcIVNtzb",
        "5U06h9U6aiNy",
        "PZTNbJe1amWj",
        "xkSvfBItbI0T",
        "qXsIyqVwdT-C",
        "8BowsTl5AzP5",
        "QaWdGlHLBBPM",
        "nSheZbYYkpE3",
        "9MAtxIWckkZ7",
        "RcqPbzgaOxyl",
        "DI7DBWJc2iOe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d8d0c2bcbba49138c1ee95ab0f2408d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_740b5569e8e843ddbbad7be685a7044a",
              "IPY_MODEL_839d558d7b234c36827a72a922e190ce",
              "IPY_MODEL_175e2e4af4e34002be77acb888da642b"
            ],
            "layout": "IPY_MODEL_a219f512c9894af384cb8dce488acd22"
          }
        },
        "740b5569e8e843ddbbad7be685a7044a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c77b3b0f9194f05beeb2760a0f494a5",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_9ff45590bd6a4e6a8e90eccb8fd064fb",
            "value": "config.json:\u2007"
          }
        },
        "839d558d7b234c36827a72a922e190ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac1229de7d04c3385fe2cd3d3f5424f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e894ae546af544359a96d08abd0bfb22",
            "value": 1
          }
        },
        "175e2e4af4e34002be77acb888da642b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e7dcaa3592742d1bbeec50416a3ad4f",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_ce22548a50f1468dbc8fe76849729274",
            "value": "\u20073.57k/?\u2007[00:00&lt;00:00,\u200793.8kB/s]"
          }
        },
        "a219f512c9894af384cb8dce488acd22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c77b3b0f9194f05beeb2760a0f494a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff45590bd6a4e6a8e90eccb8fd064fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fac1229de7d04c3385fe2cd3d3f5424f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e894ae546af544359a96d08abd0bfb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e7dcaa3592742d1bbeec50416a3ad4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce22548a50f1468dbc8fe76849729274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2683132df3b342d08491ec7d09b0d978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60225e25edfa46b19f32adcc12e6d452",
              "IPY_MODEL_cde33e149da64731a13d5d594b56089c",
              "IPY_MODEL_7432053e45874de0a6de7ff196c0c204"
            ],
            "layout": "IPY_MODEL_c045a0025cc841eaa96916e3dd79f082"
          }
        },
        "60225e25edfa46b19f32adcc12e6d452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba0357e1a5f54442a1b4a4bd90c9a45b",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_2782c52e6f9446d0a389a1fd976110a2",
            "value": "pytorch_model.bin:\u2007100%"
          }
        },
        "cde33e149da64731a13d5d594b56089c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5079719f72904ec893d6ed17e4058ead",
            "max": 9335093,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4577426091304106aa1ef2224bd5c816",
            "value": 9335093
          }
        },
        "7432053e45874de0a6de7ff196c0c204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bc030bee88a43ec9c2560735d70e502",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_4780628dbc914f528bd9e16556c263c8",
            "value": "\u20079.34M/9.34M\u2007[00:01&lt;00:00,\u200710.1MB/s]"
          }
        },
        "c045a0025cc841eaa96916e3dd79f082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba0357e1a5f54442a1b4a4bd90c9a45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2782c52e6f9446d0a389a1fd976110a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5079719f72904ec893d6ed17e4058ead": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4577426091304106aa1ef2224bd5c816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bc030bee88a43ec9c2560735d70e502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4780628dbc914f528bd9e16556c263c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "637dd74a9dfc4d0eb84c6f4fd514de34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c62ecbcb0b4643cda9a12e07adf67889",
              "IPY_MODEL_333304b4358d461e952e09f1b406e43e",
              "IPY_MODEL_8af90d8b4f3740e8ae8bcd55e7b01bec"
            ],
            "layout": "IPY_MODEL_dc3e63b0f22c42b88b215149f1277c95"
          }
        },
        "c62ecbcb0b4643cda9a12e07adf67889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36341a7c25c54bc1832ab5f883c766e4",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_007b5f40371b43788c7b3bbca93df72f",
            "value": "preprocessor_config.json:\u2007100%"
          }
        },
        "333304b4358d461e952e09f1b406e43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c47c558bbc6d452cb0ca36ea628dd08a",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_756a3a00f6324c3d952d6702cb12dcdd",
            "value": 408
          }
        },
        "8af90d8b4f3740e8ae8bcd55e7b01bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f8ffbc7238e48fa804c2689be76ae13",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_f569dd6a3a4a42e68d18122307868764",
            "value": "\u2007408/408\u2007[00:00&lt;00:00,\u200727.5kB/s]"
          }
        },
        "dc3e63b0f22c42b88b215149f1277c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36341a7c25c54bc1832ab5f883c766e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "007b5f40371b43788c7b3bbca93df72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c47c558bbc6d452cb0ca36ea628dd08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "756a3a00f6324c3d952d6702cb12dcdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f8ffbc7238e48fa804c2689be76ae13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f569dd6a3a4a42e68d18122307868764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2bd8e24f68845bab0e5389365f4a32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb18a233c63d43f3b3bbacd1188a7203",
              "IPY_MODEL_e8997f6352c24d2cb656f5016a3ea851",
              "IPY_MODEL_b5da1fd1452d46f3af7bc7c970bdefcd"
            ],
            "layout": "IPY_MODEL_08668b5630c146f2bec398d367b83c41"
          }
        },
        "eb18a233c63d43f3b3bbacd1188a7203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5948e05579f42e1964d75c5e6ed450a",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_6ad3ea9a64aa473aa6e5ce84c66b6155",
            "value": "model.safetensors:\u2007100%"
          }
        },
        "e8997f6352c24d2cb656f5016a3ea851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa59a9bff984c499da0bd021a5f05af",
            "max": 9264680,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_650e9706506b48259d3f47131efc1358",
            "value": 9264680
          }
        },
        "b5da1fd1452d46f3af7bc7c970bdefcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a032ddcccee4a089ee8d5ca441c5deb",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_7ed14efab5a74d8ca2ae040c98ab0dc9",
            "value": "\u20079.26M/9.26M\u2007[00:01&lt;00:00,\u20078.19MB/s]"
          }
        },
        "08668b5630c146f2bec398d367b83c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5948e05579f42e1964d75c5e6ed450a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad3ea9a64aa473aa6e5ce84c66b6155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fa59a9bff984c499da0bd021a5f05af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "650e9706506b48259d3f47131efc1358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a032ddcccee4a089ee8d5ca441c5deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed14efab5a74d8ca2ae040c98ab0dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}