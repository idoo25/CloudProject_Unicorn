{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/idoo25/CloudProject_Unicorn/blob/master/HW3_Unicorn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udf3f CloudGarden - Smart Plant Care System\n\n## Layered Architecture Overview\n\nThis notebook follows a **Layered Architecture** pattern for maintainability and scalability:\n\n| Layer | Description | Contents |\n|-------|-------------|----------|\n| **Layer 1** | Dependencies & Configuration | Package installation, imports, API keys, constants |\n| **Layer 2** | Data Access Layer (DAL) | Firebase operations, external API clients |\n| **Layer 3** | Business Logic Layer (BLL) | Data processing, ML models, analytics |\n| **Layer 4** | Service Layer | Microservices, report generation |\n| **Layer 5** | Presentation Layer | Gradio UI components for each tab |\n| **Layer 6** | Application Layer | App builder and launch |\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udce6 LAYER 1: Dependencies & Configuration\n\nThis layer contains all package installations, imports, and configuration constants.\nAll dependencies are centralized here to avoid duplication throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# Package Installation - Run this cell first\n# ============================================================================\n!pip install -q --upgrade gradio pandas matplotlib python-docx\n!pip install -q --upgrade firebase-admin plotly gdown beautifulsoup4\n!pip install -q cerebras-cloud-sdk google-genai\n!pip install -q fastapi uvicorn python-dotenv nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Global Imports\n\nAll imports are consolidated here to prevent duplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# GLOBAL IMPORTS - Consolidated to avoid duplication\n# ============================================================================\n\n# Standard Library\nimport os\nimport re\nimport sys\nimport json\nimport time\nimport math\nimport random\nimport tempfile\nimport subprocess\nimport warnings\nfrom io import BytesIO\nfrom datetime import datetime, timedelta, timezone\nfrom zoneinfo import ZoneInfo\nfrom collections import Counter, defaultdict\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom urllib.parse import quote\n\n# Data Processing\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# Web & API\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Document Generation\nfrom docx import Document\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\nfrom docx.shared import Inches, Pt, RGBColor\n\n# Firebase\nimport firebase_admin\nfrom firebase_admin import credentials, db\n\n# Machine Learning\nimport gradio as gr\nfrom transformers import pipeline\n\n# NLP\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\n# AI/LLM Services\nfrom cerebras.cloud.sdk import Cerebras\nfrom google import genai\nfrom google.genai import types\n\n# API Services\nfrom fastapi import FastAPI\nfrom fastapi.responses import Response\nfrom pydantic import BaseModel\nfrom dotenv import load_dotenv\n\n# Google Colab\ntry:\n    from google.colab import drive, files\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\nimport gdown\n\n# Suppress warnings and initialize NLP\nwarnings.filterwarnings('ignore')\nnltk.download(\"stopwords\", quiet=True)\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words(\"english\"))\n\nprint(\"\u2705 All imports loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Configuration Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CONFIGURATION CONSTANTS\n# ============================================================================\n\n# --- Server Configuration ---\nFEED = \"json\"\nBASE_URL = \"https://server-cloud-v645.onrender.com/\"\nBATCH_LIMIT = 200\n\n# --- Firebase Configuration ---\nFIREBASE_KEY_ID = '1ESnh8BIbGKrVEijA9nKNgNJNdD5kAaYC'\nFIREBASE_KEY_FILE = 'firebase_key.json'\nFIREBASE_URL = \"https://cloud-81451-default-rtdb.europe-west1.firebasedatabase.app/\"\n\n# --- API Key Drive IDs ---\nCEREBRAS_KEY_DRIVE_ID = \"1zOdWD70pxR_BKBW8vMU3FTN1MqL1fkYD\"\nRAG_CEREBRAS_KEY_DRIVE_ID = \"1vuCquKjZPwCHVNbRoLfAay7MxI7MjOru\"\nGEMINI_KEY_DRIVE_ID = \"1eC4l2drO8dL8S8M50wD7TWs8iLXHlWT-\"\n\n# --- Model Configuration ---\nREPORT_MODEL_NAME = \"llama3.1-8b\"\nRAG_MODEL_ID = \"llama3.1-8b\"\nGEMINI_MODEL_ID = \"gemini-2.5-flash\"\nPLANT_DISEASE_MODEL = \"linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification\"\n\n# --- Microservice Configuration ---\nREPORT_SERVICE_HOST = \"127.0.0.1\"\nREPORT_SERVICE_PORT = 8001\nREPORT_SERVICE_URL = f\"http://{REPORT_SERVICE_HOST}:{REPORT_SERVICE_PORT}/generate-docx\"\n\n# --- Timezone ---\nTZ_NAME = \"Asia/Jerusalem\"\n\n# --- Visualization Colors ---\nCOLORS = {\n    'temperature': {'color': '#ef4444'},\n    'humidity': {'color': '#3b82f6'},\n    'soil': {'color': '#8b5cf6'}\n}\nCOLOR_TEMP = COLORS['temperature']['color']\nCOLOR_HUM = COLORS['humidity']['color']\nCOLOR_SOIL = COLORS['soil']['color']\n\nSTATUS_OK_COLOR = \"#2ca02c\"\nSTATUS_WARN_COLOR = \"#ffbf00\"\nSTATUS_BAD_COLOR = \"#d62728\"\n\n# --- Sensor Thresholds ---\nSENSOR_THRESHOLDS = {\n    'temperature': {'low': 18, 'high': 32, 'margin': 1},\n    'humidity': {'low': 35, 'high': 75, 'margin': 3},\n    'soil': {'low': 20, 'high': 60, 'margin': 3}\n}\n\n# --- RAG Document URLs ---\nDOC_URLS = [\n    \"https://doi.org/10.1038/s41598-025-20629-y\",\n    \"https://doi.org/10.3389/fpls.2016.01419\",\n    \"https://doi.org/10.1038/s41598-025-05102-0\",\n    \"https://doi.org/10.1038/s41598-025-04758-y\",\n    \"https://doi.org/10.2174/0118743315321139240627092707\",\n]\n\n# --- Firebase Index Paths ---\nINDEX_PATH = \"indexes/public_index\"\nMAP_PATH = \"indexes/doc_map\"\nTEXT_PATH = \"indexes/doc_text\"\n\nprint(\"\u2705 Configuration constants loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 CSS Styling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CUSTOM CSS STYLING\n# ============================================================================\n\nCUSTOM_CSS = \"\"\"\n@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');\n* { font-family: 'Inter', sans-serif; }\n\n.kpi-card {\n    background: white;\n    padding: 24px;\n    border-radius: 12px;\n    box-shadow: 0 1px 3px rgba(0,0,0,0.12);\n    text-align: center;\n    border-left: 4px solid;\n}\n.kpi-label { color: #6b7280; font-size: 14px; font-weight: 600; }\n.kpi-value { font-size: 48px; font-weight: 700; color: #1f2937; }\n.trend-up { color: #10b981; }\n.trend-down { color: #ef4444; }\n\n.explanation-card {\n    padding: 16px;\n    border-radius: 10px;\n    margin-bottom: 16px;\n    color: white;\n}\n.explanation-card h3 { margin: 0 0 8px 0; font-size: 16px; }\n.explanation-card p { margin: 4px 0; font-size: 14px; opacity: 0.95; }\n\n.status-badge {\n    display: inline-flex;\n    align-items: center;\n    padding: 4px 12px;\n    background: #10b981;\n    color: white;\n    border-radius: 20px;\n    font-weight: 600;\n    font-size: 12px;\n}\n\n.status-dot {\n    width: 8px;\n    height: 8px;\n    background: white;\n    border-radius: 50%;\n    margin-right: 6px;\n    animation: pulse 1.5s infinite;\n}\n\n@keyframes pulse {\n    0%, 100% { opacity: 1; }\n    50% { opacity: 0.5; }\n}\n\"\"\"\n\nprint(\"\u2705 CSS styling loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\uddc4\ufe0f LAYER 2: Data Access Layer (DAL)\n\nThis layer handles all external data operations:\n- Firebase CRUD operations\n- External API communication\n- Data fetching and persistence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Credential Management"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# CREDENTIAL MANAGEMENT\n# ============================================================================\n\ndef download_api_key_from_drive(file_id: str) -> str:\n    \"\"\"Download an API key from Google Drive using file ID.\"\"\"\n    url = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n    response = requests.get(url, timeout=60)\n    if not response.ok:\n        raise RuntimeError(f\"Failed to download API key: {response.status_code}\")\n    return response.text.strip()\n\ndef download_file_from_drive(file_id: str, output_path: str) -> str:\n    \"\"\"Download a file from Google Drive using gdown.\"\"\"\n    url = f'https://drive.google.com/uc?id={file_id}'\n    gdown.download(url, output_path, quiet=False, fuzzy=True)\n    return output_path\n\n# Global API key storage\nCEREBRAS_API_KEY = None\nRAG_API_KEY = None\nGEMINI_API_KEY = None\n\ndef initialize_api_keys():\n    \"\"\"Load all API keys from Google Drive.\"\"\"\n    global CEREBRAS_API_KEY, RAG_API_KEY, GEMINI_API_KEY\n    \n    print(\"\ud83d\udce5 Downloading API keys...\")\n    CEREBRAS_API_KEY = download_api_key_from_drive(CEREBRAS_KEY_DRIVE_ID)\n    assert CEREBRAS_API_KEY, \"Cerebras API key is empty\"\n    print(\"\u2713 Cerebras API key loaded\")\n    \n    RAG_API_KEY = download_api_key_from_drive(RAG_CEREBRAS_KEY_DRIVE_ID)\n    assert RAG_API_KEY, \"RAG API key is empty\"\n    print(\"\u2713 RAG API key loaded\")\n    \n    GEMINI_API_KEY = download_api_key_from_drive(GEMINI_KEY_DRIVE_ID)\n    assert GEMINI_API_KEY, \"Gemini API key is empty\"\n    print(\"\u2713 Gemini API key loaded\")\n    return True\n\nprint(\"\u2705 Credential management loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Firebase Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# FIREBASE OPERATIONS\n# ============================================================================\n\n_firebase_initialized = False\n\ndef initialize_firebase():\n    \"\"\"Initialize Firebase connection.\"\"\"\n    global _firebase_initialized\n    \n    if _firebase_initialized or firebase_admin._apps:\n        print(\"\u2713 Firebase already initialized\")\n        return True\n    \n    if os.path.exists(FIREBASE_KEY_FILE):\n        os.remove(FIREBASE_KEY_FILE)\n    \n    print(\"\ud83d\udce5 Downloading Firebase credentials...\")\n    try:\n        download_file_from_drive(FIREBASE_KEY_ID, FIREBASE_KEY_FILE)\n        with open(FIREBASE_KEY_FILE, 'r') as f:\n            creds = json.load(f)\n        print(f'\u2713 Project: {creds.get(\"project_id\")}')\n    except Exception as e:\n        print(f'\u26a0\ufe0f Error: {e}')\n        if IN_COLAB:\n            uploaded = files.upload()\n            if uploaded:\n                os.rename(list(uploaded.keys())[0], FIREBASE_KEY_FILE)\n    \n    firebase_admin.initialize_app(\n        credentials.Certificate(FIREBASE_KEY_FILE),\n        {'databaseURL': FIREBASE_URL}\n    )\n    _firebase_initialized = True\n    print(\"\u2705 Firebase initialized\")\n    return True\n\ndef firebase_get(path: str) -> Any:\n    \"\"\"Read data from Firebase via HTTP GET.\"\"\"\n    url = f\"{FIREBASE_URL.rstrip('/')}/{path}.json\"\n    r = requests.get(url, timeout=30)\n    if r.status_code != 200:\n        raise RuntimeError(f\"GET {path} failed: {r.status_code}\")\n    return r.json()\n\ndef firebase_put(path: str, data: Any) -> Tuple[int, str]:\n    \"\"\"Write data to Firebase via HTTP PUT.\"\"\"\n    url = f\"{FIREBASE_URL.rstrip('/')}/{path}.json\"\n    r = requests.put(url, json=data, timeout=30)\n    if r.status_code != 200:\n        raise RuntimeError(f\"PUT {path} failed: {r.status_code}\")\n    return r.status_code, r.text\n\ndef load_sensor_data_from_firebase() -> pd.DataFrame:\n    \"\"\"Load all sensor data from Firebase as DataFrame.\"\"\"\n    data = db.reference('/sensor_data').get()\n    if not data:\n        return pd.DataFrame()\n    \n    df = pd.DataFrame([{\n        'timestamp': pd.to_datetime(v['created_at']),\n        'temperature': float(v['temperature']),\n        'humidity': float(v['humidity']),\n        'soil': float(v['soil'])\n    } for v in data.values()])\n    \n    df = df.sort_values('timestamp').reset_index(drop=True)\n    df['humidity'] = df['humidity'].clip(0, 100)\n    df['soil'] = df['soil'].clip(0, 100)\n    df['temperature'] = df['temperature'].clip(-50, 100)\n    return df\n\ndef get_latest_timestamp_from_firebase() -> Optional[str]:\n    \"\"\"Get the latest timestamp from Firebase.\"\"\"\n    try:\n        latest = db.reference('/sensor_data').order_by_child('created_at').limit_to_last(1).get()\n        return list(latest.values())[0]['created_at'] if latest else None\n    except Exception:\n        return None\n\ndef save_sensor_data_to_firebase(data_list: List[Dict]) -> int:\n    \"\"\"Save sensor data to Firebase.\"\"\"\n    if not data_list:\n        return 0\n    ref = db.reference('/sensor_data')\n    saved = 0\n    for sample in data_list:\n        try:\n            vals = json.loads(sample['value'])\n            timestamp_key = sample['created_at'].replace(':', '-').replace('.', '-')\n            ref.child(timestamp_key).set({\n                'created_at': sample['created_at'],\n                'temperature': max(-50, min(100, float(vals['temperature']))),\n                'humidity': max(0, min(100, float(vals['humidity']))),\n                'soil': max(0, min(100, float(vals['soil'])))\n            })\n            saved += 1\n        except Exception:\n            continue\n    return saved\n\nprint(\"\u2705 Firebase operations loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 External API Client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# EXTERNAL API CLIENT\n# ============================================================================\n\ndef fetch_iot_data(feed: str, limit: int) -> Optional[pd.DataFrame]:\n    \"\"\"Fetch IoT sensor data from external server.\"\"\"\n    try:\n        resp = requests.get(f\"{BASE_URL}/history\", params={\"feed\": feed, \"limit\": limit}, timeout=30)\n        data = resp.json()\n        if \"data\" not in data or not data[\"data\"]:\n            return None\n        df = pd.DataFrame(data[\"data\"])\n        if \"created_at\" not in df.columns or \"value\" not in df.columns:\n            return None\n        df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n        df = df.dropna(subset=[\"created_at\", \"value\"]).sort_values(\"created_at\")\n        return None if df.empty else df\n    except Exception as e:\n        print(f\"Error fetching IoT data: {e}\")\n        return None\n\ndef fetch_batch_from_server(before_timestamp: Optional[str] = None) -> Dict:\n    \"\"\"Fetch a batch of data from the IoT server.\"\"\"\n    params = {\"feed\": FEED, \"limit\": BATCH_LIMIT}\n    if before_timestamp:\n        params[\"before_created_at\"] = before_timestamp\n    try:\n        return requests.get(f\"{BASE_URL}/history\", params=params, timeout=180).json()\n    except Exception:\n        return {}\n\ndef sync_new_data_from_server() -> Tuple[str, int]:\n    \"\"\"Sync new data from IoT server to Firebase.\"\"\"\n    msgs = [\"Starting sync...\"]\n    latest = get_latest_timestamp_from_firebase()\n    msgs.append(f\"Latest: {latest}\" if latest else \"No existing data\")\n    resp = fetch_batch_from_server()\n    if \"data\" not in resp:\n        return \"\\n\".join(msgs + [\"Error fetching data\"]), 0\n    new_samples = [s for s in resp[\"data\"] if not latest or s[\"created_at\"] > latest]\n    if new_samples:\n        saved = save_sensor_data_to_firebase(new_samples)\n        return \"\\n\".join(msgs + [f\"Found {len(new_samples)} new\", f\"Saved {saved}!\"]), saved\n    return \"\\n\".join(msgs + [\"No new data\"]), 0\n\nprint(\"\u2705 External API client loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 HTTP Session & Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# HTTP SESSION & WEB SCRAPING (for RAG)\n# ============================================================================\n\nhttp_session = requests.Session()\nBROWSER_HEADERS = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n}\n\n_STOP_SECTION_TITLES = {\n    \"references\", \"reference\", \"bibliography\", \"acknowledgements\", \"acknowledgments\",\n    \"author information\", \"ethics declarations\", \"additional information\",\n    \"supplementary information\", \"rights and permissions\", \"about this article\",\n    \"availability of data and materials\", \"data availability\", \"publisher's note\"\n}\n\n_SKIP_LINE_RES = [\n    re.compile(r\"(?i)^\\s*cite this article\\s*$\"),\n    re.compile(r\"(?i)^\\s*google\\s+scholar\\s*$\"),\n    re.compile(r\"(?i)creative\\s+commons\"),\n    re.compile(r\"(?i)springer\\s+nature\"),\n]\n\ndef _normalize_doi(doi_or_url: str) -> str:\n    s = (doi_or_url or \"\").strip()\n    return s.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\").strip()\n\ndef fetch_html(url: str, timeout: int = 15) -> Tuple[Optional[str], str, int]:\n    try:\n        r = http_session.get(url, headers=BROWSER_HEADERS, timeout=timeout, allow_redirects=True)\n        return r.text, r.url, r.status_code\n    except Exception:\n        return None, url, 0\n\ndef extract_main_text_from_html(html: str) -> str:\n    if not html:\n        return \"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n    for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n        element.decompose()\n    main_content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"body\")\n    return main_content.get_text(separator=\" \", strip=True) if main_content else soup.get_text(separator=\" \", strip=True)\n\ndef postprocess_document_text(text: str) -> str:\n    if not text:\n        return \"\"\n    lines = text.split(\"\\n\")\n    clean_lines = []\n    for line in lines:\n        line = line.strip()\n        if not line or any(pat.match(line) for pat in _SKIP_LINE_RES):\n            continue\n        if line.lower() in _STOP_SECTION_TITLES:\n            break\n        clean_lines.append(line)\n    return \" \".join(clean_lines)\n\nprint(\"\u2705 HTTP session & web scraping loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \u2699\ufe0f LAYER 3: Business Logic Layer (BLL)\n\nThis layer contains all business logic:\n- Data processing and transformations\n- ML model operations\n- Analytics and calculations\n- No direct database access (uses DAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Data Processing Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# DATA PROCESSING UTILITIES\n# ============================================================================\n\ndef normalize_series(series: pd.Series) -> pd.Series:\n    \"\"\"Normalize a series to 0-1 range.\"\"\"\n    mn, mx = float(series.min()), float(series.max())\n    if mx - mn == 0:\n        return series * 0.0\n    return (series - mn) / (mx - mn)\n\ndef records_to_df(records: List[Dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"Convert records list to DataFrame.\"\"\"\n    if not records:\n        return pd.DataFrame(columns=[\"created_at\", \"value\"])\n    df = pd.DataFrame(records)\n    if \"created_at\" not in df.columns or \"value\" not in df.columns:\n        return pd.DataFrame(columns=[\"created_at\", \"value\"])\n    return df\n\ndef df_to_records(df: pd.DataFrame) -> List[Dict]:\n    \"\"\"Convert DataFrame to records list.\"\"\"\n    if df is None or df.empty:\n        return []\n    out = df[[\"created_at\", \"value\"]].copy()\n    out[\"created_at\"] = out[\"created_at\"].astype(str)\n    return out.to_dict(\"records\")\n\ndef unify_sensor_dfs(dfs: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"Unify multiple sensor DataFrames into one.\"\"\"\n    def prep(df, col):\n        if df is None or df.empty:\n            return pd.DataFrame(columns=[\"timestamp\", col])\n        out = df.copy()\n        if \"timestamp\" not in out.columns and \"created_at\" in out.columns:\n            out = out.rename(columns={\"created_at\": \"timestamp\"})\n        if \"timestamp\" not in out.columns:\n            out = out.reset_index().rename(columns={\"index\": \"timestamp\"})\n        if \"timestamp\" not in out.columns or \"value\" not in out.columns:\n            return pd.DataFrame(columns=[\"timestamp\", col])\n        out = out[[\"timestamp\", \"value\"]]\n        ts = out[\"timestamp\"]\n        if pd.api.types.is_numeric_dtype(ts) or ts.astype(str).str.fullmatch(r\"\\d+\").all():\n            ts_num = pd.to_numeric(ts, errors=\"coerce\")\n            unit = \"ms\" if ts_num.dropna().astype(int).astype(str).str.len().median() >= 13 else \"s\"\n            out[\"timestamp\"] = pd.to_datetime(ts_num, errors=\"coerce\", unit=unit, utc=True).dt.tz_convert(\"Asia/Jerusalem\").dt.tz_localize(None)\n        else:\n            out[\"timestamp\"] = pd.to_datetime(ts, errors=\"coerce\", utc=True).dt.tz_convert(\"Asia/Jerusalem\").dt.tz_localize(None)\n        out = out.dropna(subset=[\"timestamp\"])\n        out[\"value\"] = pd.to_numeric(out[\"value\"], errors=\"coerce\")\n        out = out.dropna(subset=[\"value\"])\n        return out.rename(columns={\"value\": col})\n    \n    t = prep(dfs.get(\"temperature\"), \"temperature\")\n    h = prep(dfs.get(\"humidity\"), \"humidity\")\n    s = prep(dfs.get(\"soil\"), \"soil\")\n    df = t.merge(h, on=\"timestamp\", how=\"outer\").merge(s, on=\"timestamp\", how=\"outer\")\n    return df.sort_values(\"timestamp\").reset_index(drop=True)\n\nprint(\"\u2705 Data processing utilities loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Plant Status Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# PLANT STATUS ANALYSIS\n# ============================================================================\n\ndef analyze_sensor_values(temp: float, hum: float, soil: float) -> Tuple[List[str], List[str]]:\n    \"\"\"Analyze sensor values and return issues and warnings.\"\"\"\n    issues, warnings_list = [], []\n    checks = [\n        (\"Temperature\", temp, SENSOR_THRESHOLDS['temperature']),\n        (\"Air humidity\", hum, SENSOR_THRESHOLDS['humidity']),\n        (\"Soil moisture\", soil, SENSOR_THRESHOLDS['soil']),\n    ]\n    for name, value, thresholds in checks:\n        low, high, margin = thresholds['low'], thresholds['high'], thresholds['margin']\n        if not (low <= value <= high):\n            issues.append(f\"{name} out of range ({value:.1f})\")\n        elif value <= low + margin or value >= high - margin:\n            warnings_list.append(f\"{name} near threshold ({value:.1f})\")\n    return issues, warnings_list\n\ndef get_plant_status(issues: List[str], warnings_list: List[str]) -> Tuple[str, str]:\n    \"\"\"Get overall plant status based on issues and warnings.\"\"\"\n    if issues:\n        return \"\ud83d\udd34 Not OK\", STATUS_BAD_COLOR\n    elif warnings_list:\n        return \"\ud83d\udfe1 Warning\", STATUS_WARN_COLOR\n    return \"\ud83d\udfe2 Healthy\", STATUS_OK_COLOR\n\ndef plant_dashboard(limit: int):\n    \"\"\"Main plant dashboard analysis function.\"\"\"\n    try:\n        dfs = {\n            \"temperature\": fetch_iot_data(\"temperature\", limit),\n            \"humidity\": fetch_iot_data(\"humidity\", limit),\n            \"soil\": fetch_iot_data(\"soil\", limit),\n        }\n        missing = [k for k, v in dfs.items() if v is None]\n        if missing:\n            return \"\u26a0\ufe0f Partial Data\", f\"Missing sensors: {', '.join(missing)}\", None, None, None, None\n\n        temp = float(dfs[\"temperature\"][\"value\"].iloc[-1])\n        hum = float(dfs[\"humidity\"][\"value\"].iloc[-1])\n        soil = float(dfs[\"soil\"][\"value\"].iloc[-1])\n\n        issues, warnings_list = analyze_sensor_values(temp, hum, soil)\n        status, _ = get_plant_status(issues, warnings_list)\n        \n        details = f\"Temp: {temp:.1f}\u00b0C | Humidity: {hum:.1f}% | Soil: {soil:.1f}%\\n\"\n        if issues:\n            details += \"Issues: \" + \", \".join(issues)\n        elif warnings_list:\n            details += \"Warnings: \" + \", \".join(warnings_list)\n        else:\n            details += \"All readings normal\"\n\n        # Create plots (simplified)\n        import plotly.graph_objects as go\n        plots = {}\n        for name, df in dfs.items():\n            if df is not None:\n                fig = go.Figure()\n                fig.add_trace(go.Scatter(x=df[\"created_at\"], y=df[\"value\"], mode='lines', name=name))\n                fig.update_layout(title=f\"{name.title()} Over Time\", height=300)\n                plots[name] = fig\n        \n        return status, details, plots.get(\"temperature\"), plots.get(\"humidity\"), plots.get(\"soil\"), None\n    except Exception as e:\n        return f\"\u274c Error: {str(e)}\", \"\", None, None, None, None\n\nprint(\"\u2705 Plant status analysis loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Plant Disease Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# PLANT DISEASE DETECTION\n# ============================================================================\n\n# Initialize ML classifier\nclf = pipeline(\"image-classification\", model=PLANT_DISEASE_MODEL)\n\ndef analyze_plant_image(image, temp: float, humidity: float, soil: float) -> Tuple[str, str, str, str]:\n    \"\"\"Analyze plant image and environmental conditions.\"\"\"\n    preds = clf(image)\n    top = preds[0]\n    label, score = top[\"label\"], top[\"score\"]\n\n    alerts, advice = [], []\n\n    # Temperature checks\n    if temp < SENSOR_THRESHOLDS['temperature']['low']:\n        alerts.append(\"Low temperature\")\n        advice.append(\"Move plant to warmer environment\")\n    elif temp > SENSOR_THRESHOLDS['temperature']['high']:\n        alerts.append(\"High temperature\")\n        advice.append(\"Move plant to shaded area\")\n\n    # Humidity checks\n    if humidity < SENSOR_THRESHOLDS['humidity']['low']:\n        alerts.append(\"Low air humidity\")\n        advice.append(\"Increase humidity (e.g. misting)\")\n    elif humidity > SENSOR_THRESHOLDS['humidity']['high']:\n        alerts.append(\"High air humidity\")\n        advice.append(\"Improve ventilation\")\n\n    # Soil moisture checks\n    if soil < SENSOR_THRESHOLDS['soil']['low']:\n        alerts.append(\"Low soil moisture\")\n        advice.append(\"Water the plant\")\n    elif soil > SENSOR_THRESHOLDS['soil']['high']:\n        alerts.append(\"High soil moisture\")\n        advice.append(\"Reduce watering\")\n\n    is_bad = \"healthy\" not in label.lower()\n    status_html = f\"\"\"<div style='padding:10px;border-radius:10px;\n        background:{'#ffdddd' if is_bad else '#ddffdd'};\n        border:1px solid {'#ff0000' if is_bad else '#00aa00'};\n        font-weight:700;'>\n        {'\ud83d\udd34 Plant status: BAD' if is_bad else '\ud83d\udfe2 Plant status: GOOD'}\n    </div>\"\"\"\n\n    if not alerts:\n        alerts.append(\"Status looks normal\")\n\n    return (\n        f\"Detected: {label} ({score:.2%})\",\n        status_html,\n        \"\\n\".join(alerts),\n        \"\\n\".join(advice)\n    )\n\nprint(\"\u2705 Plant disease detection loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 NLP Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# NLP PROCESSING (for RAG)\n# ============================================================================\n\ndef tokenize(text: str) -> List[str]:\n    \"\"\"Convert text to lowercase word tokens.\"\"\"\n    return re.findall(r\"\\w+\", (text or \"\").lower())\n\ndef remove_stopwords(tokens: List[str], stop_words: set) -> List[str]:\n    \"\"\"Remove stop words from token list.\"\"\"\n    return [t for t in tokens if t not in stop_words]\n\ndef apply_stemming(tokens: List[str]) -> List[str]:\n    \"\"\"Apply Porter stemming to tokens.\"\"\"\n    return [stemmer.stem(t) for t in tokens]\n\ndef preprocess_query(query: str) -> List[str]:\n    \"\"\"Preprocess query for search.\"\"\"\n    tokens = tokenize(query)\n    tokens = remove_stopwords(tokens, stop_words)\n    return apply_stemming(tokens)\n\nprint(\"\u2705 NLP processing loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 RAG Index Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# RAG INDEX OPERATIONS\n# ============================================================================\n\n# Global stores\npublic_index = None\ndoc_map = None\ndoc_text = None\n_DOC_TEXT_CACHE: Dict[int, str] = {}\n\ndef build_inverted_index(urls: List[str], stop_words: set, doc_text_map: Dict) -> Tuple[Dict, Dict]:\n    \"\"\"Build inverted index from document texts.\"\"\"\n    inverted = defaultdict(set)\n    doc_map_local = {i: url for i, url in enumerate(urls)}\n    \n    for doc_id in range(len(urls)):\n        text = doc_text_map.get(doc_id) or doc_text_map.get(str(doc_id)) or \"\"\n        tokens = tokenize(text)\n        tokens = remove_stopwords(tokens, stop_words)\n        tokens = apply_stemming(tokens)\n        for term in set(tokens):\n            inverted[term].add(doc_id)\n    \n    inverted = {term: sorted(list(ids)) for term, ids in inverted.items()}\n    return inverted, doc_map_local\n\ndef load_store_from_firebase(load_text: bool = False):\n    \"\"\"Load index from Firebase.\"\"\"\n    global public_index, doc_map, doc_text\n    public_index = firebase_get(INDEX_PATH) or {}\n    doc_map = firebase_get(MAP_PATH) or {}\n    if isinstance(doc_map, list):\n        doc_map = {str(i): v for i, v in enumerate(doc_map)}\n    if load_text:\n        doc_text = firebase_get(TEXT_PATH) or {}\n    print(f\"Loaded: terms={len(public_index)} | docs={len(doc_map)}\")\n    return public_index, doc_map, doc_text\n\ndef get_doc_text(doc_id: int) -> str:\n    \"\"\"Get document text from Firebase.\"\"\"\n    if doc_id in _DOC_TEXT_CACHE:\n        return _DOC_TEXT_CACHE[doc_id]\n    try:\n        v = firebase_get(f\"indexes/doc_text/{str(int(doc_id))}\")\n        if v:\n            _DOC_TEXT_CACHE[doc_id] = str(v).strip()\n            return _DOC_TEXT_CACHE[doc_id]\n    except Exception as e:\n        print(f\"Error getting doc text: {e}\")\n    return \"\"\n\ndef search_top_k(query: str, k: int = 3) -> Tuple[List[str], List[Dict]]:\n    \"\"\"Search for top-k relevant documents.\"\"\"\n    if public_index is None or doc_map is None:\n        load_store_from_firebase(load_text=False)\n    \n    q_terms = preprocess_query(query)\n    scores = defaultdict(int)\n    \n    for term in q_terms:\n        for doc_id in (public_index.get(term, []) or []):\n            scores[int(doc_id)] += 1\n    \n    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n    results = [{\"doc_id\": doc_id, \"score\": score, \"url\": doc_map.get(str(doc_id))} \n               for doc_id, score in ranked]\n    return q_terms, results\n\nprint(\"\u2705 RAG index operations loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.6 BM25 Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# BM25 RANKING\n# ============================================================================\n\ndef bm25_rank(query: str, k: int = 3, b: float = 0.75, k1: float = 1.5) -> Tuple[List[str], List[Tuple[int, float]]]:\n    \"\"\"BM25 ranking algorithm.\"\"\"\n    if public_index is None or doc_map is None:\n        load_store_from_firebase(load_text=False)\n    \n    q_terms = preprocess_query(query)\n    all_ids = [int(k) for k in doc_map.keys() if str(k).isdigit()]\n    N = len(all_ids)\n    \n    if N == 0:\n        return q_terms, []\n    \n    # Calculate document lengths\n    doc_lens = {}\n    for doc_id in all_ids:\n        text = get_doc_text(doc_id)\n        doc_lens[doc_id] = len(tokenize(text))\n    \n    avgdl = sum(doc_lens.values()) / max(N, 1)\n    scores = defaultdict(float)\n    \n    for term in q_terms:\n        doc_ids = public_index.get(term, [])\n        df = len(doc_ids)\n        if df == 0:\n            continue\n        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)\n        \n        for doc_id in doc_ids:\n            doc_id = int(doc_id)\n            dl = doc_lens.get(doc_id, 1)\n            tf = 1  # Simplified\n            numerator = tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * dl / avgdl)\n            scores[doc_id] += idf * numerator / denominator\n    \n    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n    return q_terms, ranked\n\nprint(\"\u2705 BM25 ranking loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.7 LLM Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# LLM INTEGRATION\n# ============================================================================\n\n# Cerebras client for RAG\nc_client = None\n\ndef init_cerebras_client():\n    \"\"\"Initialize Cerebras client.\"\"\"\n    global c_client\n    if RAG_API_KEY:\n        c_client = Cerebras(api_key=RAG_API_KEY)\n        print(\"\u2713 Cerebras client initialized\")\n    return c_client\n\ndef llm_generate(prompt: str, temperature: float = 0.3, max_tokens: int = 500) -> str:\n    \"\"\"Generate text using Cerebras LLM.\"\"\"\n    if c_client is None:\n        init_cerebras_client()\n    if c_client is None:\n        return \"Error: LLM client not initialized\"\n    \n    try:\n        response = c_client.chat.completions.create(\n            model=RAG_MODEL_ID,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\nprint(\"\u2705 LLM integration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.8 RAG Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# RAG ANSWER GENERATION\n# ============================================================================\n\ndef _chunk_text(text: str, max_chars: int = 4000) -> List[str]:\n    \"\"\"Split text into overlapping chunks.\"\"\"\n    if not text:\n        return []\n    overlap = 400\n    chunks = []\n    for i in range(0, len(text), max_chars - overlap):\n        chunks.append(text[i : i + max_chars])\n    return chunks\n\ndef _extract_evidence_from_chunk(question: str, doc_id: int, chunk_id: int, chunk_text: str) -> Dict[str, Any]:\n    \"\"\"Extract evidence from a text chunk using LLM.\"\"\"\n    prompt = f\"\"\"You are an expert researcher. Analyze the TEXT to answer the QUESTION.\nReturn ONLY a JSON object with:\n- 'found': boolean (true if answer found)\n- 'answer': string (summary of findings)\n- 'evidence': array of strings (supporting quotes)\n\nQUESTION: {question}\n\nTEXT: {chunk_text}\"\"\"\n\n    raw = llm_generate(prompt, temperature=0.0)\n    match = re.search(r\"\\{.*\\}\", raw, flags=re.DOTALL)\n    if match:\n        try:\n            res = json.loads(match.group(0))\n            res = {k.lower(): v for k, v in res.items()}\n            res.update({\"doc_id\": doc_id, \"chunk_id\": chunk_id})\n            return res\n        except:\n            pass\n    return {\"found\": False, \"doc_id\": doc_id, \"chunk_id\": chunk_id}\n\ndef rag_answer_with_model(query: str, k: int = 3) -> Dict[str, Any]:\n    \"\"\"Generate RAG answer using retrieved documents.\"\"\"\n    q_terms, ranked = bm25_rank(query, k=k)\n    if not ranked:\n        return {\"answer\": \"No relevant documents found.\", \"sources\": []}\n    \n    evidence_packets = []\n    sources = []\n    \n    for doc_id, score in ranked:\n        text = get_doc_text(doc_id)\n        if not text:\n            continue\n        \n        url = doc_map.get(str(doc_id), \"\") if doc_map else \"\"\n        sources.append(url)\n        \n        chunks = _chunk_text(text)\n        for i, chunk in enumerate(chunks[:3]):  # Limit chunks\n            evidence = _extract_evidence_from_chunk(query, doc_id, i, chunk)\n            if evidence.get(\"found\"):\n                evidence_packets.append(evidence)\n    \n    if not evidence_packets:\n        return {\"answer\": \"Could not find specific information.\", \"sources\": sources}\n    \n    # Synthesize final answer\n    context = \"\\n\".join([f\"- {e.get('answer', '')}\" for e in evidence_packets if e.get('answer')])\n    synthesis_prompt = f\"\"\"Based on these findings, provide a comprehensive answer:\n\nQUESTION: {query}\n\nFINDINGS:\n{context}\n\nProvide a clear, professional answer in 2-3 paragraphs.\"\"\"\n\n    final_answer = llm_generate(synthesis_prompt, temperature=0.3, max_tokens=800)\n    return {\"answer\": final_answer, \"sources\": sources}\n\nprint(\"\u2705 RAG answer generation loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.9 Gemini Chat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# GEMINI CHAT\n# ============================================================================\n\ngemini_client = None\n\ndef init_gemini_client():\n    \"\"\"Initialize Gemini client.\"\"\"\n    global gemini_client\n    if GEMINI_API_KEY:\n        gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n        print(\"\u2713 Gemini client initialized\")\n    return gemini_client\n\ndef build_system_instruction() -> str:\n    return \"You are a helpful, friendly chatbot. Answer clearly. Keep a natural conversation.\"\n\ndef gemini_turn(user_message: str, history: List[Dict], temperature: float = 0.7):\n    \"\"\"Process a Gemini chat turn.\"\"\"\n    user_message = (user_message or \"\").strip()\n    if not user_message:\n        return \"\", history, history\n    \n    if gemini_client is None:\n        init_gemini_client()\n    if gemini_client is None:\n        return \"\", history + [{\"role\": \"user\", \"content\": user_message}, \n                              {\"role\": \"assistant\", \"content\": \"Error: Client not initialized\"}], history\n\n    contents = []\n    for msg in history:\n        role = msg.get(\"role\")\n        text = (msg.get(\"content\") or \"\").strip()\n        if not text:\n            continue\n        gemini_role = \"user\" if role == \"user\" else \"model\"\n        contents.append(types.Content(role=gemini_role, parts=[types.Part.from_text(text=text)]))\n    \n    contents.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=user_message)]))\n\n    try:\n        resp = gemini_client.models.generate_content(\n            model=GEMINI_MODEL_ID,\n            contents=contents,\n            config=types.GenerateContentConfig(\n                system_instruction=build_system_instruction(),\n                temperature=temperature,\n                max_output_tokens=512,\n            ),\n        )\n        answer = (resp.text or \"\").strip() or \"Could not generate answer.\"\n    except Exception as e:\n        answer = f\"Error: {str(e)}\"\n\n    new_history = history + [\n        {\"role\": \"user\", \"content\": user_message},\n        {\"role\": \"assistant\", \"content\": answer},\n    ]\n    return \"\", new_history, new_history\n\nprint(\"\u2705 Gemini chat loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.10 Gamification Logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# GAMIFICATION LOGIC\n# ============================================================================\n\nGAMIFICATION_REF = None\n\ndef init_gamification_ref():\n    global GAMIFICATION_REF\n    GAMIFICATION_REF = db.reference(\"gamification/global\")\n    return GAMIFICATION_REF\n\nDEFAULT_PROFILE = {\n    \"points\": 0,\n    \"spins_available\": 0,\n    \"missions\": {\n        \"sync_data\": {\"last_completed\": None, \"total_completed\": 0},\n        \"analyze_plant\": {\"last_completed\": None, \"total_completed\": 0},\n        \"generate_report\": {\"last_completed\": None, \"total_completed\": 0},\n    },\n    \"coupons\": []\n}\n\ndef _today_key() -> str:\n    return datetime.now(ZoneInfo(TZ_NAME)).strftime(\"%Y-%m-%d\")\n\ndef _now_iso() -> str:\n    return datetime.now(ZoneInfo(TZ_NAME)).isoformat()\n\ndef _get_profile() -> Dict:\n    if GAMIFICATION_REF is None:\n        init_gamification_ref()\n    p = GAMIFICATION_REF.get() or {}\n    prof = {\n        \"points\": int(p.get(\"points\", 0)),\n        \"spins_available\": int(p.get(\"spins_available\", 0)),\n        \"missions\": p.get(\"missions\", {}) or {},\n        \"coupons\": p.get(\"coupons\", []) or [],\n    }\n    merged = {}\n    for mid, base in DEFAULT_PROFILE[\"missions\"].items():\n        m = prof[\"missions\"].get(mid) or {}\n        merged[mid] = {\n            \"last_completed\": m.get(\"last_completed\"),\n            \"total_completed\": int(m.get(\"total_completed\", 0)),\n        }\n    prof[\"missions\"] = merged\n    return prof\n\ndef _save_profile(profile: Dict):\n    if GAMIFICATION_REF is None:\n        init_gamification_ref()\n    GAMIFICATION_REF.set(profile)\n\ndef complete_mission(mission_id: str, points: int = 50) -> Tuple[str, int]:\n    \"\"\"Complete a mission and award points.\"\"\"\n    today = _today_key()\n    profile = _get_profile()\n    mission = profile[\"missions\"].get(mission_id)\n    \n    if not mission:\n        return f\"Unknown mission: {mission_id}\", 0\n    \n    if mission[\"last_completed\"] == today:\n        return f\"Mission '{mission_id}' already completed today!\", 0\n    \n    mission[\"last_completed\"] = today\n    mission[\"total_completed\"] += 1\n    profile[\"points\"] += points\n    profile[\"spins_available\"] += 1\n    \n    _save_profile(profile)\n    return f\"\u2705 +{points} points! Spin earned!\", points\n\ndef spin_wheel() -> Tuple[str, int]:\n    \"\"\"Spin the reward wheel.\"\"\"\n    profile = _get_profile()\n    if profile[\"spins_available\"] <= 0:\n        return \"No spins available!\", 0\n    \n    rewards = [10, 20, 30, 50, 100, 0, 25, 15]\n    reward = random.choice(rewards)\n    profile[\"spins_available\"] -= 1\n    profile[\"points\"] += reward\n    \n    if reward >= 50:\n        code = f\"COUPON-{random.randint(1000,9999)}\"\n        profile[\"coupons\"].append({\"code\": code, \"label\": f\"{reward}pts\", \"created_at\": _now_iso()})\n    \n    _save_profile(profile)\n    return f\"\ud83c\udfb0 You won {reward} points!\", reward\n\ndef rewards_refresh():\n    \"\"\"Refresh rewards display.\"\"\"\n    profile = _get_profile()\n    points = profile[\"points\"]\n    spins = profile[\"spins_available\"]\n    \n    missions_md = \"\"\n    for mid, m in profile[\"missions\"].items():\n        done_today = m.get(\"last_completed\") == _today_key()\n        status = \"\u2705\" if done_today else \"\u2b1c\"\n        missions_md += f\"{status} **{mid}** (Total: {m.get('total_completed', 0)})\\n\"\n    \n    coupons_txt = \"\\n\".join([f\"- {c['code']}: {c['label']}\" for c in profile[\"coupons\"][-5:]]) or \"None\"\n    \n    return points, spins, missions_md, coupons_txt\n\nprint(\"\u2705 Gamification logic loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.11 Gamified Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# GAMIFIED WRAPPERS\n# ============================================================================\n\ndef sync_screen_gamified() -> str:\n    \"\"\"Gamified sync function.\"\"\"\n    msg, count = sync_new_data_from_server()\n    if count > 0:\n        reward_msg, _ = complete_mission(\"sync_data\")\n        msg += f\"\\n{reward_msg}\"\n    return msg\n\ndef analyze_plant_gamified(image, temp, humidity, soil):\n    \"\"\"Gamified plant analysis.\"\"\"\n    result = analyze_plant_image(image, temp, humidity, soil)\n    complete_mission(\"analyze_plant\")\n    return result\n\ndef generate_report_screen_gamified(limit: int) -> Tuple[str, Optional[str]]:\n    \"\"\"Gamified report generation.\"\"\"\n    try:\n        dfs = {\n            \"temperature\": fetch_iot_data(\"temperature\", limit),\n            \"humidity\": fetch_iot_data(\"humidity\", limit),\n            \"soil\": fetch_iot_data(\"soil\", limit),\n        }\n        if all(df is None or df.empty for df in dfs.values()):\n            return \"No data available.\", None\n        \n        out_path = call_report_microservice(dfs, limit)\n        complete_mission(\"generate_report\")\n        return \"\u2705 Report generated successfully!\", out_path\n    except Exception as e:\n        return f\"\u274c Error: {str(e)}\", None\n\ndef call_report_microservice(dfs: Dict, limit: int) -> str:\n    \"\"\"Call report microservice.\"\"\"\n    payload = {\n        \"limit\": int(limit),\n        \"temperature\": df_to_records(dfs.get(\"temperature\")),\n        \"humidity\": df_to_records(dfs.get(\"humidity\")),\n        \"soil\": df_to_records(dfs.get(\"soil\")),\n    }\n    r = requests.post(REPORT_SERVICE_URL, json=payload, timeout=120)\n    if not r.ok:\n        raise RuntimeError(f\"Error: {r.status_code} | {r.text}\")\n    \n    fd, path = tempfile.mkstemp(suffix=\".docx\", prefix=\"daily_report_\")\n    os.close(fd)\n    with open(path, \"wb\") as f:\n        f.write(r.content)\n    return path\n\nprint(\"\u2705 Gamified wrappers loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.12 Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# VISUALIZATION FUNCTIONS\n# ============================================================================\n\nSENSORS = [\n    ('temperature', '\u00b0C', COLORS['temperature']['color'], 'TEMPERATURE'),\n    ('humidity', '%', COLORS['humidity']['color'], 'HUMIDITY'),\n    ('soil', '%', COLORS['soil']['color'], 'SOIL MOISTURE')\n]\n\ndef create_kpi_card(label: str, value: float, unit: str, change: float, trend: str = \"up\", color: str = None) -> str:\n    bc = color or COLORS['temperature']['color']\n    icon = \"\u2191\" if trend == \"up\" else (\"\u2193\" if trend == \"down\" else \"\u2192\")\n    return f\"\"\"<div class=\"kpi-card\" style=\"border-left-color: {bc};\">\n        <p class=\"kpi-label\">{label}</p>\n        <p class=\"kpi-value\">{value:.1f}<span style=\"font-size: 24px;\">{unit}</span></p>\n        <p class=\"kpi-change trend-{trend}\"><span>{icon}</span><span>{change:.1f}%</span></p>\n    </div>\"\"\"\n\ndef create_kpi_cards(df: pd.DataFrame) -> str:\n    if df.empty:\n        return \"<p>No data</p>\"\n    cards = []\n    for col, unit, color, name in SENSORS:\n        if col in df.columns:\n            current = df[col].iloc[-1]\n            change = ((current - df[col].mean()) / df[col].mean() * 100) if df[col].mean() != 0 else 0\n            trend = \"up\" if change > 0 else (\"down\" if change < 0 else \"neutral\")\n            cards.append(create_kpi_card(name, current, unit, abs(change), trend, color))\n    return f\"<div style='display:flex;gap:16px;'>{' '.join(cards)}</div>\"\n\ndef create_stat_cards_html(df: pd.DataFrame) -> str:\n    if df.empty:\n        return \"<p>No data</p>\"\n    stats = []\n    for col, unit, color, name in SENSORS:\n        if col in df.columns:\n            stats.append(f\"<div><b>{name}</b>: Mean={df[col].mean():.1f}{unit}, Std={df[col].std():.1f}</div>\")\n    return \"\".join(stats)\n\ndef create_time_series_plot(df: pd.DataFrame):\n    if df.empty:\n        return None\n    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, subplot_titles=['Temperature', 'Humidity', 'Soil'])\n    for i, (col, unit, color, name) in enumerate(SENSORS, 1):\n        if col in df.columns:\n            fig.add_trace(go.Scatter(x=df['timestamp'], y=df[col], name=name, line=dict(color=color)), row=i, col=1)\n    fig.update_layout(height=600, showlegend=True)\n    return fig\n\ndef calculate_correlations(df: pd.DataFrame):\n    if df.empty or len(df) < 3:\n        return \"<p>Not enough data</p>\", None\n    corr = df[['temperature', 'humidity', 'soil']].corr()\n    fig = px.imshow(corr, text_auto=True, title=\"Correlation Matrix\")\n    card = f\"<div>Temp-Humidity: {corr.loc['temperature','humidity']:.2f}</div>\"\n    return card, fig\n\ndef hourly_patterns(df: pd.DataFrame):\n    if df.empty:\n        return \"<p>No data</p>\", None\n    df['hour'] = df['timestamp'].dt.hour\n    hourly = df.groupby('hour')[['temperature', 'humidity', 'soil']].mean()\n    fig = go.Figure()\n    for col, _, color, name in SENSORS:\n        fig.add_trace(go.Scatter(x=hourly.index, y=hourly[col], name=name, line=dict(color=color)))\n    fig.update_layout(title=\"Hourly Patterns\", xaxis_title=\"Hour\", yaxis_title=\"Value\")\n    return \"<div>Hourly averages</div>\", fig\n\ndef daily_patterns(df: pd.DataFrame):\n    if df.empty:\n        return \"<p>No data</p>\", None\n    df['date'] = df['timestamp'].dt.date\n    daily = df.groupby('date')[['temperature', 'humidity', 'soil']].mean()\n    fig = go.Figure()\n    for col, _, color, name in SENSORS:\n        fig.add_trace(go.Scatter(x=daily.index, y=daily[col], name=name, line=dict(color=color)))\n    fig.update_layout(title=\"Daily Patterns\")\n    return \"<div>Daily averages</div>\", fig\n\ndef distribution_analysis(df: pd.DataFrame):\n    if df.empty:\n        return \"<p>No data</p>\", None\n    fig = make_subplots(rows=1, cols=3, subplot_titles=['Temp', 'Humidity', 'Soil'])\n    for i, (col, _, color, _) in enumerate(SENSORS, 1):\n        if col in df.columns:\n            fig.add_trace(go.Histogram(x=df[col], marker_color=color), row=1, col=i)\n    fig.update_layout(height=300, showlegend=False)\n    return \"<div>Distribution analysis</div>\", fig\n\ndef time_series_decomposition(df: pd.DataFrame, variable: str):\n    if df.empty or variable not in df.columns:\n        return \"<p>No data</p>\", None\n    df_sorted = df.sort_values('timestamp')\n    df_sorted['MA_24'] = df_sorted[variable].rolling(window=24, min_periods=1).mean()\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=df_sorted['timestamp'], y=df_sorted[variable], name='Raw', opacity=0.5))\n    fig.add_trace(go.Scatter(x=df_sorted['timestamp'], y=df_sorted['MA_24'], name='24-pt MA', line=dict(width=2)))\n    fig.update_layout(title=f\"{variable.title()} with Moving Average\")\n    return \"<div>Moving average analysis</div>\", fig\n\nprint(\"\u2705 Visualization functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd27 LAYER 4: Service Layer\n\nThis layer contains microservices and external service integrations:\n- Report generation microservice\n- Service orchestration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Report Microservice Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%writefile report_service.py\n# ============================================================================\n# REPORT GENERATION MICROSERVICE\n# Note: This file runs as a separate process, so it requires its own imports\n# and utility functions (unify_sensor_dfs, records_to_df) that are also \n# defined in the main notebook. This duplication is intentional for isolation.\n# ============================================================================\n# REPORT GENERATION MICROSERVICE\n# ============================================================================\nfrom fastapi import FastAPI\nfrom fastapi.responses import Response\nfrom pydantic import BaseModel\nimport os\nimport tempfile\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any\nimport pandas as pd\nfrom docx import Document\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\nfrom cerebras.cloud.sdk import Cerebras\n\napp = FastAPI(title=\"Report Generation Microservice\")\n\nclass ReportPayload(BaseModel):\n    limit: int\n    temperature: List[Dict[str, Any]]\n    humidity: List[Dict[str, Any]]\n    soil: List[Dict[str, Any]]\n\n@app.get(\"/health\")\ndef health():\n    return {\"ok\": True}\n\ndef records_to_df(records: List[Dict[str, Any]]) -> pd.DataFrame:\n    if not records:\n        return pd.DataFrame(columns=[\"created_at\", \"value\"])\n    df = pd.DataFrame(records)\n    if \"created_at\" not in df.columns or \"value\" not in df.columns:\n        return pd.DataFrame(columns=[\"created_at\", \"value\"])\n    return df\n\ndef unify_sensor_dfs(dfs: dict) -> pd.DataFrame:\n    def prep(df, col):\n        if df is None or df.empty:\n            return pd.DataFrame(columns=[\"timestamp\", col])\n        out = df.copy()\n        if \"timestamp\" not in out.columns and \"created_at\" in out.columns:\n            out = out.rename(columns={\"created_at\": \"timestamp\"})\n        if \"timestamp\" not in out.columns:\n            out = out.reset_index().rename(columns={\"index\": \"timestamp\"})\n        if \"timestamp\" not in out.columns or \"value\" not in out.columns:\n            return pd.DataFrame(columns=[\"timestamp\", col])\n        out = out[[\"timestamp\", \"value\"]]\n        ts = out[\"timestamp\"]\n        if pd.api.types.is_numeric_dtype(ts) or ts.astype(str).str.fullmatch(r\"\\d+\").all():\n            ts_num = pd.to_numeric(ts, errors=\"coerce\")\n            unit = \"ms\" if ts_num.dropna().astype(int).astype(str).str.len().median() >= 13 else \"s\"\n            out[\"timestamp\"] = pd.to_datetime(ts_num, errors=\"coerce\", unit=unit, utc=True).dt.tz_convert(\"Asia/Jerusalem\").dt.tz_localize(None)\n        else:\n            out[\"timestamp\"] = pd.to_datetime(ts, errors=\"coerce\", utc=True).dt.tz_convert(\"Asia/Jerusalem\").dt.tz_localize(None)\n        out = out.dropna(subset=[\"timestamp\"])\n        out[\"value\"] = pd.to_numeric(out[\"value\"], errors=\"coerce\")\n        out = out.dropna(subset=[\"value\"])\n        return out.rename(columns={\"value\": col})\n    \n    t = prep(dfs.get(\"temperature\"), \"temperature\")\n    h = prep(dfs.get(\"humidity\"), \"humidity\")\n    s = prep(dfs.get(\"soil\"), \"soil\")\n    df = t.merge(h, on=\"timestamp\", how=\"outer\").merge(s, on=\"timestamp\", how=\"outer\")\n    return df.sort_values(\"timestamp\").reset_index(drop=True)\n\nclass AutomatedReportGenerator:\n    def __init__(self, cerebras_client, model_name: str):\n        self.client = cerebras_client\n        self.model_name = model_name\n\n    def generate_daily_report(self, df: pd.DataFrame) -> str:\n        if df.empty:\n            return \"No data available.\"\n        try:\n            cutoff = df[\"timestamp\"].max() - timedelta(hours=24)\n            daily = df[df[\"timestamp\"] > cutoff]\n            if daily.empty:\n                daily = df.tail(100)\n        except Exception:\n            daily = df.tail(100)\n\n        stats = {\n            \"date\": daily[\"timestamp\"].max().strftime(\"%Y-%m-%d\"),\n            \"readings\": len(daily),\n            \"temp_avg\": daily[\"temperature\"].mean(),\n            \"temp_min\": daily[\"temperature\"].min(),\n            \"temp_max\": daily[\"temperature\"].max(),\n            \"humidity_avg\": daily[\"humidity\"].mean(),\n            \"humidity_min\": daily[\"humidity\"].min(),\n            \"humidity_max\": daily[\"humidity\"].max(),\n            \"soil_avg\": daily[\"soil\"].mean(),\n            \"soil_min\": daily[\"soil\"].min(),\n            \"soil_max\": daily[\"soil\"].max(),\n        }\n\n        prompt = f\"\"\"Generate a professional daily plant health report:\n\nDATE: {stats['date']}\nREADINGS: {stats['readings']}\n\nCONDITIONS:\n- Temperature: {stats['temp_avg']:.1f}\u00b0C (range: {stats['temp_min']:.1f}-{stats['temp_max']:.1f}\u00b0C)\n- Humidity: {stats['humidity_avg']:.1f}% (range: {stats['humidity_min']:.1f}-{stats['humidity_max']:.1f}%)\n- Soil: {stats['soil_avg']:.1f}% (range: {stats['soil_min']:.1f}-{stats['soil_max']:.1f}%)\n\nGenerate a concise summary (3-4 paragraphs) covering:\n1) Overall conditions\n2) Risks and stress factors\n3) Care recommendations\"\"\"\n\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an agricultural consultant.\"},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            temperature=0.3,\n            max_tokens=800,\n        )\n        return response.choices[0].message.content\n\n    def create_docx_report(self, df: pd.DataFrame, output_path: str) -> str:\n        doc = Document()\n        title = doc.add_heading(\"\ud83c\udf31 Daily Plant Health Report\", 0)\n        title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n        date_para = doc.add_paragraph()\n        date_run = date_para.add_run(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n        date_run.bold = True\n        date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n        doc.add_heading(\"Executive Summary\", 1)\n        doc.add_paragraph(self.generate_daily_report(df))\n        doc.add_heading(\"Statistical Summary\", 1)\n        stats_text = f\"Total Readings: {len(df)}\\nPeriod: {df['timestamp'].min().strftime('%Y-%m-%d')} to {df['timestamp'].max().strftime('%Y-%m-%d')}\"\n        doc.add_paragraph(stats_text)\n        doc.save(output_path)\n        return output_path\n\n@app.post(\"/generate-docx\")\ndef generate_docx(payload: ReportPayload):\n    api_key = os.environ.get(\"CEREBRAS_API_KEY\", \"\").strip()\n    model_name = os.environ.get(\"REPORT_MODEL_NAME\", \"llama3.1-8b\").strip()\n    if not api_key:\n        return Response(content=b\"Missing CEREBRAS_API_KEY\", status_code=500)\n    \n    client = Cerebras(api_key=api_key)\n    report_gen = AutomatedReportGenerator(client, model_name)\n    \n    dfs = {\n        \"temperature\": records_to_df(payload.temperature),\n        \"humidity\": records_to_df(payload.humidity),\n        \"soil\": records_to_df(payload.soil),\n    }\n    df = unify_sensor_dfs(dfs).dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n    if df.empty:\n        return Response(content=b\"No data available.\", status_code=400)\n    \n    fd, path = tempfile.mkstemp(suffix=\".docx\", prefix=\"report_\")\n    os.close(fd)\n    try:\n        report_gen.create_docx_report(df, output_path=path)\n        with open(path, \"rb\") as f:\n            docx_bytes = f.read()\n    finally:\n        try:\n            os.remove(path)\n        except Exception:\n            pass\n    \n    return Response(\n        content=docx_bytes,\n        media_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n        headers={\"Content-Disposition\": \"attachment; filename=report.docx\"},\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Microservice Launcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# MICROSERVICE LAUNCHER\n# ============================================================================\n\n_microservice_proc = None\n\ndef start_report_microservice():\n    \"\"\"Start the report microservice.\"\"\"\n    global _microservice_proc\n    \n    # Set environment variables\n    os.environ[\"CEREBRAS_API_KEY\"] = CEREBRAS_API_KEY or \"\"\n    os.environ[\"REPORT_MODEL_NAME\"] = REPORT_MODEL_NAME\n    \n    # Start uvicorn\n    cmd = [sys.executable, \"-m\", \"uvicorn\", \"report_service:app\", \n           \"--host\", REPORT_SERVICE_HOST, \"--port\", str(REPORT_SERVICE_PORT)]\n    _microservice_proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n    \n    # Health check\n    health_url = f\"http://{REPORT_SERVICE_HOST}:{REPORT_SERVICE_PORT}/health\"\n    for _ in range(30):\n        try:\n            r = requests.get(health_url, timeout=2)\n            if r.ok and r.json().get(\"ok\"):\n                print(f\"\u2705 Microservice UP: {health_url}\")\n                return True\n        except Exception:\n            pass\n        time.sleep(0.5)\n    \n    print(\"\u274c Microservice failed to start\")\n    return False\n\nprint(\"\u2705 Microservice launcher loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udda5\ufe0f LAYER 5: Presentation Layer\n\nThis layer contains all Gradio UI components organized by tab.\nEach tab builder function creates the UI for one feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Realtime Dashboard Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: REALTIME DASHBOARD\n# ============================================================================\n\ndef build_realtime_dashboard_tab():\n    gr.Markdown(\"<h3 style='margin:0; font-size:22px;'>\ud83c\udf3f Overall Plant Status (Real-Time)</h3>\")\n    \n    samples = gr.Slider(1, 200, value=20, step=1, label=\"Number of Samples\")\n    overall_btn = gr.Button(\"Update Plant Dashboard\", variant=\"primary\")\n    \n    overall_status = gr.Textbox(label=\"Overall Status\", lines=1)\n    overall_info = gr.Textbox(label=\"Status Details\", lines=4)\n    \n    with gr.Row():\n        gr.Markdown(f\"\"\"<div style=\"padding:14px;border:1px solid #ccc;border-radius:10px;\">\n            <h4>\ud83c\udf3f Plant Status Legend</h4>\n            <span style=\"color:{STATUS_OK_COLOR};font-size:26px;\">\u25cf</span> <b>Healthy</b> - All values normal<br>\n            <span style=\"color:{STATUS_WARN_COLOR};font-size:26px;\">\u25cf</span> <b>Warning</b> - Near threshold<br>\n            <span style=\"color:{STATUS_BAD_COLOR};font-size:26px;\">\u25cf</span> <b>Not OK</b> - Out of range\n        </div>\"\"\")\n        gr.Markdown(f\"\"\"<div style=\"padding:14px;border:1px solid #ccc;border-radius:10px;\">\n            <h4>\u2139\ufe0f Valid Ranges</h4>\n            <span style=\"color:{COLOR_TEMP};font-size:26px;\">\u25cf</span> \ud83c\udf21\ufe0f Temperature: 18-32\u00b0C<br>\n            <span style=\"color:{COLOR_HUM};font-size:26px;\">\u25cf</span> \ud83d\udca7 Humidity: 35-75%<br>\n            <span style=\"color:{COLOR_SOIL};font-size:26px;\">\u25cf</span> \ud83c\udf31 Soil: 20-60%\n        </div>\"\"\")\n    \n    with gr.Row():\n        temp_plot = gr.Plot(label=\"Temperature\")\n        humidity_plot = gr.Plot(label=\"Humidity\")\n    with gr.Row():\n        soil_plot = gr.Plot(label=\"Soil Moisture\")\n        combined_plot = gr.Plot(label=\"Combined\")\n    \n    overall_btn.click(\n        fn=plant_dashboard,\n        inputs=[samples],\n        outputs=[overall_status, overall_info, temp_plot, humidity_plot, soil_plot, combined_plot]\n    )\n\nprint(\"\u2705 Realtime dashboard tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Analytics Dashboard Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: ANALYTICS DASHBOARD\n# ============================================================================\n\ndef dashboard_screen():\n    \"\"\"Load all data and return comprehensive dashboard.\"\"\"\n    df = load_sensor_data_from_firebase()\n    if df.empty:\n        empty_msg = \"<div style='padding:20px;text-align:center;'>No data. Click Sync!</div>\"\n        return empty_msg, None, None, None, None, None, None, None, None, None, None\n    \n    kpi = create_kpi_cards(df)\n    stats = create_stat_cards_html(df)\n    ts = create_time_series_plot(df)\n    corr_card, corr_plot = calculate_correlations(df)\n    hourly_card, hourly_plot = hourly_patterns(df)\n    daily_card, daily_plot = daily_patterns(df)\n    dist_card, dist_plot = distribution_analysis(df)\n    \n    return kpi, stats, ts, corr_card, corr_plot, hourly_card, hourly_plot, daily_card, daily_plot, dist_card, dist_plot\n\ndef dashboard_moving_avg(variable):\n    df = load_sensor_data_from_firebase()\n    if df.empty:\n        return None, \"No data\"\n    return time_series_decomposition(df, variable)\n\ndef build_iot_dashboard_tab():\n    gr.Markdown('### \ud83d\udcc8 Comprehensive Sensor Analytics')\n    \n    refresh_btn = gr.Button('\ud83d\udd04 Refresh All Data', variant='primary')\n    \n    gr.Markdown('#### \ud83d\udccc Current Readings')\n    kpi_html = gr.HTML()\n    \n    gr.Markdown('#### \ud83d\udcca Statistical Summary')\n    stats_html = gr.HTML()\n    \n    gr.Markdown('#### \ud83d\udcc8 Time Series')\n    ts_plot = gr.Plot()\n    \n    gr.Markdown('#### \ud83d\udd17 Correlations')\n    corr_card = gr.HTML()\n    corr_plot = gr.Plot()\n    \n    gr.Markdown('#### \u23f0 Hourly Patterns')\n    hourly_card = gr.HTML()\n    hourly_plot = gr.Plot()\n    \n    gr.Markdown('#### \ud83d\udcc5 Daily Trends')\n    daily_card = gr.HTML()\n    daily_plot = gr.Plot()\n    \n    gr.Markdown('#### \ud83d\udcca Distributions')\n    dist_card = gr.HTML()\n    dist_plot = gr.Plot()\n    \n    gr.Markdown('#### \ud83d\udcc9 Moving Averages')\n    with gr.Row():\n        ma_variable = gr.Dropdown(choices=['temperature', 'humidity', 'soil'], value='temperature', label='Variable')\n        ma_btn = gr.Button('Generate')\n    ma_card = gr.HTML()\n    ma_plot = gr.Plot()\n    \n    refresh_btn.click(\n        dashboard_screen,\n        outputs=[kpi_html, stats_html, ts_plot, corr_card, corr_plot, \n                 hourly_card, hourly_plot, daily_card, daily_plot, dist_card, dist_plot]\n    )\n    ma_btn.click(dashboard_moving_avg, inputs=ma_variable, outputs=[ma_card, ma_plot])\n\nprint(\"\u2705 Analytics dashboard tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Generate Report Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: GENERATE REPORT\n# ============================================================================\n\ndef build_generate_report_tab():\n    gr.Markdown(\"## \ud83d\udcc4 Generate Report\")\n    gr.Markdown(\"Generate a Word report based on sensor data with AI-powered analysis.\")\n    \n    report_samples = gr.Slider(minimum=5, maximum=200, value=20, step=1, label=\"Samples per sensor\")\n    report_btn = gr.Button(\"\ud83d\udce5 Generate & Download Report\", variant=\"primary\")\n    report_status = gr.Textbox(label=\"Status\", lines=2)\n    report_file = gr.File(label=\"Download DOCX\")\n    \n    report_btn.click(\n        fn=generate_report_screen_gamified,\n        inputs=[report_samples],\n        outputs=[report_status, report_file]\n    )\n\nprint(\"\u2705 Generate report tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 Plant Disease Detection Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: PLANT DISEASE DETECTION\n# ============================================================================\n\ndef build_plant_disease_detection_tab():\n    gr.Markdown(\"## \ud83d\uddbc\ufe0f Plant Disease Detection\")\n    \n    with gr.Row():\n        with gr.Column(scale=2):\n            image = gr.Image(type=\"filepath\", label=\"Upload plant image\", sources=[\"upload\"])\n            temp = gr.Slider(0, 45, value=25, label=\"Temperature (\u00b0C)\")\n            humidity = gr.Slider(0, 100, value=50, label=\"Humidity (%)\")\n            soil = gr.Slider(0, 100, value=50, label=\"Soil Moisture (%)\")\n            run_btn = gr.Button(\"Analyze Plant\", variant=\"primary\")\n        \n        with gr.Column(scale=2):\n            diagnosis = gr.Textbox(label=\"Diagnosis\")\n            status = gr.HTML(label=\"Status\")\n            alerts = gr.Textbox(label=\"Alerts\", lines=5)\n            recommendations = gr.Textbox(label=\"Recommendations\", lines=5)\n    \n    run_btn.click(\n        fn=analyze_plant_gamified,\n        inputs=[image, temp, humidity, soil],\n        outputs=[diagnosis, status, alerts, recommendations]\n    )\n\nprint(\"\u2705 Plant disease detection tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 RAG Chat Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: RAG CHAT\n# ============================================================================\n\ndef rag_ui(query: str, k: int = 3):\n    \"\"\"Gradio adapter for RAG system.\"\"\"\n    if not query.strip():\n        return \"Please enter a question.\", \"\"\n    try:\n        result = rag_answer_with_model(query, k=int(k))\n        answer = result.get(\"answer\", \"No answer found\")\n        sources = result.get(\"sources\", [])\n        unique_sources = [s for s in list(set(sources)) if s]\n        source_text = \"Sources:\\n\" + \"\\n\".join([f\"- {url}\" for url in unique_sources])\n        return answer, source_text\n    except Exception as e:\n        return f\"Error: {str(e)}\", \"\"\n\ndef build_rag_chat_tab():\n    gr.Markdown(\"### \ud83d\udd0d Plant Disease Research Assistant\")\n    \n    with gr.Row():\n        with gr.Column():\n            q_input = gr.Textbox(label=\"Question\", placeholder=\"Ask about plant pathology...\", lines=2)\n            k_slider = gr.Slider(1, 5, value=3, step=1, label=\"Search Depth\")\n            ask_btn = gr.Button(\"Analyze Documents\", variant=\"primary\")\n    \n    with gr.Row():\n        ans_out = gr.Textbox(label=\"Analysis Result\", lines=8, interactive=False)\n        src_out = gr.Textbox(label=\"Citations\", lines=4, interactive=False)\n    \n    ask_btn.click(fn=rag_ui, inputs=[q_input, k_slider], outputs=[ans_out, src_out])\n\nprint(\"\u2705 RAG chat tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.6 Gemini Chat Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: GEMINI CHAT\n# ============================================================================\n\ndef build_gemini_chat_tab():\n    gr.Markdown(\"## \ud83d\udcac Gemini Free Chat\")\n    gr.Markdown(\"Free conversation powered by Gemini (multi-turn history).\")\n    \n    chat = gr.Chatbot(label=\"Chat\")\n    state = gr.State([])\n    \n    msg = gr.Textbox(label=\"Message\", placeholder=\"Type here...\", lines=2)\n    temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.7, step=0.05, label=\"Creativity\")\n    \n    with gr.Row():\n        send_btn = gr.Button(\"Send\", variant=\"primary\")\n        clear_btn = gr.Button(\"Clear\")\n    \n    send_btn.click(fn=gemini_turn, inputs=[msg, state, temperature], outputs=[msg, chat, state])\n    msg.submit(fn=gemini_turn, inputs=[msg, state, temperature], outputs=[msg, chat, state])\n    \n    def clear_chat():\n        return [], []\n    clear_btn.click(fn=clear_chat, inputs=[], outputs=[chat, state])\n\nprint(\"\u2705 Gemini chat tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.7 Sync Data Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: SYNC DATA\n# ============================================================================\n\ndef build_sync_data_tab():\n    gr.Markdown('## \ud83d\udd04 Sync Data')\n    gr.Markdown('Upload IoT Data from Server to Firebase')\n    \n    sync_btn = gr.Button('\ud83d\udd04 Sync New Data', variant='primary', size='lg')\n    sync_output = gr.Textbox(label='Status', lines=5)\n    \n    sync_btn.click(fn=sync_screen_gamified, outputs=sync_output)\n\nprint(\"\u2705 Sync data tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.8 Rewards Tab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB: FARM REWARDS (GAMIFICATION)\n# ============================================================================\n\ndef build_rewards_tab():\n    gr.Markdown(\"## \ud83c\udfae Farm Rewards\")\n    gr.Markdown(\"Complete daily missions to earn points and spins!\")\n    \n    with gr.Row():\n        points_box = gr.Number(label=\"\ud83c\udfc6 Total Points\", interactive=False)\n        spins_box = gr.Number(label=\"\ud83c\udfb0 Spins Available\", interactive=False)\n    \n    gr.Markdown(\"### \ud83d\udccb Daily Missions\")\n    missions_md = gr.Markdown()\n    \n    gr.Markdown(\"### \ud83c\udfab Recent Coupons\")\n    coupons_txt = gr.Textbox(label=\"Coupons\", lines=3, interactive=False)\n    \n    with gr.Row():\n        spin_btn = gr.Button(\"\ud83c\udfb0 Spin Wheel!\", variant=\"primary\")\n        refresh_btn = gr.Button(\"\ud83d\udd04 Refresh\")\n    \n    spin_result = gr.Textbox(label=\"Spin Result\", lines=1)\n    \n    spin_btn.click(\n        fn=lambda: (spin_wheel()[0],) + rewards_refresh(),\n        outputs=[spin_result, points_box, spins_box, missions_md, coupons_txt]\n    )\n    refresh_btn.click(fn=rewards_refresh, outputs=[points_box, spins_box, missions_md, coupons_txt])\n    \n    return points_box, spins_box, missions_md, coupons_txt\n\nprint(\"\u2705 Rewards tab loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 LAYER 6: Application Layer\n\nThis layer contains the application orchestration:\n- Tab registry\n- App builder\n- Initialization and launch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Tab Registry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# TAB REGISTRY\n# ============================================================================\n\n# Central registry of all tabs - add/remove tabs here only\nTABS = [\n    (\"\ud83c\udf31 Realtime Dashboard\", build_realtime_dashboard_tab),\n    (\"\ud83d\udcca Analytics Dashboard\", build_iot_dashboard_tab),\n    (\"\ud83d\udcc4 Generate Report\", build_generate_report_tab),\n    (\"\ud83d\uddbc\ufe0f Plant Disease Detection\", build_plant_disease_detection_tab),\n    (\"\ud83d\udcac RAG Chat\", build_rag_chat_tab),\n    (\"\ud83d\udcac Gemini Chat\", build_gemini_chat_tab),\n    (\"\ud83d\udd04 Sync Data\", build_sync_data_tab),\n    (\"\ud83c\udfae Farm Rewards\", build_rewards_tab),\n]\n\nprint(\"\u2705 Tab registry loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Application Builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# APPLICATION BUILDER\n# ============================================================================\n\ndef build_app():\n    \"\"\"Build the main Gradio application.\"\"\"\n    with gr.Blocks(css=CUSTOM_CSS, title=\"Cloud Garden - IoT & AI\") as demo:\n        gr.Markdown(\"# \ud83c\udf3f Cloud Garden - IoT & AI\")\n        \n        rewards_tab_ref = None\n        rewards_outputs = None\n        \n        with gr.Tabs():\n            for tab_name, tab_builder in TABS:\n                if tab_name.startswith(\"\ud83c\udfae\"):\n                    with gr.Tab(tab_name) as rewards_tab_ref:\n                        rewards_outputs = tab_builder()\n                else:\n                    with gr.Tab(tab_name):\n                        tab_builder()\n        \n        # Auto-refresh rewards on load\n        if rewards_tab_ref and rewards_outputs:\n            points_box, spins_box, missions_md, coupons_txt = rewards_outputs\n            demo.load(fn=rewards_refresh, outputs=[points_box, spins_box, missions_md, coupons_txt])\n            rewards_tab_ref.select(fn=rewards_refresh, outputs=[points_box, spins_box, missions_md, coupons_txt])\n    \n    return demo\n\nprint(\"\u2705 Application builder loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3 Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# INITIALIZATION\n# ============================================================================\n\ndef initialize_all():\n    \"\"\"Initialize all services and connections.\"\"\"\n    print(\"=\" * 50)\n    print(\"\ud83d\ude80 Initializing CloudGarden Application\")\n    print(\"=\" * 50)\n    \n    # Initialize API keys\n    initialize_api_keys()\n    \n    # Initialize Firebase\n    initialize_firebase()\n    \n    # Initialize LLM clients\n    init_cerebras_client()\n    init_gemini_client()\n    \n    # Initialize gamification\n    init_gamification_ref()\n    \n    # Load RAG index\n    try:\n        load_store_from_firebase(load_text=False)\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Could not load RAG index: {e}\")\n    \n    # Start microservice\n    start_report_microservice()\n    \n    print(\"=\" * 50)\n    print(\"\u2705 Initialization Complete!\")\n    print(\"=\" * 50)\n\nprint(\"\u2705 Initialization function defined (call initialize_all() to start)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.4 Data Ingestion Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# DATA INGESTION EXAMPLE\n# ============================================================================\n\n# Load data from Firebase (Data Lake pattern)\ndf = load_sensor_data_from_firebase()\nprint(f\"Data shape: {df.shape}\")\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5 MapReduce Analysis Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# MAPREDUCE ANALYSIS EXAMPLE\n# ============================================================================\n\nif not df.empty:\n    # Map: create hourly buckets\n    df['hour_bucket'] = df['timestamp'].dt.floor('H')\n    \n    # Reduce: aggregate per hour\n    agg_hourly = df.groupby('hour_bucket', as_index=False).agg(\n        avg_temperature=('temperature', 'mean'),\n        avg_humidity=('humidity', 'mean'),\n        avg_soil=('soil', 'mean')\n    )\n    \n    # Visualize\n    plt.figure(figsize=(10, 4))\n    plt.plot(agg_hourly['hour_bucket'], agg_hourly['avg_temperature'])\n    plt.xlabel('Time (hourly)')\n    plt.ylabel('Average Temperature')\n    plt.title('Average Temperature per Hour')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"avg_temperature_per_hour.png\")\n    plt.show()\n    \n    print(\"Graph saved to: avg_temperature_per_hour.png\")\n    agg_hourly.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.6 Launch Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n# LAUNCH APPLICATION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    # Initialize all services before launching\n    initialize_all()\n    \n    # Build and launch the app\n    app = build_app()\n    app.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}